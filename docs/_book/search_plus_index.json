{"./":{"url":"./","title":"前言","keywords":"","body":"A high performance real-time analytics database A fast, modern analytics database Druid is designed for workflows where fast ad-hoc analytics, instant data visibility, or supporting high concurrency is important. As such, Druid is often used to power UIs where an interactive, consistent user experience is desired. Easy integration with your existing data pipelines Druid streams data from message buses such as Kafka, and Amazon Kinesis, and batch load files from data lakes such as HDFS, and Amazon S3. Druid supports most popular file formats for structured and semi-structured data. Fast, consistent queries at high concurrency Druid has been benchmarked to greatly outperform legacy solutions. Druid combines novel storage ideas, indexing structures, and both exact and approximate queries to return most results in under a second. Broad applicability Druid unlocks new types of queries and workflows for clickstream, APM, supply chain, network telemetry, digital marketing, risk/fraud, and many other types of data. Druid is purpose built for rapid, ad-hoc queries on both real-time and historical data. Deploy in public, private, and hybrid clouds Druid can be deployed in any *NIX environment on commodity hardware, both in the cloud and on premise. Deploying Druid is easy: scaling up and down is as simple as adding and removing Druid services. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:44 "},"design/":{"url":"design/","title":"Apache Druid简介","keywords":"","body":" What is Druid? Apache Druid is a real-time analytics database designed for fast slice-and-dice analytics (\"OLAP\" queries) on large data sets. Druid is most often used as a database for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data. Common application areas for Druid include: Clickstream analytics (web and mobile analytics) Network telemetry analytics (network performance monitoring) Server metrics storage Supply chain analytics (manufacturing metrics) Application performance metrics Digital marketing/advertising analytics Business intelligence / OLAP Druid's core architecture combines ideas from data warehouses, timeseries databases, and logsearch systems. Some of Druid's key features are: Columnar storage format. Druid uses column-oriented storage, meaning it only needs to load the exact columns needed for a particular query. This gives a huge speed boost to queries that only hit a few columns. In addition, each column is stored optimized for its particular data type, which supports fast scans and aggregations. Scalable distributed system. Druid is typically deployed in clusters of tens to hundreds of servers, and can offer ingest rates of millions of records/sec, retention of trillions of records, and query latencies of sub-second to a few seconds. Massively parallel processing. Druid can process a query in parallel across the entire cluster. Realtime or batch ingestion. Druid can ingest data either real-time (ingested data is immediately available for querying) or in batches. Self-healing, self-balancing, easy to operate. As an operator, to scale the cluster out or in, simply add or remove servers and the cluster will rebalance itself automatically, in the background, without any downtime. If any Druid servers fail, the system will automatically route around the damage until those servers can be replaced. Druid is designed to run 24/7 with no need for planned downtimes for any reason, including configuration changes and software updates. Cloud-native, fault-tolerant architecture that won't lose data. Once Druid has ingested your data, a copy is stored safely in deep storage (typically cloud storage, HDFS, or a shared filesystem). Your data can be recovered from deep storage even if every single Druid server fails. For more limited failures affecting just a few Druid servers, replication ensures that queries are still possible while the system recovers. Indexes for quick filtering. Druid uses Roaring or CONCISE compressed bitmap indexes to create indexes that power fast filtering and searching across multiple columns. Time-based partitioning. Druid first partitions data by time, and can additionally partition based on other fields. This means time-based queries will only access the partitions that match the time range of the query. This leads to significant performance improvements for time-based data. Approximate algorithms. Druid includes algorithms for approximate count-distinct, approximate ranking, and computation of approximate histograms and quantiles. These algorithms offer bounded memory usage and are often substantially faster than exact computations. For situations where accuracy is more important than speed, Druid also offers exact count-distinct and exact ranking. Automatic summarization at ingest time. Druid optionally supports data summarization at ingestion time. This summarization partially pre-aggregates your data, and can lead to big costs savings and performance boosts. When should I use Druid? Druid is used by many companies of various sizes for many different use cases. Check out the Powered by Apache Druid page Druid is likely a good choice if your use case fits a few of the following descriptors: Insert rates are very high, but updates are less common. Most of your queries are aggregation and reporting queries (\"group by\" queries). You may also have searching and scanning queries. You are targeting query latencies of 100ms to a few seconds. Your data has a time component (Druid includes optimizations and design choices specifically related to time). You may have more than one table, but each query hits just one big distributed table. Queries may potentially hit more than one smaller \"lookup\" table. You have high cardinality data columns (e.g. URLs, user IDs) and need fast counting and ranking over them. You want to load data from Kafka, HDFS, flat files, or object storage like Amazon S3. Situations where you would likely not want to use Druid include: You need low-latency updates of existing records using a primary key. Druid supports streaming inserts, but not streaming updates (updates are done using background batch jobs). You are building an offline reporting system where query latency is not very important. You want to do \"big\" joins (joining one big fact table to another big fact table) and you are okay with these queries taking a long time to complete. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/":{"url":"tutorials/","title":"快速入门","keywords":"","body":" This quickstart gets you started with Apache Druid and introduces you to some of its basic features. Following these steps, you will install Druid and load sample data using its native batch ingestion feature. Before starting, you may want to read the general Druid overview and ingestion overview, as the tutorials refer to concepts discussed on those pages. Requirements You can follow these steps on a relatively small machine, such as a laptop with around 4 CPU and 16 GB of RAM. Druid comes with several startup configuration profiles for a range of machine sizes. The micro-quickstartconfiguration profile shown here is suitable for evaluating Druid. If you want to try out Druid's performance or scaling capabilities, you'll need a larger machine and configuration profile. The configuration profiles included with Druid range from the even smaller Nano-Quickstart configuration (1 CPU, 4GB RAM) to the X-Large configuration (64 CPU, 512GB RAM). For more information, see Single server deployment. Alternatively, see Clustered deployment for information on deploying Druid services across clustered machines. The software requirements for the installation machine are: Linux, Mac OS X, or other Unix-like OS (Windows is not supported) Java 8, Update 92 or later (8u92+) Druid officially supports Java 8 only. Support for later major versions of Java is currently in experimental status. Druid relies on the environment variables JAVA_HOME or DRUID_JAVA_HOME to find Java on the machine. You can set DRUID_JAVA_HOME if there is more than one instance of Java. To verify Java requirements for your environment, run the bin/verify-java script. Before installing a production Druid instance, be sure to consider the user account on the operating system under which Druid will run. This is important because any Druid console user will have, effectively, the same permissions as that user. So, for example, the file browser UI will show console users the files that the underlying user can access. In general, avoid running Druid as root user. Consider creating a dedicated user account for running Druid. Step 1. Install Druid After confirming the requirements, follow these steps: Download the release. In your terminal, extract Druid and change directories to the distribution directory: tar -xzf apache-druid--bin.tar.gz cd apache-druid- In the directory, you'll find LICENSE and NOTICE files and subdirectories for executable files, configuration files, sample data and more. Step 2: Start up Druid services Start up Druid services using the micro-quickstart single-machine configuration. From the apache-druid- package root, run the following command: ./bin/start-micro-quickstart This brings up instances of ZooKeeper and the Druid services: $ ./bin/start-micro-quickstart [Fri May 3 11:40:50 2019] Running command[zk], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/zk.log]: bin/run-zk conf [Fri May 3 11:40:50 2019] Running command[coordinator-overlord], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/coordinator-overlord.log]: bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart [Fri May 3 11:40:50 2019] Running command[broker], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/broker.log]: bin/run-druid broker conf/druid/single-server/micro-quickstart [Fri May 3 11:40:50 2019] Running command[router], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/router.log]: bin/run-druid router conf/druid/single-server/micro-quickstart [Fri May 3 11:40:50 2019] Running command[historical], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/historical.log]: bin/run-druid historical conf/druid/single-server/micro-quickstart [Fri May 3 11:40:50 2019] Running command[middleManager], logging to[/apache-druid-{{DRUIDVERSION}}/var/sv/middleManager.log]: bin/run-druid middleManager conf/druid/single-server/micro-quickstart All persistent state, such as the cluster metadata store and segments for the services, are kept in the var directory under the Druid root directory, apache-druid-. Each service writes to a log file under var/sv, as noted in the startup script output above. At any time, you can revert Druid to its original, post-installation state by deleting the entire var directory. You may want to do this, for example, between Druid tutorials or after experimentation, to start with a fresh instance. To stop Druid at any time, use CTRL-C in the terminal. This exits the bin/start-micro-quickstart script and terminates all Druid processes. Step 3. Open the Druid console After the Druid services finish startup, open the Druid console at http://localhost:8888. It may take a few seconds for all Druid services to finish starting, including the Druid router, which serves the console. If you attempt to open the Druid console before startup is complete, you may see errors in the browser. Wait a few moments and try again. Step 4. Load data Ingestion specs define the schema of the data Druid reads and stores. You can write ingestion specs by hand or using the data loader, as we'll do here to perform batch file loading with Druid's native batch ingestion. The Druid distribution bundles sample data we can use. The sample data located in quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz in the Druid root directory represents Wikipedia page edits for a given day. Click Load data from the Druid console header (). Select the Local disk tile and then click Connect data. Enter the following values: Base directory: quickstart/tutorial/ File filter: wikiticker-2015-09-12-sampled.json.gz Entering the base directory and wildcard file filter separately, as afforded by the UI, allows you to specify multiple files for ingestion at once. Click Apply. The data loader displays the raw data, giving you a chance to verify that the data appears as expected. Notice that your position in the sequence of steps to load data, Connect in our case, appears at the top of the console, as shown below. You can click other steps to move forward or backward in the sequence at any time. Click Next: Parse data. The data loader tries to determine the parser appropriate for the data format automatically. In this case it identifies the data format as json, as shown in the Input format field at the bottom right. Feel free to select other Input format options to get a sense of their configuration settings and how Druid parses other types of data. With the JSON parser selected, click Next: Parse time. The Parse time settings are where you view and adjust the primary timestamp column for the data. Druid requires data to have a primary timestamp column (internally stored in a column called __time). If you do not have a timestamp in your data, select Constant value. In our example, the data loader determines that the time column is the only candidate that can be used as the primary time column. Click Next: Transform, Next: Filter, and then Next: Configure schema, skipping a few steps. You do not need to adjust transformation or filtering settings, as applying ingestion time transforms and filters are out of scope for this tutorial. The Configure schema settings are where you configure what dimensions and metrics are ingested. The outcome of this configuration represents exactly how the data will appear in Druid after ingestion. Since our dataset is very small, you can turn off rollup by unsetting the Rollup switch and confirming the change when prompted. Click Next: Partition to configure how the data will be split into segments. In this case, choose DAY as the Segment granularity. Since this is a small dataset, we can have just a single segment, which is what selecting DAY as the segment granularity gives us. Click Next: Tune and Next: Publish. The Publish settings are where you specify the datasource name in Druid. Let's change the default name from wikiticker-2015-09-12-sampled to wikipedia. Click Next: Edit spec to review the ingestion spec we've constructed with the data loader. Feel free to go back and change settings from previous steps to see how doing so updates the spec. Similarly, you can edit the spec directly and see it reflected in the previous steps. For other ways to load ingestion specs in Druid, see Tutorial: Loading a file. Once you are satisfied with the spec, click Submit. The new task for our wikipedia datasource now appears in the Ingestion view. The task may take a minute or two to complete. When done, the task status should be \"SUCCESS\", with the duration of the task indicated. Note that the view is set to automatically refresh, so you do not need to refresh the browser to see the status change. A successful task means that one or more segments have been built and are now picked up by our data servers. Step 5. Query the data You can now see the data as a datasource in the console and try out a query, as follows: Click Datasources from the console header. If the wikipedia datasource doesn't appear, wait a few moments for the segment to finish loading. A datasource is queryable once it is shown to be \"Fully available\" in the Availability column. When the datasource is available, open the Actions menu () for that datasource and choose Query with SQL. Notice the other actions you can perform for a datasource, including configuring retention rules, compaction, and more. Run the prepopulated query, SELECT * FROM \"wikipedia\" to see the results. Congratulations! You've gone from downloading Druid to querying data in just one quickstart. See the following section for what to do next. Next steps After finishing the quickstart, check out the query tutorial to further explore Query features in the Druid console. Alternatively, learn about other ways to ingest data in one of these tutorials: Loading stream data from Apache Kafka – How to load streaming data from a Kafka topic. Loading a file using Apache Hadoop – How to perform a batch file load, using a remote Hadoop cluster. Writing your own ingestion spec – How to write a new ingestion spec and use it to load data. Remember that after stopping Druid services, you can start clean next time by deleting the var directory from the Druid root directory and running the bin/start-micro-quickstart script again. You will likely want to do this before taking other data ingestion tutorials, since in them you will create the same wikipedia datasource. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"tutorials/docker.html":{"url":"tutorials/docker.html","title":"通过Docker启动","keywords":"","body":" In this quickstart, we will download the Apache Druid image from Docker Hub and set it up on a single machine using Docker and Docker Compose. The cluster will be ready to load data after completing this initial setup. Before beginning the quickstart, it is helpful to read the general Druid overview and the ingestion overview, as the tutorials will refer to concepts discussed on those pages. Additionally, familiarity with Docker is recommended. Prerequisites Docker Getting started The Druid source code contains an example docker-compose.yml which can pull an image from Docker Hub and is suited to be used as an example environment and to experiment with Docker based Druid configuration and deployments. Compose file The example docker-compose.yml will create a container for each Druid service, as well as Zookeeper and a PostgreSQL container as the metadata store. Deep storage will be a local directory, by default configured as ./storage relative to your docker-compose.yml file, and will be mounted as /opt/data and shared between Druid containers which require access to deep storage. The Druid containers are configured via an environment file. Configuration Configuration of the Druid Docker container is done via environment variables, which may additionally specify paths to the standard Druid configuration files Special environment variables: JAVA_OPTS -- set java options DRUID_LOG4J -- set the entire log4j.xml verbatim DRUID_LOG_LEVEL -- override the default log level in default log4j DRUID_XMX -- set Java Xmx DRUID_XMS -- set Java Xms DRUID_MAXNEWSIZE -- set Java max new size DRUID_NEWSIZE -- set Java new size DRUID_MAXDIRECTMEMORYSIZE -- set Java max direct memory size DRUID_CONFIG_COMMON -- full path to a file for druid 'common' properties DRUID_CONFIG_${service} -- full path to a file for druid 'service' properties In addition to the special environment variables, the script which launches Druid in the container will also attempt to use any environment variable starting with the druid_ prefix as a command-line configuration. For example, an environment variable would be translated into ```-Ddruid.metadata.storage.type=postgresql for the Druid process in the container. The Druid docker-compose.yml example utilizes a single environment file to specify the complete Druid configuration; however, in production use cases we suggest using either DRUID_COMMON_CONFIG and DRUID_CONFIG_${service} or specially tailored, service-specific environment files. Launching the cluster Run docker-compose up to launch the cluster with a shell attached, or docker-compose up -d to run the cluster in the background. If using the example files directly, this command should be run from distribution/docker/ in your Druid installation directory. Once the cluster has started, you can navigate to http://localhost:8888. The Druid router process, which serves the Druid console, resides at this address. It takes a few seconds for all the Druid processes to fully start up. If you open the console immediately after starting the services, you may see some errors that you can safely ignore. From here you can follow along with the Quickstart, or elaborate on your docker-compose.yml to add any additional external service dependencies as necessary. Docker Memory Requirements If you experience any processes crashing with a 137 error code you likely don't have enough memory allocated to Docker. 6 GB may be a good place to start. 本文源自Apache Druid，阅读原文 更新于： 2020-12-08 09:31:10 "},"operations/single-server.html":{"url":"operations/single-server.html","title":"单机部署","keywords":"","body":" Druid includes a set of reference configurations and launch scripts for single-machine deployments: nano-quickstart micro-quickstart small medium large xlarge The micro-quickstart is sized for small machines like laptops and is intended for quick evaluation use-cases. The nano-quickstart is an even smaller configuration, targeting a machine with 1 CPU and 4GB memory. It is meant for limited evaluations in resource constrained environments, such as small Docker containers. The other configurations are intended for general use single-machine deployments. They are sized for hardware roughly based on Amazon's i3 series of EC2 instances. The startup scripts for these example configurations run a single ZK instance along with the Druid services. You can choose to deploy ZK separately as well. The example configurations run the Druid Coordinator and Overlord together in a single process using the optional configuration druid.coordinator.asOverlord.enabled=true, described in the Coordinator configuration documentation. While example configurations are provided for very large single machines, at higher scales we recommend running Druid in a clustered deployment, for fault-tolerance and reduced resource contention. Single server reference configurations Nano-Quickstart: 1 CPU, 4GB RAM Launch command: bin/start-nano-quickstart Configuration directory: conf/druid/single-server/nano-quickstart Micro-Quickstart: 4 CPU, 16GB RAM Launch command: bin/start-micro-quickstart Configuration directory: conf/druid/single-server/micro-quickstart Small: 8 CPU, 64GB RAM (~i3.2xlarge) Launch command: bin/start-small Configuration directory: conf/druid/single-server/small Medium: 16 CPU, 128GB RAM (~i3.4xlarge) Launch command: bin/start-medium Configuration directory: conf/druid/single-server/medium Large: 32 CPU, 256GB RAM (~i3.8xlarge) Launch command: bin/start-large Configuration directory: conf/druid/single-server/large X-Large: 64 CPU, 512GB RAM (~i3.16xlarge) Launch command: bin/start-xlarge Configuration directory: conf/druid/single-server/xlarge 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/cluster.html":{"url":"tutorials/cluster.html","title":"集群部署","keywords":"","body":" Apache Druid is designed to be deployed as a scalable, fault-tolerant cluster. In this document, we'll set up a simple cluster and discuss how it can be further configured to meet your needs. This simple cluster will feature: A Master server to host the Coordinator and Overlord processes Two scalable, fault-tolerant Data servers running Historical and MiddleManager processes A query server, hosting the Druid Broker and Router processes In production, we recommend deploying multiple Master servers and multiple Query servers in a fault-tolerant configuration based on your specific fault-tolerance needs, but you can get started quickly with one Master and one Query server and add more servers later. Select hardware Fresh Deployment If you do not have an existing Druid cluster, and wish to start running Druid in a clustered deployment, this guide provides an example clustered deployment with pre-made configurations. Master server The Coordinator and Overlord processes are responsible for handling the metadata and coordination needs of your cluster. They can be colocated together on the same server. In this example, we will be deploying the equivalent of one AWS m5.2xlarge instance. This hardware offers: 8 vCPUs 31 GB RAM Example Master server configurations that have been sized for this hardware can be found under conf/druid/cluster/master. Data server Historicals and MiddleManagers can be colocated on the same server to handle the actual data in your cluster. These servers benefit greatly from CPU, RAM, and SSDs. In this example, we will be deploying the equivalent of two AWS i3.4xlarge instances. This hardware offers: 16 vCPUs 122 GB RAM 2 * 1.9TB SSD storage Example Data server configurations that have been sized for this hardware can be found under conf/druid/cluster/data. Query server Druid Brokers accept queries and farm them out to the rest of the cluster. They also optionally maintain an in-memory query cache. These servers benefit greatly from CPU and RAM. In this example, we will be deploying the equivalent of one AWS m5.2xlarge instance. This hardware offers: 8 vCPUs 31 GB RAM You can consider co-locating any open source UIs or query libraries on the same server that the Broker is running on. Example Query server configurations that have been sized for this hardware can be found under conf/druid/cluster/query. Other Hardware Sizes The example cluster above is chosen as a single example out of many possible ways to size a Druid cluster. You can choose smaller/larger hardware or less/more servers for your specific needs and constraints. If your use case has complex scaling requirements, you can also choose to not co-locate Druid processes (e.g., standalone Historical servers). The information in the basic cluster tuning guide can help with your decision-making process and with sizing your configurations. Migrating from a single-server deployment If you have an existing single-server deployment, such as the ones from the single-server deployment examples, and you wish to migrate to a clustered deployment of similar scale, the following section contains guidelines for choosing equivalent hardware using the Master/Data/Query server organization. Master server The main considerations for the Master server are available CPUs and RAM for the Coordinator and Overlord heaps. Sum up the allocated heap sizes for your Coordinator and Overlord from the single-server deployment, and choose Master server hardware with enough RAM for the combined heaps, with some extra RAM for other processes on the machine. For CPU cores, you can choose hardware with approximately 1/4th of the cores of the single-server deployment. Data server When choosing Data server hardware for the cluster, the main considerations are available CPUs and RAM, and using SSD storage if feasible. In a clustered deployment, having multiple Data servers is a good idea for fault-tolerance purposes. When choosing the Data server hardware, you can choose a split factor N, divide the original CPU/RAM of the single-server deployment by N, and deploy N Data servers of reduced size in the new cluster. Instructions for adjusting the Historical/MiddleManager configs for the split are described in a later section in this guide. Query server The main considerations for the Query server are available CPUs and RAM for the Broker heap + direct memory, and Router heap. Sum up the allocated memory sizes for your Broker and Router from the single-server deployment, and choose Query server hardware with enough RAM to cover the Broker/Router, with some extra RAM for other processes on the machine. For CPU cores, you can choose hardware with approximately 1/4th of the cores of the single-server deployment. The basic cluster tuning guide has information on how to calculate Broker/Router memory usage. Select OS We recommend running your favorite Linux distribution. You will also need: Java 8 or later Warning: Druid only officially supports Java 8. Any Java version later than 8 is still experimental. If needed, you can specify where to find Java using the environment variables DRUID_JAVA_HOME or JAVA_HOME. For more details run the verify-java script. Your OS package manager should be able to help for both Java. If your Ubuntu-based OS does not have a recent enough version of Java, WebUpd8 offers packages for those OSes. Download the distribution First, download and unpack the release archive. It's best to do this on a single machine at first, since you will be editing the configurations and then copying the modified distribution out to all of your servers. Download the release. Extract Druid by running the following commands in your terminal: tar -xzf apache-druid-{{DRUIDVERSION}}-bin.tar.gz cd apache-druid-{{DRUIDVERSION}} In the package, you should find: LICENSE and NOTICE files bin/* - scripts related to the single-machine quickstart conf/druid/cluster/* - template configurations for a clustered setup extensions/* - core Druid extensions hadoop-dependencies/* - Druid Hadoop dependencies lib/* - libraries and dependencies for core Druid quickstart/* - files related to the single-machine quickstart We'll be editing the files in conf/druid/cluster/ in order to get things running. Migrating from Single-Server Deployments In the following sections we will be editing the configs under conf/druid/cluster. If you have an existing single-server deployment, please copy your existing configs to conf/druid/cluster to preserve any config changes you have made. Configure metadata storage and deep storage Migrating from Single-Server Deployments If you have an existing single-server deployment and you wish to preserve your data across the migration, please follow the instructions at metadata migration and deep storage migration before updating your metadata/deep storage configs. These guides are targeted at single-server deployments that use the Derby metadata store and local deep storage. If you are already using a non-Derby metadata store in your single-server cluster, you can reuse the existing metadata store for the new cluster. These guides also provide information on migrating segments from local deep storage. A clustered deployment requires distributed deep storage like S3 or HDFS. If your single-server deployment was already using distributed deep storage, you can reuse the existing deep storage for the new cluster. Metadata storage In conf/druid/cluster/_common/common.runtime.properties, replace \"metadata.storage.*\" with the address of the machine that you will use as your metadata store: druid.metadata.storage.connector.connectURI druid.metadata.storage.connector.host In a production deployment, we recommend running a dedicated metadata store such as MySQL or PostgreSQL with replication, deployed separately from the Druid servers. The MySQL extension and PostgreSQL extension docs have instructions for extension configuration and initial database setup. Deep storage Druid relies on a distributed filesystem or large object (blob) store for data storage. The most commonly used deep storage implementations are S3 (popular for those on AWS) and HDFS (popular if you already have a Hadoop deployment). S3 In conf/druid/cluster/_common/common.runtime.properties, Add \"druid-s3-extensions\" to druid.extensions.loadList. Comment out the configurations for local storage under \"Deep Storage\" and \"Indexing service logs\". Uncomment and configure appropriate values in the \"For S3\" sections of \"Deep Storage\" and \"Indexing service logs\". After this, you should have made the following changes: druid.extensions.loadList=[\"druid-s3-extensions\"] #druid.storage.type=local #druid.storage.storageDirectory=var/druid/segments druid.storage.type=s3 druid.storage.bucket=your-bucket druid.storage.baseKey=druid/segments druid.s3.accessKey=... druid.s3.secretKey=... #druid.indexer.logs.type=file #druid.indexer.logs.directory=var/druid/indexing-logs druid.indexer.logs.type=s3 druid.indexer.logs.s3Bucket=your-bucket druid.indexer.logs.s3Prefix=druid/indexing-logs Please see the S3 extension documentation for more info. HDFS In conf/druid/cluster/_common/common.runtime.properties, Add \"druid-hdfs-storage\" to druid.extensions.loadList. Comment out the configurations for local storage under \"Deep Storage\" and \"Indexing service logs\". Uncomment and configure appropriate values in the \"For HDFS\" sections of \"Deep Storage\" and \"Indexing service logs\". After this, you should have made the following changes: druid.extensions.loadList=[\"druid-hdfs-storage\"] #druid.storage.type=local #druid.storage.storageDirectory=var/druid/segments druid.storage.type=hdfs druid.storage.storageDirectory=/druid/segments #druid.indexer.logs.type=file #druid.indexer.logs.directory=var/druid/indexing-logs druid.indexer.logs.type=hdfs druid.indexer.logs.directory=/druid/indexing-logs Also, Place your Hadoop configuration XMLs (core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml) on the classpath of your Druid processes. You can do this by copying them into conf/druid/cluster/_common/. Please see the HDFS extension documentation for more info. Configure for connecting to Hadoop (optional) If you will be loading data from a Hadoop cluster, then at this point you should configure Druid to be aware of your cluster: Update druid.indexer.task.hadoopWorkingPath in conf/druid/cluster/middleManager/runtime.properties to a path on HDFS that you'd like to use for temporary files required during the indexing process. druid.indexer.task.hadoopWorkingPath=/tmp/druid-indexing is a common choice. Place your Hadoop configuration XMLs (core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml) on the classpath of your Druid processes. You can do this by copying them into conf/druid/cluster/_common/core-site.xml, conf/druid/cluster/_common/hdfs-site.xml, and so on. Note that you don't need to use HDFS deep storage in order to load data from Hadoop. For example, if your cluster is running on Amazon Web Services, we recommend using S3 for deep storage even if you are loading data using Hadoop or Elastic MapReduce. For more info, please see the Hadoop-based ingestion page. Configure Zookeeper connection In a production cluster, we recommend using a dedicated ZK cluster in a quorum, deployed separately from the Druid servers. In conf/druid/cluster/_common/common.runtime.properties, set druid.zk.service.host to a connection string containing a comma separated list of host:port pairs, each corresponding to a ZooKeeper server in your ZK quorum. (e.g. \"127.0.0.1:4545\" or \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\") You can also choose to run ZK on the Master servers instead of having a dedicated ZK cluster. If doing so, we recommend deploying 3 Master servers so that you have a ZK quorum. Configuration Tuning Migrating from a Single-Server Deployment Master If you are using an example configuration from single-server deployment examples, these examples combine the Coordinator and Overlord processes into one combined process. The example configs under conf/druid/cluster/master/coordinator-overlord also combine the Coordinator and Overlord processes. You can copy your existing coordinator-overlord configs from the single-server deployment to conf/druid/cluster/master/coordinator-overlord. Data Suppose we are migrating from a single-server deployment that had 32 CPU and 256GB RAM. In the old deployment, the following configurations for Historicals and MiddleManagers were applied: Historical (Single-server) druid.processing.buffer.sizeBytes=500000000 druid.processing.numMergeBuffers=8 druid.processing.numThreads=31 MiddleManager (Single-server) druid.worker.capacity=8 druid.indexer.fork.property.druid.processing.numMergeBuffers=2 druid.indexer.fork.property.druid.processing.buffer.sizeBytes=100000000 druid.indexer.fork.property.druid.processing.numThreads=1 In the clustered deployment, we can choose a split factor (2 in this example), and deploy 2 Data servers with 16CPU and 128GB RAM each. The areas to scale are the following: Historical druid.processing.numThreads: Set to (num_cores - 1) based on the new hardware druid.processing.numMergeBuffers: Divide the old value from the single-server deployment by the split factor druid.processing.buffer.sizeBytes: Keep this unchanged MiddleManager: druid.worker.capacity: Divide the old value from the single-server deployment by the split factor druid.indexer.fork.property.druid.processing.numMergeBuffers: Keep this unchanged druid.indexer.fork.property.druid.processing.buffer.sizeBytes: Keep this unchanged druid.indexer.fork.property.druid.processing.numThreads: Keep this unchanged The resulting configs after the split: New Historical (on 2 Data servers) druid.processing.buffer.sizeBytes=500000000 druid.processing.numMergeBuffers=8 druid.processing.numThreads=31 New MiddleManager (on 2 Data servers) druid.worker.capacity=4 druid.indexer.fork.property.druid.processing.numMergeBuffers=2 druid.indexer.fork.property.druid.processing.buffer.sizeBytes=100000000 druid.indexer.fork.property.druid.processing.numThreads=1 Query You can copy your existing Broker and Router configs to the directories under conf/druid/cluster/query, no modifications are needed, as long as the new hardware is sized accordingly. Fresh deployment If you are using the example cluster described above: 1 Master server (m5.2xlarge) 2 Data servers (i3.4xlarge) 1 Query server (m5.2xlarge) The configurations under conf/druid/cluster have already been sized for this hardware and you do not need to make further modifications for general use cases. If you have chosen different hardware, the basic cluster tuning guide can help you size your configurations. Open ports (if using a firewall) If you're using a firewall or some other system that only allows traffic on specific ports, allow inbound connections on the following: Master Server 1527 (Derby metadata store; not needed if you are using a separate metadata store like MySQL or PostgreSQL) 2181 (ZooKeeper; not needed if you are using a separate ZooKeeper cluster) 8081 (Coordinator) 8090 (Overlord) Data Server 8083 (Historical) 8091, 8100–8199 (Druid Middle Manager; you may need higher than port 8199 if you have a very high druid.worker.capacity) Query Server 8082 (Broker) 8088 (Router, if used) In production, we recommend deploying ZooKeeper and your metadata store on their own dedicated hardware, rather than on the Master server. Start Master Server Copy the Druid distribution and your edited configurations to your Master server. If you have been editing the configurations on your local machine, you can use rsync to copy them: rsync -az apache-druid-{{DRUIDVERSION}}/ MASTER_SERVER:apache-druid-{{DRUIDVERSION}}/ No Zookeeper on Master From the distribution root, run the following command to start the Master server: bin/start-cluster-master-no-zk-server With Zookeeper on Master If you plan to run ZK on Master servers, first update conf/zoo.cfg to reflect how you plan to run ZK. Then, you can start the Master server processes together with ZK using: bin/start-cluster-master-with-zk-server In production, we also recommend running a ZooKeeper cluster on its own dedicated hardware. Start Data Server Copy the Druid distribution and your edited configurations to your Data servers. From the distribution root, run the following command to start the Data server: bin/start-cluster-data-server You can add more Data servers as needed. For clusters with complex resource allocation needs, you can break apart Historicals and MiddleManagers and scale the components individually. This also allows you take advantage of Druid's built-in MiddleManager autoscaling facility. Start Query Server Copy the Druid distribution and your edited configurations to your Query servers. From the distribution root, run the following command to start the Query server: bin/start-cluster-query-server You can add more Query servers as needed based on query load. If you increase the number of Query servers, be sure to adjust the connection pools on your Historicals and Tasks as described in the basic cluster tuning guide. Loading data Congratulations, you now have a Druid cluster! The next step is to learn about recommended ways to load data into Druid based on your use case. Read more about loading data. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-batch.html":{"url":"tutorials/tutorial-batch.html","title":"从本地文件摄入数据","keywords":"","body":" This tutorial demonstrates how to load data into Apache Druid from a file using Apache Druid's native batch ingestion feature. You initiate data loading in Druid by submitting an ingestion task spec to the Druid Overlord. You can write ingestion specs by hand or using the data loader built into the Druid console. The Quickstart shows you how to use the data loader to build an ingestion spec. For production environments, it's likely that you'll want to automate data ingestion. This tutorial starts by showing you how to submit an ingestion spec directly in the Druid console, and then introduces ways to ingest batch data that lend themselves to automation—from the command line and from a script. Loading data with a spec (via console) The Druid package includes the following sample native batch ingestion task spec at quickstart/tutorial/wikipedia-index.json, shown here for convenience, which has been configured to read the quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz input file: { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/tutorial/\", \"filter\" : \"wikiticker-2015-09-12-sampled.json.gz\" }, \"inputFormat\" : { \"type\": \"json\" }, \"appendToExisting\" : false }, \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 25000 } } } This spec creates a datasource named \"wikipedia\". From the Ingestion view, click the ellipses next to Tasks and choose Submit JSON task. This brings up the spec submission dialog where you can paste the spec above. Once the spec is submitted, wait a few moments for the data to load, after which you can query it. Loading data with a spec (via command line) For convenience, the Druid package includes a batch ingestion helper script at bin/post-index-task. This script will POST an ingestion task to the Druid Overlord and poll Druid until the data is available for querying. Run the following command from Druid package root: bin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://localhost:8081 You should see output like the following: Beginning indexing data for wikipedia Task started: index_wikipedia_2018-07-27T06:37:44.323Z Task log: http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/log Task status: http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/status Task index_wikipedia_2018-07-27T06:37:44.323Z still running... Task index_wikipedia_2018-07-27T06:37:44.323Z still running... Task finished with status: SUCCESS Completed indexing data for wikipedia. Now loading indexed data onto the cluster... wikipedia loading complete! You may now query your data Once the spec is submitted, you can follow the same instructions as above to wait for the data to load and then query it. Loading data without the script Let's briefly discuss how we would've submitted the ingestion task without using the script. You do not need to run these commands. To submit the task, POST it to Druid in a new terminal window from the apache-druid- directory: curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task Which will print the ID of the task if the submission was successful: {\"task\":\"index_wikipedia_2018-06-09T21:30:32.802Z\"} You can monitor the status of this task from the console as outlined above. Querying your data Once the data is loaded, please follow the query tutorial to run some example queries on the newly loaded data. Cleanup If you wish to go through any of the other ingestion tutorials, you will need to shut down the cluster and reset the cluster state by removing the contents of the var directory under the druid package, as the other tutorials will write to the same \"wikipedia\" datasource. Further reading For more information on loading batch data, please see the native batch ingestion documentation. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"tutorials/tutorial-kafka.html":{"url":"tutorials/tutorial-kafka.html","title":"从Kafka摄入数据","keywords":"","body":" Getting started This tutorial demonstrates how to load data into Apache Druid from a Kafka stream, using Druid's Kafka indexing service. For this tutorial, we'll assume you've already downloaded Druid as described in the quickstart using the micro-quickstart single-machine configuration and have it running on your local machine. You don't need to have loaded any data yet. Download and start Kafka Apache Kafka is a high throughput message bus that works well with Druid. For this tutorial, we will use Kafka 2.7.0. To download Kafka, issue the following commands in your terminal: curl -O https://archive.apache.org/dist/kafka/2.7.0/kafka_2.13-2.7.0.tgz tar -xzf kafka_2.13-2.7.0.tgz cd kafka_2.13-2.7.0 Start a Kafka broker by running the following command in a new terminal: ./bin/kafka-server-start.sh config/server.properties Run this command to create a Kafka topic called wikipedia, to which we'll send data: ./bin/kafka-topics.sh --create --topic wikipedia --bootstrap-server localhost:9092 Load data into Kafka Let's launch a producer for our topic and send some data! In your Druid directory, run the following command: cd quickstart/tutorial gunzip -c wikiticker-2015-09-12-sampled.json.gz > wikiticker-2015-09-12-sampled.json In your Kafka directory, run the following command, where {PATH_TO_DRUID} is replaced by the path to the Druid directory: export KAFKA_OPTS=\"-Dfile.encoding=UTF-8\" ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wikipedia The previous command posted sample events to the wikipedia Kafka topic. Now we will use Druid's Kafka indexing service to ingest messages from our newly created topic. Loading data with the data loader Navigate to localhost:8888 and click Load data in the console header. Select Apache Kafka and click Connect data. Enter localhost:9092 as the bootstrap server and wikipedia as the topic. Click Apply and make sure that the data you are seeing is correct. Once the data is located, you can click \"Next: Parse data\" to go to the next step. The data loader will try to automatically determine the correct parser for the data. In this case it will successfully determine json. Feel free to play around with different parser options to get a preview of how Druid will parse your data. With the json parser selected, click Next: Parse time to get to the step centered around determining your primary timestamp column. Druid's architecture requires a primary timestamp column (internally stored in a column called __time). If you do not have a timestamp in your data, select Constant value. In our example, the data loader will determine that the time column in our raw data is the only candidate that can be used as the primary time column. Click Next: ... twice to go past the Transform and Filter steps. You do not need to enter anything in these steps as applying ingestion time transforms and filters are out of scope for this tutorial. In the Configure schema step, you can configure which dimensions and metrics will be ingested into Druid. This is exactly what the data will appear like in Druid once it is ingested. Since our dataset is very small, go ahead and turn off Rollup by clicking on the switch and confirming the change. Once you are satisfied with the schema, click Next to go to the Partition step where you can fine tune how the data will be partitioned into segments. Here, you can adjust how the data will be split up into segments in Druid. Since this is a small dataset, there are no adjustments that need to be made in this step. Click Next: Tune to go to the tuning step. In the Tune step is it very important to set Use earliest offset to True since we want to consume the data from the start of the stream. There are no other changes that need to be made here, so click Next: Publish to go to the Publish step. Let's name this datasource wikipedia-kafka. Finally, click Next to review your spec. This is the spec you have constructed. Feel free to go back and make changes in previous steps to see how changes will update the spec. Similarly, you can also edit the spec directly and see it reflected in the previous steps. Once you are satisfied with the spec, click Submit and an ingestion task will be created. You will be taken to the task view with the focus on the newly created supervisor. The task view is set to auto refresh, wait until your supervisor launches a task. When a tasks starts running, it will also start serving the data that it is ingesting. Navigate to the Datasources view from the header. When the wikipedia-kafka datasource appears here it can be queried. Note: if the datasource does not appear after a minute you might have not set the supervisor to read from the start of the stream (in the Tune step). At this point, you can go to the Query view to run SQL queries against the datasource. Since this is a small dataset, you can simply run a SELECT * FROM \"wikipedia-kafka\" query to see your results. Check out the query tutorial to run some example queries on the newly loaded data. Submit a supervisor via the console In the console, click Submit supervisor to open the submit supervisor dialog. Paste in this spec and click Submit. { \"type\": \"kafka\", \"spec\" : { \"dataSchema\": { \"dataSource\": \"wikipedia\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia\", \"inputFormat\": { \"type\": \"json\" }, \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"localhost:9092\" } } } } This will start the supervisor that will in turn spawn some tasks that will start listening for incoming data. Submit a supervisor directly To start the service directly, we will need to submit a supervisor spec to the Druid overlord by running the following from the Druid package root: curl -XPOST -H'Content-Type: application/json' -d @quickstart/tutorial/wikipedia-kafka-supervisor.json http://localhost:8081/druid/indexer/v1/supervisor If the supervisor was successfully created, you will get a response containing the ID of the supervisor; in our case we should see {\"id\":\"wikipedia\"}. For more details about what's going on here, check out the Druid Kafka indexing service documentation. You can view the current supervisors and tasks in the Druid Console: http://localhost:8888/unified-console.md#tasks. Querying your data After data is sent to the Kafka stream, it is immediately available for querying. Please follow the query tutorial to run some example queries on the newly loaded data. Cleanup To go through any of the other ingestion tutorials, you will need to shut down the cluster and reset the cluster state by removing the contents of the var directory in the Druid home, as the other tutorials will write to the same \"wikipedia\" datasource. You should additionally clear out any Kafka state. Do so by shutting down the Kafka broker with CTRL-C before stopping ZooKeeper and the Druid services, and then deleting the Kafka log directory at /tmp/kafka-logs: rm -rf /tmp/kafka-logs Further reading For more information on loading data from Kafka streams, please see the Druid Kafka indexing service documentation. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-batch-hadoop.html":{"url":"tutorials/tutorial-batch-hadoop.html","title":"从Haddop摄入数据","keywords":"","body":" This tutorial shows you how to load data files into Apache Druid using a remote Hadoop cluster. For this tutorial, we'll assume that you've already completed the previous batch ingestion tutorial using Druid's native batch ingestion system and are using the micro-quickstart single-machine configuration as described in the quickstart. Install Docker This tutorial requires Docker to be installed on the tutorial machine. Once the Docker install is complete, please proceed to the next steps in the tutorial. Build the Hadoop docker image For this tutorial, we've provided a Dockerfile for a Hadoop 2.8.5 cluster, which we'll use to run the batch indexing task. This Dockerfile and related files are located at quickstart/tutorial/hadoop/docker. From the apache-druid- package root, run the following commands to build a Docker image named \"druid-hadoop-demo\" with version tag \"2.8.5\": cd quickstart/tutorial/hadoop/docker docker build -t druid-hadoop-demo:2.8.5 . This will start building the Hadoop image. Once the image build is done, you should see the message Successfully tagged druid-hadoop-demo:2.8.5 printed to the console. Setup the Hadoop docker cluster Create temporary shared directory We'll need a shared folder between the host and the Hadoop container for transferring some files. Let's create some folders under /tmp, we will use these later when starting the Hadoop container: mkdir -p /tmp/shared mkdir -p /tmp/shared/hadoop_xml Configure /etc/hosts On the host machine, add the following entry to /etc/hosts: 127.0.0.1 druid-hadoop-demo Start the Hadoop container Once the /tmp/shared folder has been created and the etc/hosts entry has been added, run the following command to start the Hadoop container. docker run -it -h druid-hadoop-demo --name druid-hadoop-demo -p 2049:2049 -p 2122:2122 -p 8020:8020 -p 8021:8021 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 9000:9000 -p 10020:10020 -p 19888:19888 -p 34455:34455 -p 49707:49707 -p 50010:50010 -p 50020:50020 -p 50030:50030 -p 50060:50060 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 51111:51111 -v /tmp/shared:/shared druid-hadoop-demo:2.8.5 /etc/bootstrap.sh -bash Once the container is started, your terminal will attach to a bash shell running inside the container: Starting sshd: [ OK ] 18/07/26 17:27:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [druid-hadoop-demo] druid-hadoop-demo: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-druid-hadoop-demo.out localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-druid-hadoop-demo.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-druid-hadoop-demo.out 18/07/26 17:27:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable starting yarn daemons starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-druid-hadoop-demo.out localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-druid-hadoop-demo.out starting historyserver, logging to /usr/local/hadoop/logs/mapred--historyserver-druid-hadoop-demo.out bash-4.1# The Unable to load native-hadoop library for your platform... using builtin-java classes where applicable warning messages can be safely ignored. Accessing the Hadoop container shell To open another shell to the Hadoop container, run the following command: docker exec -it druid-hadoop-demo bash Copy input data to the Hadoop container From the apache-druid- package root on the host, copy the quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz sample data to the shared folder: cp quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz /tmp/shared/wikiticker-2015-09-12-sampled.json.gz Setup HDFS directories In the Hadoop container's shell, run the following commands to setup the HDFS directories needed by this tutorial and copy the input data to HDFS. cd /usr/local/hadoop/bin ./hdfs dfs -mkdir /druid ./hdfs dfs -mkdir /druid/segments ./hdfs dfs -mkdir /quickstart ./hdfs dfs -chmod 777 /druid ./hdfs dfs -chmod 777 /druid/segments ./hdfs dfs -chmod 777 /quickstart ./hdfs dfs -chmod -R 777 /tmp ./hdfs dfs -chmod -R 777 /user ./hdfs dfs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz If you encounter namenode errors when running this command, the Hadoop container may not be finished initializing. When this occurs, wait a couple of minutes and retry the commands. Configure Druid to use Hadoop Some additional steps are needed to configure the Druid cluster for Hadoop batch indexing. Copy Hadoop configuration to Druid classpath From the Hadoop container's shell, run the following command to copy the Hadoop .xml configuration files to the shared folder: cp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml From the host machine, run the following, where {PATH_TO_DRUID} is replaced by the path to the Druid package. mkdir -p {PATH_TO_DRUID}/conf/druid/single-server/micro-quickstart/_common/hadoop-xml cp /tmp/shared/hadoop_xml/*.xml {PATH_TO_DRUID}/conf/druid/single-server/micro-quickstart/_common/hadoop-xml/ Update Druid segment and log storage In your favorite text editor, open conf/druid/single-server/micro-quickstart/_common/common.runtime.properties, and make the following edits: Disable local deep storage and enable HDFS deep storage # # Deep storage # # For local disk (only viable in a cluster if this is a network mount): #druid.storage.type=local #druid.storage.storageDirectory=var/druid/segments # For HDFS: druid.storage.type=hdfs druid.storage.storageDirectory=/druid/segments Disable local log storage and enable HDFS log storage # # Indexing service logs # # For local disk (only viable in a cluster if this is a network mount): #druid.indexer.logs.type=file #druid.indexer.logs.directory=var/druid/indexing-logs # For HDFS: druid.indexer.logs.type=hdfs druid.indexer.logs.directory=/druid/indexing-logs Restart Druid cluster Once the Hadoop .xml files have been copied to the Druid cluster and the segment/log storage configuration has been updated to use HDFS, the Druid cluster needs to be restarted for the new configurations to take effect. If the cluster is still running, CTRL-C to terminate the bin/start-micro-quickstart script, and re-run it to bring the Druid services back up. Load batch data We've included a sample of Wikipedia edits from September 12, 2015 to get you started. To load this data into Druid, you can submit an ingestion task pointing to the file. We've included a task that loads the wikiticker-2015-09-12-sampled.json.gz file included in the archive. Let's submit the wikipedia-index-hadoop.json task: bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop.json --url http://localhost:8081 Querying your data After the data load is complete, please follow the query tutorial to run some example queries on the newly loaded data. Cleanup This tutorial is only meant to be used together with the query tutorial. If you wish to go through any of the other tutorials, you will need to: Shut down the cluster and reset the cluster state by removing the contents of the var directory under the druid package. Revert the deep storage and task storage config back to local types in conf/druid/single-server/micro-quickstart/_common/common.runtime.properties Restart the cluster This is necessary because the other ingestion tutorials will write to the same \"wikipedia\" datasource, and later tutorials expect the cluster to use local deep storage. Example reverted config: # # Deep storage # # For local disk (only viable in a cluster if this is a network mount): druid.storage.type=local druid.storage.storageDirectory=var/druid/segments # For HDFS: #druid.storage.type=hdfs #druid.storage.storageDirectory=/druid/segments # # Indexing service logs # # For local disk (only viable in a cluster if this is a network mount): druid.indexer.logs.type=file druid.indexer.logs.directory=var/druid/indexing-logs # For HDFS: #druid.indexer.logs.type=hdfs #druid.indexer.logs.directory=/druid/indexing-logs Further reading For more information on loading batch data with Hadoop, please see the Hadoop batch ingestion documentation. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-query.html":{"url":"tutorials/tutorial-query.html","title":"查询数据","keywords":"","body":" This tutorial demonstrates how to query data in Apache Druid using SQL. It assumes that you've completed the Quickstart or one of the following tutorials, since we'll query datasources that you would have created by following one of them: Tutorial: Loading a file Tutorial: Loading stream data from Kafka Tutorial: Loading a file using Hadoop There are various ways to run Druid SQL queries: from the Druid console, using a command line utility and by posting the query by HTTP. We'll look at each of these. Query SQL from the Druid console The Druid console includes a view that makes it easier to build and test queries, and view their results. Start up the Druid cluster, if it's not already running, and open the Druid console in your web browser. Click Query from the header to open the Query view: You can always write queries directly in the edit pane, but the Query view also provides facilities to help you construct SQL queries, which we will use to generate a starter query. Expand the wikipedia datasource tree in the left pane. We'll create a query for the page dimension. Click page and then Show:page from the menu: A SELECT query appears in the query edit pane and immediately runs. However, in this case, the query returns no data, since by default the query filters for data from the last day, while our data is considerably older than that. Let's remove the filter. In the datasource tree, click __time and Remove Filter. Click Run to run the query. You should now see two columns of data, a page name and the count: Notice that the results are limited in the console to about a hundred, by default, due to the Smart query limit feature. This helps users avoid inadvertently running queries that return an excessive amount of data, possibly overwhelming their system. Let's edit the query directly and take a look at a few more query building features in the editor. Click in the query edit pane and make the following changes: Add a line after the first column, \"page\" and Start typing the name of a new column, \"countryName\". Notice that the autocomplete menu suggests column names, functions, keywords, and more. Choose \"countryName\" and add the new column to the GROUP BY clause as well, either by name or by reference to its position, 2. For readability, replace Count column name with Edits, since the COUNT() function actually returns the number of edits for the page. Make the same column name change in the ORDER BY clause as well. The COUNT() function is one of many functions available for use in Druid SQL queries. You can mouse over a function name in the autocomplete menu to see a brief description of a function. Also, you can find more information in the Druid documentation; for example, the COUNT() function is documented in Aggregation functions. The query should now be: SELECT \"page\", \"countryName\", COUNT(*) AS \"Edits\" FROM \"wikipedia\" GROUP BY 1, 2 ORDER BY \"Edits\" DESC When you run the query again, notice that we're getting the new dimension,countryName, but for most of the rows, its value is null. Let's show only rows with a countryName value. Click the countryName dimension in the left pane and choose the first filtering option. It's not exactly what we want, but we'll edit it by hand. The new WHERE clause should appear in your query. Modify the WHERE clause to exclude results that do not have a value for countryName: WHERE \"countryName\" IS NOT NULL Run the query again. You should now see the top edits by country: Under the covers, every Druid SQL query is translated into a query in the JSON-based Druid native query format before it runs on data nodes. You can view the native query for this query by clicking ... and Explain SQL Query. While you can use Druid SQL for most purposes, familiarity with native query is useful for composing complex queries and for troubleshooting performance issues. For more information, see Native queries. Another way to view the explain plan is by adding EXPLAIN PLAN FOR to the front of your query, as follows: EXPLAIN PLAN FOR SELECT \"page\", \"countryName\", COUNT(*) AS \"Edits\" FROM \"wikipedia\" WHERE \"countryName\" IS NOT NULL GROUP BY 1, 2 ORDER BY \"Edits\" DESC This is particularly useful when running queries from the command line or over HTTP. Finally, click ... and Edit context to see how you can add additional parameters controlling the execution of the query execution. In the field, enter query context options as JSON key-value pairs, as described in Context flags. That's it! We've built a simple query using some of the query builder features built into the Druid Console. The following sections provide a few more example queries you can try. Also, see Other ways to invoke SQL queries to learn how to run Druid SQL from the command line or over HTTP. More Druid SQL examples Here is a collection of queries to try out: Query over time SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY 1 General group by SELECT channel, page, SUM(added) FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY channel, page ORDER BY SUM(added) DESC Other ways to invoke SQL queries Query SQL via dsql For convenience, the Druid package includes a SQL command-line client, located at bin/dsql in the Druid package root. Let's now run bin/dsql; you should see the following prompt: Welcome to dsql, the command-line client for Druid SQL. Type \"\\h\" for help. dsql> To submit the query, paste it to the dsql prompt and press enter: dsql> SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10; ┌──────────────────────────────────────────────────────────┬───────┐ │ page │ Edits │ ├──────────────────────────────────────────────────────────┼───────┤ │ Wikipedia:Vandalismusmeldung │ 33 │ │ User:Cyde/List of candidates for speedy deletion/Subpage │ 28 │ │ Jeremy Corbyn │ 27 │ │ Wikipedia:Administrators' noticeboard/Incidents │ 21 │ │ Flavia Pennetta │ 20 │ │ Total Drama Presents: The Ridonculous Race │ 18 │ │ User talk:Dudeperson176123 │ 18 │ │ Wikipédia:Le Bistro/12 septembre 2015 │ 18 │ │ Wikipedia:In the news/Candidates │ 17 │ │ Wikipedia:Requests for page protection │ 17 │ └──────────────────────────────────────────────────────────┴───────┘ Retrieved 10 rows in 0.06s. Query SQL over HTTP You can submit queries directly to the Druid Broker over HTTP. The tutorial package includes an example file that contains the SQL query shown above at quickstart/tutorial/wikipedia-top-pages-sql.json. Let's submit that query to the Druid Broker: curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql The following results should be returned: [ { \"page\": \"Wikipedia:Vandalismusmeldung\", \"Edits\": 33 }, { \"page\": \"User:Cyde/List of candidates for speedy deletion/Subpage\", \"Edits\": 28 }, { \"page\": \"Jeremy Corbyn\", \"Edits\": 27 }, { \"page\": \"Wikipedia:Administrators' noticeboard/Incidents\", \"Edits\": 21 }, { \"page\": \"Flavia Pennetta\", \"Edits\": 20 }, { \"page\": \"Total Drama Presents: The Ridonculous Race\", \"Edits\": 18 }, { \"page\": \"User talk:Dudeperson176123\", \"Edits\": 18 }, { \"page\": \"Wikipédia:Le Bistro/12 septembre 2015\", \"Edits\": 18 }, { \"page\": \"Wikipedia:In the news/Candidates\", \"Edits\": 17 }, { \"page\": \"Wikipedia:Requests for page protection\", \"Edits\": 17 } ] Further reading See the Druid SQL documentation for more information on using Druid SQL queries. See the Queries documentation for more information on Druid native queries. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"tutorials/tutorial-rollup.html":{"url":"tutorials/tutorial-rollup.html","title":"预聚合","keywords":"","body":" Apache Druid can summarize raw data at ingestion time using a process we refer to as \"roll-up\". Roll-up is a first-level aggregation operation over a selected set of columns that reduces the size of stored data. This tutorial will demonstrate the effects of roll-up on an example dataset. For this tutorial, we'll assume you've already downloaded Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file and Tutorial: Querying data. Example data For this tutorial, we'll use a small sample of network flow event data, representing packet and byte counts for traffic from a source to a destination IP address that occurred within a particular second. {\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024} {\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133} {\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780} {\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289} {\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971} {\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204} {\"timestamp\":\"2018-01-02T21:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":38,\"bytes\":6289} {\"timestamp\":\"2018-01-02T21:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":123,\"bytes\":93999} {\"timestamp\":\"2018-01-02T21:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\",\"packets\":12,\"bytes\":2818} A file containing this sample input data is located at quickstart/tutorial/rollup-data.json. We'll ingest this data using the following ingestion task spec, located at quickstart/tutorial/rollup-index.json. { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"rollup-tutorial\", \"dimensionsSpec\" : { \"dimensions\" : [ \"srcIP\", \"dstIP\" ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"iso\" }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"week\", \"queryGranularity\" : \"minute\", \"intervals\" : [\"2018-01-01/2018-01-03\"], \"rollup\" : true } }, \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/tutorial\", \"filter\" : \"rollup-data.json\" }, \"inputFormat\" : { \"type\" : \"json\" }, \"appendToExisting\" : false }, \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 25000 } } } Roll-up has been enabled by setting \"rollup\" : true in the granularitySpec. Note that we have srcIP and dstIP defined as dimensions, a longSum metric is defined for the packets and bytes columns, and the queryGranularity has been defined as minute. We will see how these definitions are used after we load this data. Load the example data From the apache-druid- package root, run the following command: bin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081 After the script completes, we will query the data. Query the example data Let's run bin/dsql and issue a select * from \"rollup-tutorial\"; query to see what data was ingested. $ bin/dsql Welcome to dsql, the command-line client for Druid SQL. Type \"\\h\" for help. dsql> select * from \"rollup-tutorial\"; ┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐ │ __time │ bytes │ count │ dstIP │ packets │ srcIP │ ├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤ │ 2018-01-01T01:01:00.000Z │ 35937 │ 3 │ 2.2.2.2 │ 286 │ 1.1.1.1 │ │ 2018-01-01T01:02:00.000Z │ 366260 │ 2 │ 2.2.2.2 │ 415 │ 1.1.1.1 │ │ 2018-01-01T01:03:00.000Z │ 10204 │ 1 │ 2.2.2.2 │ 49 │ 1.1.1.1 │ │ 2018-01-02T21:33:00.000Z │ 100288 │ 2 │ 8.8.8.8 │ 161 │ 7.7.7.7 │ │ 2018-01-02T21:35:00.000Z │ 2818 │ 1 │ 8.8.8.8 │ 12 │ 7.7.7.7 │ └──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘ Retrieved 5 rows in 1.18s. dsql> Let's look at the three events in the original input data that occurred during 2018-01-01T01:01: {\"timestamp\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":20,\"bytes\":9024} {\"timestamp\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":255,\"bytes\":21133} {\"timestamp\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":11,\"bytes\":5780} These three rows have been \"rolled up\" into the following row: ┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐ │ __time │ bytes │ count │ dstIP │ packets │ srcIP │ ├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤ │ 2018-01-01T01:01:00.000Z │ 35937 │ 3 │ 2.2.2.2 │ 286 │ 1.1.1.1 │ └──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘ The input rows have been grouped by the timestamp and dimension columns {timestamp, srcIP, dstIP} with sum aggregations on the metric columns packets and bytes. Before the grouping occurs, the timestamps of the original input data are bucketed/floored by minute, due to the \"queryGranularity\":\"minute\" setting in the ingestion spec. Likewise, these two events that occurred during 2018-01-01T01:02 have been rolled up: {\"timestamp\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":38,\"bytes\":6289} {\"timestamp\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":377,\"bytes\":359971} ┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐ │ __time │ bytes │ count │ dstIP │ packets │ srcIP │ ├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤ │ 2018-01-01T01:02:00.000Z │ 366260 │ 2 │ 2.2.2.2 │ 415 │ 1.1.1.1 │ └──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘ For the last event recording traffic between 1.1.1.1 and 2.2.2.2, no roll-up took place, because this was the only event that occurred during 2018-01-01T01:03: {\"timestamp\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\",\"packets\":49,\"bytes\":10204} ┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐ │ __time │ bytes │ count │ dstIP │ packets │ srcIP │ ├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤ │ 2018-01-01T01:03:00.000Z │ 10204 │ 1 │ 2.2.2.2 │ 49 │ 1.1.1.1 │ └──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘ Note that the count metric shows how many rows in the original input data contributed to the final \"rolled up\" row. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-retention.html":{"url":"tutorials/tutorial-retention.html","title":"数据留存规则","keywords":"","body":" This tutorial demonstrates how to configure retention rules on a datasource to set the time intervals of data that will be retained or dropped. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file and Tutorial: Querying data. Load the example data For this tutorial, we'll be using the Wikipedia edits sample data, with an ingestion task spec that will create a separate segment for each hour in the input data. The ingestion spec can be found at quickstart/tutorial/retention-index.json. Let's submit that spec, which will create a datasource called retention-tutorial: bin/post-index-task --file quickstart/tutorial/retention-index.json --url http://localhost:8081 After the ingestion completes, go to http://localhost:8888/unified-console.html#datasources in a browser to access the Druid Console's datasource view. This view shows the available datasources and a summary of the retention rules for each datasource: Currently there are no rules set for the retention-tutorial datasource. Note that there are default rules for the cluster: load forever with 2 replicas in _default_tier. This means that all data will be loaded regardless of timestamp, and each segment will be replicated to two Historical processes in the default tier. In this tutorial, we will ignore the tiering and redundancy concepts for now. Let's view the segments for the retention-tutorial datasource by clicking the \"24 Segments\" link next to \"Fully Available\". The segments view (http://localhost:8888/unified-console.html#segments) provides information about what segments a datasource contains. The page shows that there are 24 segments, each one containing data for a specific hour of 2015-09-12: Set retention rules Suppose we want to drop data for the first 12 hours of 2015-09-12 and keep data for the later 12 hours of 2015-09-12. Go to the datasources view and click the blue pencil icon next to Cluster default: loadForever for the retention-tutorial datasource. A rule configuration window will appear: Now click the + New rule button twice. In the upper rule box, select Load and by interval, and then enter 2015-09-12T12:00:00.000Z/2015-09-13T00:00:00.000Z in field next to by interval. Replicas can remain at 2 in the _default_tier. In the lower rule box, select Drop and forever. The rules should look like this: Now click Next. The rule configuration process will ask for a user name and comment, for change logging purposes. You can enter tutorial for both. Now click Save. You can see the new rules in the datasources view: Give the cluster a few minutes to apply the rule change, and go to the segments view in the Druid Console. The segments for the first 12 hours of 2015-09-12 are now gone: The resulting retention rule chain is the following: loadByInterval 2015-09-12T12/2015-09-13 (12 hours) dropForever loadForever (default rule) The rule chain is evaluated from top to bottom, with the default rule chain always added at the bottom. The tutorial rule chain we just created loads data if it is within the specified 12 hour interval. If data is not within the 12 hour interval, the rule chain evaluates dropForever next, which will drop any data. The dropForever terminates the rule chain, effectively overriding the default loadForever rule, which will never be reached in this rule chain. Note that in this tutorial we defined a load rule on a specific interval. If instead you want to retain data based on how old it is (e.g., retain data that ranges from 3 months in the past to the present time), you would define a Period load rule instead. Further reading Load rules 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-update-data.html":{"url":"tutorials/tutorial-update-data.html","title":"更新数据","keywords":"","body":" This tutorial demonstrates how to update existing data, showing both overwrites and appends. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file, Tutorial: Querying data, and Tutorial: Rollup. Overwrite This section of the tutorial will cover how to overwrite an existing interval of data. Load initial data Let's load an initial data set which we will overwrite and append to. The spec we'll use for this tutorial is located at quickstart/tutorial/updates-init-index.json. This spec creates a datasource called updates-tutorial from the quickstart/tutorial/updates-data.json input file. Let's submit that task: bin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081 We have three initial rows containing an \"animal\" dimension and \"number\" metric: dsql> select * from \"updates-tutorial\"; ┌──────────────────────────┬──────────┬───────┬────────┐ │ __time │ animal │ count │ number │ ├──────────────────────────┼──────────┼───────┼────────┤ │ 2018-01-01T01:01:00.000Z │ tiger │ 1 │ 100 │ │ 2018-01-01T03:01:00.000Z │ aardvark │ 1 │ 42 │ │ 2018-01-01T03:01:00.000Z │ giraffe │ 1 │ 14124 │ └──────────────────────────┴──────────┴───────┴────────┘ Retrieved 3 rows in 1.42s. Overwrite the initial data To overwrite this data, we can submit another task for the same interval, but with different input data. The quickstart/tutorial/updates-overwrite-index.json spec will perform an overwrite on the updates-tutorial datasource. Note that this task reads input from quickstart/tutorial/updates-data2.json, and appendToExisting is set to false (indicating this is an overwrite). Let's submit that task: bin/post-index-task --file quickstart/tutorial/updates-overwrite-index.json --url http://localhost:8081 When Druid finishes loading the new segment from this overwrite task, the \"tiger\" row now has the value \"lion\", the \"aardvark\" row has a different number, and the \"giraffe\" row has been replaced. It may take a couple of minutes for the changes to take effect: dsql> select * from \"updates-tutorial\"; ┌──────────────────────────┬──────────┬───────┬────────┐ │ __time │ animal │ count │ number │ ├──────────────────────────┼──────────┼───────┼────────┤ │ 2018-01-01T01:01:00.000Z │ lion │ 1 │ 100 │ │ 2018-01-01T03:01:00.000Z │ aardvark │ 1 │ 9999 │ │ 2018-01-01T04:01:00.000Z │ bear │ 1 │ 111 │ └──────────────────────────┴──────────┴───────┴────────┘ Retrieved 3 rows in 0.02s. Combine old data with new data and overwrite Let's try appending some new data to the updates-tutorial datasource now. We will add the data from quickstart/tutorial/updates-data3.json. The quickstart/tutorial/updates-append-index.json task spec has been configured to read from the existing updates-tutorial datasource and the quickstart/tutorial/updates-data3.json file. The task will combine data from the two input sources, and then overwrite the original data with the new combined data. Let's submit that task: bin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081 When Druid finishes loading the new segment from this overwrite task, the new rows will have been added to the datasource. Note that roll-up occurred for the \"lion\" row: dsql> select * from \"updates-tutorial\"; ┌──────────────────────────┬──────────┬───────┬────────┐ │ __time │ animal │ count │ number │ ├──────────────────────────┼──────────┼───────┼────────┤ │ 2018-01-01T01:01:00.000Z │ lion │ 2 │ 400 │ │ 2018-01-01T03:01:00.000Z │ aardvark │ 1 │ 9999 │ │ 2018-01-01T04:01:00.000Z │ bear │ 1 │ 111 │ │ 2018-01-01T05:01:00.000Z │ mongoose │ 1 │ 737 │ │ 2018-01-01T06:01:00.000Z │ snake │ 1 │ 1234 │ │ 2018-01-01T07:01:00.000Z │ octopus │ 1 │ 115 │ └──────────────────────────┴──────────┴───────┴────────┘ Retrieved 6 rows in 0.02s. Append to the data Let's try another way of appending data. The quickstart/tutorial/updates-append-index2.json task spec reads input from quickstart/tutorial/updates-data4.json and will append its data to the updates-tutorial datasource. Note that appendToExisting is set to true in this spec. Let's submit that task: bin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081 When the new data is loaded, we can see two additional rows after \"octopus\". Note that the new \"bear\" row with number 222 has not been rolled up with the existing bear-111 row, because the new data is held in a separate segment. dsql> select * from \"updates-tutorial\"; ┌──────────────────────────┬──────────┬───────┬────────┐ │ __time │ animal │ count │ number │ ├──────────────────────────┼──────────┼───────┼────────┤ │ 2018-01-01T01:01:00.000Z │ lion │ 2 │ 400 │ │ 2018-01-01T03:01:00.000Z │ aardvark │ 1 │ 9999 │ │ 2018-01-01T04:01:00.000Z │ bear │ 1 │ 111 │ │ 2018-01-01T05:01:00.000Z │ mongoose │ 1 │ 737 │ │ 2018-01-01T06:01:00.000Z │ snake │ 1 │ 1234 │ │ 2018-01-01T07:01:00.000Z │ octopus │ 1 │ 115 │ │ 2018-01-01T04:01:00.000Z │ bear │ 1 │ 222 │ │ 2018-01-01T09:01:00.000Z │ falcon │ 1 │ 1241 │ └──────────────────────────┴──────────┴───────┴────────┘ Retrieved 8 rows in 0.02s. If we run a GroupBy query instead of a select *, we can see that the \"bear\" rows will group together at query time: dsql> select __time, animal, SUM(\"count\"), SUM(\"number\") from \"updates-tutorial\" group by __time, animal; ┌──────────────────────────┬──────────┬────────┬────────┐ │ __time │ animal │ EXPR$2 │ EXPR$3 │ ├──────────────────────────┼──────────┼────────┼────────┤ │ 2018-01-01T01:01:00.000Z │ lion │ 2 │ 400 │ │ 2018-01-01T03:01:00.000Z │ aardvark │ 1 │ 9999 │ │ 2018-01-01T04:01:00.000Z │ bear │ 2 │ 333 │ │ 2018-01-01T05:01:00.000Z │ mongoose │ 1 │ 737 │ │ 2018-01-01T06:01:00.000Z │ snake │ 1 │ 1234 │ │ 2018-01-01T07:01:00.000Z │ octopus │ 1 │ 115 │ │ 2018-01-01T09:01:00.000Z │ falcon │ 1 │ 1241 │ └──────────────────────────┴──────────┴────────┴────────┘ Retrieved 7 rows in 0.23s. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-compaction.html":{"url":"tutorials/tutorial-compaction.html","title":"数据文件合并","keywords":"","body":" This tutorial demonstrates how to compact existing segments into fewer but larger segments. Because there is some per-segment memory and processing overhead, it can sometimes be beneficial to reduce the total number of segments. Please check Segment size optimization for details. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file and Tutorial: Querying data. Load the initial data For this tutorial, we'll be using the Wikipedia edits sample data, with an ingestion task spec that will create 1-3 segments per hour in the input data. The ingestion spec can be found at quickstart/tutorial/compaction-init-index.json. Let's submit that spec, which will create a datasource called compaction-tutorial: bin/post-index-task --file quickstart/tutorial/compaction-init-index.json --url http://localhost:8081 Please note that maxRowsPerSegment in the ingestion spec is set to 1000. This is to generate multiple segments per hour and NOT recommended in production. It's 5000000 by default and may need to be adjusted to make your segments optimized. After the ingestion completes, go to http://localhost:8888/unified-console.html#datasources in a browser to see the new datasource in the Druid Console. Click the 51 segments link next to \"Fully Available\" for the compaction-tutorial datasource to view information about the datasource's segments: There will be 51 segments for this datasource, 1-3 segments per hour in the input data: Running a COUNT(*) query on this datasource shows that there are 39,244 rows: dsql> select count(*) from \"compaction-tutorial\"; ┌────────┐ │ EXPR$0 │ ├────────┤ │ 39244 │ └────────┘ Retrieved 1 row in 1.38s. Compact the data Let's now compact these 51 small segments. We have included a compaction task spec for this tutorial datasource at quickstart/tutorial/compaction-keep-granularity.json: { \"type\": \"compact\", \"dataSource\": \"compaction-tutorial\", \"interval\": \"2015-09-12/2015-09-13\", \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 25000 } } This will compact all segments for the interval 2015-09-12/2015-09-13 in the compaction-tutorial datasource. The parameters in the tuningConfig control how many segments will be present in the compacted set of segments. In this tutorial example, only one compacted segment will be created per hour, as each hour has less rows than the 5000000 maxRowsPerSegment (note that the total number of rows is 39244). Let's submit this task now: bin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json --url http://localhost:8081 After the task finishes, refresh the segments view. The original 51 segments will eventually be marked as \"unused\" by the Coordinator and removed, with the new compacted segments remaining. By default, the Druid Coordinator will not mark segments as unused until the Coordinator process has been up for at least 15 minutes, so you may see the old segment set and the new compacted set at the same time in the Druid Console, with 75 total segments: The new compacted segments have a more recent version than the original segments, so even when both sets of segments are shown in the Druid Console, queries will only read from the new compacted segments. Let's try running a COUNT(*) on compaction-tutorial again, where the row count should still be 39,244: dsql> select count(*) from \"compaction-tutorial\"; ┌────────┐ │ EXPR$0 │ ├────────┤ │ 39244 │ └────────┘ Retrieved 1 row in 1.30s. After the Coordinator has been running for at least 15 minutes, the segments view should show there are 24 segments, one per hour: Compact the data with new segment granularity The compaction task can also produce compacted segments with a granularity different from the granularity of the input segments. We have included a compaction task spec that will create DAY granularity segments at quickstart/tutorial/compaction-day-granularity.json: { \"type\": \"compact\", \"dataSource\": \"compaction-tutorial\", \"interval\": \"2015-09-12/2015-09-13\", \"segmentGranularity\": \"DAY\", \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 25000, \"forceExtendableShardSpecs\" : true } } Note that segmentGranularity is set to DAY in this compaction task spec. Let's submit this task now: bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081 It will take a bit of time before the Coordinator marks the old input segments as unused, so you may see an intermediate state with 25 total segments. Eventually, there will only be one DAY granularity segment: Further reading Task documentation Segment optimization 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"tutorials/tutorial-delete-data.html":{"url":"tutorials/tutorial-delete-data.html","title":"删除数据","keywords":"","body":" This tutorial demonstrates how to delete existing data. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. Load initial data In this tutorial, we will use the Wikipedia edits data, with an indexing spec that creates hourly segments. This spec is located at quickstart/tutorial/deletion-index.json, and it creates a datasource called deletion-tutorial. Let's load this initial data: bin/post-index-task --file quickstart/tutorial/deletion-index.json --url http://localhost:8081 When the load finishes, open http://localhost:8888/unified-console.md#datasources in a browser. How to permanently delete data Permanent deletion of a Druid segment has two steps: The segment must first be marked as \"unused\". This occurs when a user manually disables a segment through the Coordinator API. After segments have been marked as \"unused\", a Kill Task will delete any \"unused\" segments from Druid's metadata store as well as deep storage. Let's drop some segments now, by using the coordinator API to drop data by interval and segmentIds. Disable segments by interval Let's disable segments in a specified interval. This will mark all segments in the interval as \"unused\", but not remove them from deep storage. Let's disable segments in interval 2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z i.e. between hour 18 and 20. curl -X 'POST' -H 'Content-Type:application/json' -d '{ \"interval\" : \"2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z\" }' http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused After that command completes, you should see that the segment for hour 18 and 19 have been disabled: Note that the hour 18 and 19 segments are still present in deep storage: $ ls -l1 var/druid/segments/deletion-tutorial/ 2015-09-12T00:00:00.000Z_2015-09-12T01:00:00.000Z 2015-09-12T01:00:00.000Z_2015-09-12T02:00:00.000Z 2015-09-12T02:00:00.000Z_2015-09-12T03:00:00.000Z 2015-09-12T03:00:00.000Z_2015-09-12T04:00:00.000Z 2015-09-12T04:00:00.000Z_2015-09-12T05:00:00.000Z 2015-09-12T05:00:00.000Z_2015-09-12T06:00:00.000Z 2015-09-12T06:00:00.000Z_2015-09-12T07:00:00.000Z 2015-09-12T07:00:00.000Z_2015-09-12T08:00:00.000Z 2015-09-12T08:00:00.000Z_2015-09-12T09:00:00.000Z 2015-09-12T09:00:00.000Z_2015-09-12T10:00:00.000Z 2015-09-12T10:00:00.000Z_2015-09-12T11:00:00.000Z 2015-09-12T11:00:00.000Z_2015-09-12T12:00:00.000Z 2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z 2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z 2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z 2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z 2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z 2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z 2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z 2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z 2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z 2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z 2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z 2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z Disable segments by segment IDs Let's disable some segments by their segmentID. This will again mark the segments as \"unused\", but not remove them from deep storage. You can see the full segmentID for a segment from UI as explained below. In the segments view, click the arrow on the left side of one of the remaining segments to expand the segment entry: The top of the info box shows the full segment ID, e.g. deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-02-28T01:11:51.606Z for the segment of hour 14. Let's disable the hour 13 and 14 segments by sending a POST request to the Coordinator with this payload { \"segmentIds\": [ \"deletion-tutorial_2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z_2019-05-01T17:38:46.961Z\", \"deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-05-01T17:38:46.961Z\" ] } This payload json has been provided at quickstart/tutorial/deletion-disable-segments.json. Submit the POST request to Coordinator like this: curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-disable-segments.json http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused After that command completes, you should see that the segments for hour 13 and 14 have been disabled: Note that the hour 13 and 14 segments are still in deep storage: $ ls -l1 var/druid/segments/deletion-tutorial/ 2015-09-12T00:00:00.000Z_2015-09-12T01:00:00.000Z 2015-09-12T01:00:00.000Z_2015-09-12T02:00:00.000Z 2015-09-12T02:00:00.000Z_2015-09-12T03:00:00.000Z 2015-09-12T03:00:00.000Z_2015-09-12T04:00:00.000Z 2015-09-12T04:00:00.000Z_2015-09-12T05:00:00.000Z 2015-09-12T05:00:00.000Z_2015-09-12T06:00:00.000Z 2015-09-12T06:00:00.000Z_2015-09-12T07:00:00.000Z 2015-09-12T07:00:00.000Z_2015-09-12T08:00:00.000Z 2015-09-12T08:00:00.000Z_2015-09-12T09:00:00.000Z 2015-09-12T09:00:00.000Z_2015-09-12T10:00:00.000Z 2015-09-12T10:00:00.000Z_2015-09-12T11:00:00.000Z 2015-09-12T11:00:00.000Z_2015-09-12T12:00:00.000Z 2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z 2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z 2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z 2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z 2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z 2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z 2015-09-12T18:00:00.000Z_2015-09-12T19:00:00.000Z 2015-09-12T19:00:00.000Z_2015-09-12T20:00:00.000Z 2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z 2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z 2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z 2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z Run a kill task Now that we have disabled some segments, we can submit a Kill Task, which will delete the disabled segments from metadata and deep storage. A Kill Task spec has been provided at quickstart/tutorial/deletion-kill.json. Submit this task to the Overlord with the following command: curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8081/druid/indexer/v1/task After this task completes, you can see that the disabled segments have now been removed from deep storage: $ ls -l1 var/druid/segments/deletion-tutorial/ 2015-09-12T12:00:00.000Z_2015-09-12T13:00:00.000Z 2015-09-12T15:00:00.000Z_2015-09-12T16:00:00.000Z 2015-09-12T16:00:00.000Z_2015-09-12T17:00:00.000Z 2015-09-12T17:00:00.000Z_2015-09-12T18:00:00.000Z 2015-09-12T20:00:00.000Z_2015-09-12T21:00:00.000Z 2015-09-12T21:00:00.000Z_2015-09-12T22:00:00.000Z 2015-09-12T22:00:00.000Z_2015-09-12T23:00:00.000Z 2015-09-12T23:00:00.000Z_2015-09-13T00:00:00.000Z 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-ingestion-spec.html":{"url":"tutorials/tutorial-ingestion-spec.html","title":"编写数据摄入规则","keywords":"","body":" This tutorial will guide the reader through the process of defining an ingestion spec, pointing out key considerations and guidelines. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file, Tutorial: Querying data, and Tutorial: Rollup. Example data Suppose we have the following network flow data: srcIP: IP address of sender srcPort: Port of sender dstIP: IP address of receiver dstPort: Port of receiver protocol: IP protocol number packets: number of packets transmitted bytes: number of bytes transmitted cost: the cost of sending the traffic {\"ts\":\"2018-01-01T01:01:35Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":10, \"bytes\":1000, \"cost\": 1.4} {\"ts\":\"2018-01-01T01:01:51Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":20, \"bytes\":2000, \"cost\": 3.1} {\"ts\":\"2018-01-01T01:01:59Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":2000, \"dstPort\":3000, \"protocol\": 6, \"packets\":30, \"bytes\":3000, \"cost\": 0.4} {\"ts\":\"2018-01-01T01:02:14Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":40, \"bytes\":4000, \"cost\": 7.9} {\"ts\":\"2018-01-01T01:02:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":50, \"bytes\":5000, \"cost\": 10.2} {\"ts\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":60, \"bytes\":6000, \"cost\": 4.3} {\"ts\":\"2018-01-01T02:33:14Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":100, \"bytes\":10000, \"cost\": 22.4} {\"ts\":\"2018-01-01T02:33:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":200, \"bytes\":20000, \"cost\": 34.5} {\"ts\":\"2018-01-01T02:35:45Z\",\"srcIP\":\"7.7.7.7\", \"dstIP\":\"8.8.8.8\", \"srcPort\":4000, \"dstPort\":5000, \"protocol\": 17, \"packets\":300, \"bytes\":30000, \"cost\": 46.3} Save the JSON contents above into a file called ingestion-tutorial-data.json in quickstart/. Let's walk through the process of defining an ingestion spec that can load this data. For this tutorial, we will be using the native batch indexing task. When using other task types, some aspects of the ingestion spec will differ, and this tutorial will point out such areas. Defining the schema The core element of a Druid ingestion spec is the dataSchema. The dataSchema defines how to parse input data into a set of columns that will be stored in Druid. Let's start with an empty dataSchema and add fields to it as we progress through the tutorial. Create a new file called ingestion-tutorial-index.json in quickstart/ with the following contents: \"dataSchema\" : {} We will be making successive edits to this ingestion spec as we progress through the tutorial. Datasource name The datasource name is specified by the dataSource parameter in the dataSchema. \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", } Let's call the tutorial datasource ingestion-tutorial. Time column The dataSchema needs to know how to extract the main timestamp field from the input data. The timestamp column in our input data is named \"ts\", containing ISO 8601 timestamps, so let's add a timestampSpec with that information to the dataSchema: \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" } } Column types Now that we've defined the time column, let's look at definitions for other columns. Druid supports the following column types: String, Long, Float, Double. We will see how these are used in the following sections. Before we move on to how we define our other non-time columns, let's discuss rollup first. Rollup When ingesting data, we must consider whether we wish to use rollup or not. If rollup is enabled, we will need to separate the input columns into two categories, \"dimensions\" and \"metrics\". \"Dimensions\" are the grouping columns for rollup, while \"metrics\" are the columns that will be aggregated. If rollup is disabled, then all columns are treated as \"dimensions\" and no pre-aggregation occurs. For this tutorial, let's enable rollup. This is specified with a granularitySpec on the dataSchema. \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"granularitySpec\" : { \"rollup\" : true } } Choosing dimensions and metrics For this example dataset, the following is a sensible split for \"dimensions\" and \"metrics\": Dimensions: srcIP, srcPort, dstIP, dstPort, protocol Metrics: packets, bytes, cost The dimensions here are a group of properties that identify a unidirectional flow of IP traffic, while the metrics represent facts about the IP traffic flow specified by a dimension grouping. Let's look at how to define these dimensions and metrics within the ingestion spec. Dimensions Dimensions are specified with a dimensionsSpec inside the dataSchema. \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"granularitySpec\" : { \"rollup\" : true } } Each dimension has a name and a type, where type can be \"long\", \"float\", \"double\", or \"string\". Note that srcIP is a \"string\" dimension; for string dimensions, it is enough to specify just a dimension name, since \"string\" is the default dimension type. Also note that protocol is a numeric value in the input data, but we are ingesting it as a \"string\" column; Druid will coerce the input longs to strings during ingestion. Strings vs. Numerics Should a numeric input be ingested as a numeric dimension or as a string dimension? Numeric dimensions have the following pros/cons relative to String dimensions: Pros: Numeric representation can result in smaller column sizes on disk and lower processing overhead when reading values from the column Cons: Numeric dimensions do not have indices, so filtering on them will often be slower than filtering on an equivalent String dimension (which has bitmap indices) Metrics Metrics are specified with a metricsSpec inside the dataSchema: \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"rollup\" : true } } When defining a metric, it is necessary to specify what type of aggregation should be performed on that column during rollup. Here we have defined long sum aggregations on the two long metric columns, packets and bytes, and a double sum aggregation for the cost column. Note that the metricsSpec is on a different nesting level than dimensionSpec or parseSpec; it belongs on the same nesting level as parser within the dataSchema. Note that we have also defined a count aggregator. The count aggregator will track how many rows in the original input data contributed to a \"rolled up\" row in the final ingested data. No rollup If we were not using rollup, all columns would be specified in the dimensionsSpec, e.g.: \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" }, { \"name\" : \"packets\", \"type\" : \"long\" }, { \"name\" : \"bytes\", \"type\" : \"long\" }, { \"name\" : \"srcPort\", \"type\" : \"double\" } ] }, Define granularities At this point, we are done defining the parser and metricsSpec within the dataSchema and we are almost done writing the ingestion spec. There are some additional properties we need to set in the granularitySpec: Type of granularitySpec: uniform and arbitrary are the two supported types. For this tutorial, we will use a uniform granularity spec, where all segments have uniform interval sizes (for example, all segments cover an hour's worth of data). The segment granularity: what size of time interval should a single segment contain data for? e.g., DAY, WEEK The bucketing granularity of the timestamps in the time column (referred to as queryGranularity) Segment granularity Segment granularity is configured by the segmentGranularity property in the granularitySpec. For this tutorial, we'll create hourly segments: \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"rollup\" : true } } Our input data has events from two separate hours, so this task will generate two segments. Query granularity The query granularity is configured by the queryGranularity property in the granularitySpec. For this tutorial, let's use minute granularity: \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"queryGranularity\" : \"MINUTE\", \"rollup\" : true } } To see the effect of the query granularity, let's look at this row from the raw input data: {\"ts\":\"2018-01-01T01:03:29Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":60, \"bytes\":6000, \"cost\": 4.3} When this row is ingested with minute queryGranularity, Druid will floor the row's timestamp to minute buckets: {\"ts\":\"2018-01-01T01:03:00Z\",\"srcIP\":\"1.1.1.1\", \"dstIP\":\"2.2.2.2\", \"srcPort\":5000, \"dstPort\":7000, \"protocol\": 6, \"packets\":60, \"bytes\":6000, \"cost\": 4.3} Define an interval (batch only) For batch tasks, it is necessary to define a time interval. Input rows with timestamps outside of the time interval will not be ingested. The interval is also specified in the granularitySpec: \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"queryGranularity\" : \"MINUTE\", \"intervals\" : [\"2018-01-01/2018-01-02\"], \"rollup\" : true } } Define the task type We've now finished defining our dataSchema. The remaining steps are to place the dataSchema we created into an ingestion task spec, and specify the input source. The dataSchema is shared across all task types, but each task type has its own specification format. For this tutorial, we will use the native batch ingestion task: { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"queryGranularity\" : \"MINUTE\", \"intervals\" : [\"2018-01-01/2018-01-02\"], \"rollup\" : true } } } } Define the input source Now let's define our input source, which is specified in an ioConfig object. Each task type has its own type of ioConfig. To read input data, we need to specify an inputSource. The example netflow data we saved earlier needs to be read from a local file, which is configured below: \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/\", \"filter\" : \"ingestion-tutorial-data.json\" } } Define the format of the data Since our input data is represented as JSON strings, we'll use a inputFormat to json format: \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/\", \"filter\" : \"ingestion-tutorial-data.json\" }, \"inputFormat\" : { \"type\" : \"json\" } } { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"queryGranularity\" : \"MINUTE\", \"intervals\" : [\"2018-01-01/2018-01-02\"], \"rollup\" : true } }, \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/\", \"filter\" : \"ingestion-tutorial-data.json\" }, \"inputFormat\" : { \"type\" : \"json\" } } } } Additional tuning Each ingestion task has a tuningConfig section that allows users to tune various ingestion parameters. As an example, let's add a tuningConfig that sets a target segment size for the native batch ingestion task: \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000 } Note that each ingestion task has its own type of tuningConfig. Final spec We've finished defining the ingestion spec, it should now look like the following: { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"ingestion-tutorial\", \"timestampSpec\" : { \"format\" : \"iso\", \"column\" : \"ts\" }, \"dimensionsSpec\" : { \"dimensions\": [ \"srcIP\", { \"name\" : \"srcPort\", \"type\" : \"long\" }, { \"name\" : \"dstIP\", \"type\" : \"string\" }, { \"name\" : \"dstPort\", \"type\" : \"long\" }, { \"name\" : \"protocol\", \"type\" : \"string\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"packets\", \"fieldName\" : \"packets\" }, { \"type\" : \"longSum\", \"name\" : \"bytes\", \"fieldName\" : \"bytes\" }, { \"type\" : \"doubleSum\", \"name\" : \"cost\", \"fieldName\" : \"cost\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"HOUR\", \"queryGranularity\" : \"MINUTE\", \"intervals\" : [\"2018-01-01/2018-01-02\"], \"rollup\" : true } }, \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/\", \"filter\" : \"ingestion-tutorial-data.json\" }, \"inputFormat\" : { \"type\" : \"json\" } }, \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000 } } } Submit the task and query the data From the apache-druid- package root, run the following command: bin/post-index-task --file quickstart/ingestion-tutorial-index.json --url http://localhost:8081 After the script completes, we will query the data. Let's run bin/dsql and issue a select * from \"ingestion-tutorial\"; query to see what data was ingested. $ bin/dsql Welcome to dsql, the command-line client for Druid SQL. Type \"\\h\" for help. dsql> select * from \"ingestion-tutorial\"; ┌──────────────────────────┬───────┬──────┬───────┬─────────┬─────────┬─────────┬──────────┬─────────┬─────────┐ │ __time │ bytes │ cost │ count │ dstIP │ dstPort │ packets │ protocol │ srcIP │ srcPort │ ├──────────────────────────┼───────┼──────┼───────┼─────────┼─────────┼─────────┼──────────┼─────────┼─────────┤ │ 2018-01-01T01:01:00.000Z │ 6000 │ 4.9 │ 3 │ 2.2.2.2 │ 3000 │ 60 │ 6 │ 1.1.1.1 │ 2000 │ │ 2018-01-01T01:02:00.000Z │ 9000 │ 18.1 │ 2 │ 2.2.2.2 │ 7000 │ 90 │ 6 │ 1.1.1.1 │ 5000 │ │ 2018-01-01T01:03:00.000Z │ 6000 │ 4.3 │ 1 │ 2.2.2.2 │ 7000 │ 60 │ 6 │ 1.1.1.1 │ 5000 │ │ 2018-01-01T02:33:00.000Z │ 30000 │ 56.9 │ 2 │ 8.8.8.8 │ 5000 │ 300 │ 17 │ 7.7.7.7 │ 4000 │ │ 2018-01-01T02:35:00.000Z │ 30000 │ 46.3 │ 1 │ 8.8.8.8 │ 5000 │ 300 │ 17 │ 7.7.7.7 │ 4000 │ └──────────────────────────┴───────┴──────┴───────┴─────────┴─────────┴─────────┴──────────┴─────────┴─────────┘ Retrieved 5 rows in 0.12s. dsql> 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-transform-spec.html":{"url":"tutorials/tutorial-transform-spec.html","title":"转换摄入数据","keywords":"","body":" This tutorial will demonstrate how to use transform specs to filter and transform input data during ingestion. For this tutorial, we'll assume you've already downloaded Apache Druid as described in the single-machine quickstart and have it running on your local machine. It will also be helpful to have finished Tutorial: Loading a file and Tutorial: Querying data. Sample data We've included sample data for this tutorial at quickstart/tutorial/transform-data.json, reproduced here for convenience: {\"timestamp\":\"2018-01-01T07:01:35Z\",\"animal\":\"octopus\", \"location\":1, \"number\":100} {\"timestamp\":\"2018-01-01T05:01:35Z\",\"animal\":\"mongoose\", \"location\":2,\"number\":200} {\"timestamp\":\"2018-01-01T06:01:35Z\",\"animal\":\"snake\", \"location\":3, \"number\":300} {\"timestamp\":\"2018-01-01T01:01:35Z\",\"animal\":\"lion\", \"location\":4, \"number\":300} Load data with transform specs We will ingest the sample data using the following spec, which demonstrates the use of transform specs: { \"type\" : \"index_parallel\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"transform-tutorial\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"iso\" }, \"dimensionsSpec\" : { \"dimensions\" : [ \"animal\", { \"name\": \"location\", \"type\": \"long\" } ] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"longSum\", \"name\" : \"number\", \"fieldName\" : \"number\" }, { \"type\" : \"longSum\", \"name\" : \"triple-number\", \"fieldName\" : \"triple-number\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"week\", \"queryGranularity\" : \"minute\", \"intervals\" : [\"2018-01-01/2018-01-03\"], \"rollup\" : true }, \"transformSpec\": { \"transforms\": [ { \"type\": \"expression\", \"name\": \"animal\", \"expression\": \"concat('super-', animal)\" }, { \"type\": \"expression\", \"name\": \"triple-number\", \"expression\": \"number * 3\" } ], \"filter\": { \"type\":\"or\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"animal\", \"value\": \"super-mongoose\" }, { \"type\": \"selector\", \"dimension\": \"triple-number\", \"value\": \"300\" }, { \"type\": \"selector\", \"dimension\": \"location\", \"value\": \"3\" } ] } } }, \"ioConfig\" : { \"type\" : \"index_parallel\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"quickstart/tutorial\", \"filter\" : \"transform-data.json\" }, \"inputFormat\" : { \"type\" :\"json\" }, \"appendToExisting\" : false }, \"tuningConfig\" : { \"type\" : \"index_parallel\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 25000 } } } In the transform spec, we have two expression transforms: super-animal: prepends \"super-\" to the values in the animal column. This will override the animal column with the transformed version, since the transform's name is animal. triple-number: multiplies the number column by 3. This will create a new triple-number column. Note that we are ingesting both the original and the transformed column. Additionally, we have an OR filter with three clauses: super-animal values that match \"super-mongoose\" triple-number values that match 300 location values that match 3 This filter selects the first 3 rows, and it will exclude the final \"lion\" row in the input data. Note that the filter is applied after the transformation. Let's submit this task now, which has been included at quickstart/tutorial/transform-index.json: bin/post-index-task --file quickstart/tutorial/transform-index.json --url http://localhost:8081 Query the transformed data Let's run bin/dsql and issue a select * from \"transform-tutorial\"; query to see what was ingested: dsql> select * from \"transform-tutorial\"; ┌──────────────────────────┬────────────────┬───────┬──────────┬────────┬───────────────┐ │ __time │ animal │ count │ location │ number │ triple-number │ ├──────────────────────────┼────────────────┼───────┼──────────┼────────┼───────────────┤ │ 2018-01-01T05:01:00.000Z │ super-mongoose │ 1 │ 2 │ 200 │ 600 │ │ 2018-01-01T06:01:00.000Z │ super-snake │ 1 │ 3 │ 300 │ 900 │ │ 2018-01-01T07:01:00.000Z │ super-octopus │ 1 │ 1 │ 100 │ 300 │ └──────────────────────────┴────────────────┴───────┴──────────┴────────┴───────────────┘ Retrieved 3 rows in 0.03s. The \"lion\" row has been discarded, the animal column has been transformed, and we have both the original and transformed number column. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"tutorials/tutorial-kerberos-hadoop.html":{"url":"tutorials/tutorial-kerberos-hadoop.html","title":"设置Kerberos认证的HDFS作为深度存储","keywords":"","body":" Hadoop Setup Following are the configurations files required to be copied over to Druid conf folders: For HDFS as a deep storage, hdfs-site.xml, core-site.xml For ingestion, mapred-site.xml, yarn-site.xml HDFS Folders and permissions Choose any folder name for the druid deep storage, for example 'druid' Create the folder in hdfs under the required parent folder. For example, hdfs dfs -mkdir /druid OR hdfs dfs -mkdir /apps/druid Give druid processes appropriate permissions for the druid processes to access this folder. This would ensure that druid is able to create necessary folders like data and indexing_log in HDFS. For example, if druid processes run as user 'root', then hdfs dfs -chown root:root /apps/druid OR hdfs dfs -chmod 777 /apps/druid Druid creates necessary sub-folders to store data and index under this newly created folder. Druid Setup Edit common.runtime.properties at conf/druid/_common/common.runtime.properties to include the HDFS properties. Folders used for the location are same as the ones used for example above. common.runtime.properties # Deep storage # # For HDFS: druid.storage.type=hdfs druid.storage.storageDirectory=/druid/segments # OR # druid.storage.storageDirectory=/apps/druid/segments # # Indexing service logs # # For HDFS: druid.indexer.logs.type=hdfs druid.indexer.logs.directory=/druid/indexing-logs # OR # druid.storage.storageDirectory=/apps/druid/indexing-logs Note: Comment out Local storage and S3 Storage parameters in the file Also include hdfs-storage core extension to conf/druid/_common/common.runtime.properties # # Extensions # druid.extensions.directory=dist/druid/extensions druid.extensions.hadoopDependenciesDir=dist/druid/hadoop-dependencies druid.extensions.loadList=[\"mysql-metadata-storage\", \"druid-hdfs-storage\", \"druid-kerberos\"] Hadoop Jars Ensure that Druid has necessary jars to support the Hadoop version. Find the hadoop version using command, hadoop version In case there is other software used with hadoop, like WanDisco, ensure that the necessary libraries are available add the requisite extensions to druid.extensions.loadlist in conf/druid/_common/common.runtime.properties Kerberos setup Create a headless keytab which would have access to the druid data and index. Edit conf/druid/_common/common.runtime.properties and add the following properties: druid.hadoop.security.kerberos.principal druid.hadoop.security.kerberos.keytab For example druid.hadoop.security.kerberos.principal=hdfs-test@EXAMPLE.IO druid.hadoop.security.kerberos.keytab=/etc/security/keytabs/hdfs.headless.keytab Restart Druid Services With the above changes, restart Druid. This would ensure that Druid works with Kerberized Hadoop 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"design/architecture.html":{"url":"design/architecture.html","title":"整体架构","keywords":"","body":" Druid has a multi-process, distributed architecture that is designed to be cloud-friendly and easy to operate. Each Druid process type can be configured and scaled independently, giving you maximum flexibility over your cluster. This design also provides enhanced fault tolerance: an outage of one component will not immediately affect other components. Processes and Servers Druid has several process types, briefly described below: Coordinator processes manage data availability on the cluster. Overlord processes control the assignment of data ingestion workloads. Broker processes handle queries from external clients. Router processes are optional processes that can route requests to Brokers, Coordinators, and Overlords. Historical processes store queryable data. MiddleManager processes are responsible for ingesting data. Druid processes can be deployed any way you like, but for ease of deployment we suggest organizing them into three server types: Master, Query, and Data. Master: Runs Coordinator and Overlord processes, manages data availability and ingestion. Query: Runs Broker and optional Router processes, handles queries from external clients. Data: Runs Historical and MiddleManager processes, executes ingestion workloads and stores all queryable data. For more details on process and server organization, please see Druid Processes and Servers. External dependencies In addition to its built-in process types, Druid also has three external dependencies. These are intended to be able to leverage existing infrastructure, where present. Deep storage Shared file storage accessible by every Druid server. In a clustered deployment, this is typically going to be a distributed object store like S3 or HDFS, or a network mounted filesystem. In a single-server deployment, this is typically going to be local disk. Druid uses deep storage to store any data that has been ingested into the system. Druid uses deep storage only as a backup of your data and as a way to transfer data in the background between Druid processes. To respond to queries, Historical processes do not read from deep storage, but instead read prefetched segments from their local disks before any queries are served. This means that Druid never needs to access deep storage during a query, helping it offer the best query latencies possible. It also means that you must have enough disk space both in deep storage and across your Historical processes for the data you plan to load. Deep storage is an important part of Druid's elastic, fault-tolerant design. Druid can bootstrap from deep storage even if every single data server is lost and re-provisioned. For more details, please see the Deep storage page. Metadata storage The metadata storage holds various shared system metadata such as segment usage information and task information. In a clustered deployment, this is typically going to be a traditional RDBMS like PostgreSQL or MySQL. In a single-server deployment, it is typically going to be a locally-stored Apache Derby database. For more details, please see the Metadata storage page. ZooKeeper Used for internal service discovery, coordination, and leader election. For more details, please see the ZooKeeper page. Architecture diagram The following diagram shows how queries and data flow through this architecture, using the suggested Master/Query/Data server organization: Storage design Datasources and segments Druid data is stored in \"datasources\", which are similar to tables in a traditional RDBMS. Each datasource is partitioned by time and, optionally, further partitioned by other attributes. Each time range is called a \"chunk\" (for example, a single day, if your datasource is partitioned by day). Within a chunk, data is partitioned into one or more \"segments\". Each segment is a single file, typically comprising up to a few million rows of data. Since segments are organized into time chunks, it's sometimes helpful to think of segments as living on a timeline like the following: A datasource may have anywhere from just a few segments, up to hundreds of thousands and even millions of segments. Each segment starts life off being created on a MiddleManager, and at that point, is mutable and uncommitted. The segment building process includes the following steps, designed to produce a data file that is compact and supports fast queries: Conversion to columnar format Indexing with bitmap indexes Compression using various algorithms Dictionary encoding with id storage minimization for String columns Bitmap compression for bitmap indexes Type-aware compression for all columns Periodically, segments are committed and published. At this point, they are written to deep storage, become immutable, and move from MiddleManagers to the Historical processes. An entry about the segment is also written to the metadata store. This entry is a self-describing bit of metadata about the segment, including things like the schema of the segment, its size, and its location on deep storage. These entries are what the Coordinator uses to know what data should be available on the cluster. For details on the segment file format, please see segment files. For details on modeling your data in Druid, see schema design. Indexing and handoff Indexing is the mechanism by which new segments are created, and handoff is the mechanism by which they are published and begin being served by Historical processes. The mechanism works like this on the indexing side: An indexing task starts running and building a new segment. It must determine the identifier of the segment before it starts building it. For a task that is appending (like a Kafka task, or an index task in append mode) this will be done by calling an \"allocate\" API on the Overlord to potentially add a new partition to an existing set of segments. For a task that is overwriting (like a Hadoop task, or an index task not in append mode) this is done by locking an interval and creating a new version number and new set of segments. If the indexing task is a realtime task (like a Kafka task) then the segment is immediately queryable at this point. It's available, but unpublished. When the indexing task has finished reading data for the segment, it pushes it to deep storage and then publishes it by writing a record into the metadata store. If the indexing task is a realtime task, at this point it waits for a Historical process to load the segment. If the indexing task is not a realtime task, it exits immediately. And like this on the Coordinator / Historical side: The Coordinator polls the metadata store periodically (by default, every 1 minute) for newly published segments. When the Coordinator finds a segment that is published and used, but unavailable, it chooses a Historical process to load that segment and instructs that Historical to do so. The Historical loads the segment and begins serving it. At this point, if the indexing task was waiting for handoff, it will exit. Segment identifiers Segments all have a four-part identifier with the following components: Datasource name. Time interval (for the time chunk containing the segment; this corresponds to the segmentGranularity specified at ingestion time). Version number (generally an ISO8601 timestamp corresponding to when the segment set was first started). Partition number (an integer, unique within a datasource+interval+version; may not necessarily be contiguous). For example, this is the identifier for a segment in datasource clarity-cloud0, time chunk 2018-05-21T16:00:00.000Z/2018-05-21T17:00:00.000Z, version 2018-05-21T15:56:09.909Z, and partition number 1: clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1 Segments with partition number 0 (the first partition in a chunk) omit the partition number, like the following example, which is a segment in the same time chunk as the previous one, but with partition number 0 instead of 1: clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z Segment versioning You may be wondering what the \"version number\" described in the previous section is for. Or, you might not be, in which case good for you and you can skip this section! It's there to support batch-mode overwriting. In Druid, if all you ever do is append data, then there will be just a single version for each time chunk. But when you overwrite data, what happens behind the scenes is that a new set of segments is created with the same datasource, same time interval, but a higher version number. This is a signal to the rest of the Druid system that the older version should be removed from the cluster, and the new version should replace it. The switch appears to happen instantaneously to a user, because Druid handles this by first loading the new data (but not allowing it to be queried), and then, as soon as the new data is all loaded, switching all new queries to use those new segments. Then it drops the old segments a few minutes later. Segment lifecycle Each segment has a lifecycle that involves the following three major areas: Metadata store: Segment metadata (a small JSON payload generally no more than a few KB) is stored in the metadata store once a segment is done being constructed. The act of inserting a record for a segment into the metadata store is called publishing. These metadata records have a boolean flag named used, which controls whether the segment is intended to be queryable or not. Segments created by realtime tasks will be available before they are published, since they are only published when the segment is complete and will not accept any additional rows of data. Deep storage: Segment data files are pushed to deep storage once a segment is done being constructed. This happens immediately before publishing metadata to the metadata store. Availability for querying: Segments are available for querying on some Druid data server, like a realtime task or a Historical process. You can inspect the state of currently active segments using the Druid SQL sys.segments table. It includes the following flags: is_published: True if segment metadata has been published to the metadata store and used is true. is_available: True if the segment is currently available for querying, either on a realtime task or Historical process. is_realtime: True if the segment is only available on realtime tasks. For datasources that use realtime ingestion, this will generally start off true and then become false as the segment is published and handed off. is_overshadowed: True if the segment is published (with used set to true) and is fully overshadowed by some other published segments. Generally this is a transient state, and segments in this state will soon have their used flag automatically set to false. Availability and consistency Druid has an architectural separation between ingestion and querying, as described above in Indexing and handoff. This means that when understanding Druid's availability and consistency properties, we must look at each function separately. On the ingestion side, Druid's primary ingestion methods are all pull-based and offer transactional guarantees. This means that you are guaranteed that ingestion using these will publish in an all-or-nothing manner: Supervised \"seekable-stream\" ingestion methods like Kafka and Kinesis. With these methods, Druid commits stream offsets to its metadata store alongside segment metadata, in the same transaction. Note that ingestion of data that has not yet been published can be rolled back if ingestion tasks fail. In this case, partially-ingested data is discarded, and Druid will resume ingestion from the last committed set of stream offsets. This ensures exactly-once publishing behavior. Hadoop-based batch ingestion. Each task publishes all segment metadata in a single transaction. Native batch ingestion. In parallel mode, the supervisor task publishes all segment metadata in a single transaction after the subtasks are finished. In simple (single-task) mode, the single task publishes all segment metadata in a single transaction after it is complete. Tranquility, a streaming ingestion method that is no longer recommended, does not perform transactional loading. Additionally, some ingestion methods offer an idempotency guarantee. This means that repeated executions of the same ingestion will not cause duplicate data to be ingested: Supervised \"seekable-stream\" ingestion methods like Kafka and Kinesis are idempotent due to the fact that stream offsets and segment metadata are stored together and updated in lock-step. Hadoop-based batch ingestion is idempotent unless one of your input sources is the same Druid datasource that you are ingesting into. In this case, running the same task twice is non-idempotent, because you are adding to existing data instead of overwriting it. Native batch ingestion is idempotent unless appendToExisting is true, or one of your input sources is the same Druid datasource that you are ingesting into. In either of these two cases, running the same task twice is non-idempotent, because you are adding to existing data instead of overwriting it. On the query side, the Druid Broker is responsible for ensuring that a consistent set of segments is involved in a given query. It selects the appropriate set of segments to use when the query starts based on what is currently available. This is supported by atomic replacement, a feature that ensures that from a user's perspective, queries flip instantaneously from an older set of data to a newer set of data, with no consistency or performance impact. This is used for Hadoop-based batch ingestion, native batch ingestion when appendToExisting is false, and compaction. Note that atomic replacement happens for each time chunk individually. If a batch ingestion task or compaction involves multiple time chunks, then each time chunk will undergo atomic replacement soon after the task finishes, but the replacements will not all happen simultaneously. Typically, atomic replacement in Druid is based on a core set concept that works in conjunction with segment versions. When a time chunk is overwritten, a new core set of segments is created with a higher version number. The core set must all be available before the Broker will use them instead of the older set. There can also only be one core set per version per time chunk. Druid will also only use a single version at a time per time chunk. Together, these properties provide Druid's atomic replacement guarantees. Druid also supports an experimental segment locking mode that is activated by setting forceTimeChunkLock to false in the context of an ingestion task. In this case, Druid creates an atomic update group using the existing version for the time chunk, instead of creating a new core set with a new version number. There can be multiple atomic update groups with the same version number per time chunk. Each one replaces a specific set of earlier segments in the same time chunk and with the same version number. Druid will query the latest one that is fully available. This is a more powerful version of the core set concept, because it enables atomically replacing a subset of data for a time chunk, as well as doing atomic replacement and appending simultaneously. If segments become unavailable due to multiple Historicals going offline simultaneously (beyond your replication factor), then Druid queries will include only the segments that are still available. In the background, Druid will reload these unavailable segments on other Historicals as quickly as possible, at which point they will be included in queries again. Query processing Queries first enter the Broker, where the Broker will identify which segments have data that may pertain to that query. The list of segments is always pruned by time, and may also be pruned by other attributes depending on how your datasource is partitioned. The Broker will then identify which Historicals and MiddleManagers are serving those segments and send a rewritten subquery to each of those processes. The Historical/MiddleManager processes will take in the queries, process them and return results. The Broker receives results and merges them together to get the final answer, which it returns to the original caller. Broker pruning is an important way that Druid limits the amount of data that must be scanned for each query, but it is not the only way. For filters at a more granular level than what the Broker can use for pruning, indexing structures inside each segment allow Druid to figure out which (if any) rows match the filter set before looking at any row of data. Once Druid knows which rows match a particular query, it only accesses the specific columns it needs for that query. Within those columns, Druid can skip from row to row, avoiding reading data that doesn't match the query filter. So Druid uses three different techniques to maximize query performance: Pruning which segments are accessed for each query. Within each segment, using indexes to identify which rows must be accessed. Within each segment, only reading the specific rows and columns that are relevant to a particular query. For more details about how Druid executes queries, refer to the Query execution documentation. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"design/segments.html":{"url":"design/segments.html","title":"存储结构","keywords":"","body":" Apache Druid stores its index in segment files, which are partitioned by time. In a basic setup, one segment file is created for each time interval, where the time interval is configurable in the segmentGranularity parameter of the granularitySpec. For Druid to operate well under heavy query load, it is important for the segment file size to be within the recommended range of 300MB-700MB. If your segment files are larger than this range, then consider either changing the granularity of the time interval or partitioning your data and tweaking the targetPartitionSize in your partitionsSpec (a good starting point for this parameter is 5 million rows). See the sharding section below and the 'Partitioning specification' section of the Batch ingestion documentation for more information. A segment file's core data structures Here we describe the internal structure of segment files, which is essentially columnar: the data for each column is laid out in separate data structures. By storing each column separately, Druid can decrease query latency by scanning only those columns actually needed for a query. There are three basic column types: the timestamp column, dimension columns, and metric columns, as illustrated in the image below: The timestamp and metric columns are simple: behind the scenes each of these is an array of integer or floating point values compressed with LZ4. Once a query knows which rows it needs to select, it simply decompresses these, pulls out the relevant rows, and applies the desired aggregation operator. As with all columns, if a query doesn’t require a column, then that column’s data is just skipped over. Dimensions columns are different because they support filter and group-by operations, so each dimension requires the following three data structures: A dictionary that maps values (which are always treated as strings) to integer IDs, A list of the column’s values, encoded using the dictionary in 1, and For each distinct value in the column, a bitmap that indicates which rows contain that value. Why these three data structures? The dictionary simply maps string values to integer ids so that the values in (2) and (3) can be represented compactly. The bitmaps in (3) -- also known as inverted indexes allow for quick filtering operations (specifically, bitmaps are convenient for quickly applying AND and OR operators). Finally, the list of values in (2) is needed for group by and TopN queries. In other words, queries that solely aggregate metrics based on filters do not need to touch the list of dimension values stored in (2). To get a concrete sense of these data structures, consider the ‘page’ column from the example data above. The three data structures that represent this dimension are illustrated in the diagram below. 1: Dictionary that encodes column values { \"Justin Bieber\": 0, \"Ke$ha\": 1 } 2: Column data [0, 0, 1, 1] 3: Bitmaps - one for each unique value of the column value=\"Justin Bieber\": [1,1,0,0] value=\"Ke$ha\": [0,0,1,1] Note that the bitmap is different from the first two data structures: whereas the first two grow linearly in the size of the data (in the worst case), the size of the bitmap section is the product of data size * column cardinality. Compression will help us here though because we know that for each row in 'column data', there will only be a single bitmap that has non-zero entry. This means that high cardinality columns will have extremely sparse, and therefore highly compressible, bitmaps. Druid exploits this using compression algorithms that are specially suited for bitmaps, such as roaring bitmap compression. Multi-value columns If a data source makes use of multi-value columns, then the data structures within the segment files look a bit different. Let's imagine that in the example above, the second row were tagged with both the 'Ke$ha' and 'Justin Bieber' topics. In this case, the three data structures would now look as follows: 1: Dictionary that encodes column values { \"Justin Bieber\": 0, \"Ke$ha\": 1 } 2: Column data [0, [0,1], Note the changes to the second row in the column data and the Ke$ha bitmap. If a row has more than one value for a column, its entry in the 'column data' is an array of values. Additionally, a row with n values in 'column data' will have n non-zero valued entries in bitmaps. SQL Compatible Null Handling By default, Druid string dimension columns use the values '' and null interchangeably and numeric and metric columns can not represent null at all, instead coercing nulls to 0. However, Druid also provides a SQL compatible null handling mode, which must be enabled at the system level, through druid.generic.useDefaultValueForNull. This setting, when set to false, will allow Druid to at ingestion time create segments whose string columns can distinguish '' from null, and numeric columns which can represent null valued rows instead of 0. String dimension columns contain no additional column structures in this mode, instead just reserving an additional dictionary entry for the null value. Numeric columns however will be stored in the segment with an additional bitmap whose set bits indicate null valued rows. In addition to slightly increased segment sizes, SQL compatible null handling can incur a performance cost at query time as well, due to the need to check the null bitmap. This performance cost only occurs for columns that actually contain nulls. Naming Convention Identifiers for segments are typically constructed using the segment datasource, interval start time (in ISO 8601 format), interval end time (in ISO 8601 format), and a version. If data is additionally sharded beyond a time range, the segment identifier will also contain a partition number. An example segment identifier may be: datasource_intervalStart_intervalEnd_version_partitionNum Segment Components Behind the scenes, a segment is comprised of several files, listed below. version.bin 4 bytes representing the current segment version as an integer. E.g., for v9 segments, the version is 0x0, 0x0, 0x0, 0x9 meta.smoosh A file with metadata (filenames and offsets) about the contents of the other smoosh files XXXXX.smoosh There are some number of these files, which are concatenated binary data The smoosh files represent multiple files \"smooshed\" together in order to minimize the number of file descriptors that must be open to house the data. They are files of up to 2GB in size (to match the limit of a memory mapped ByteBuffer in Java). The smoosh files house individual files for each of the columns in the data as well as an index.drd file with extra metadata about the segment. There is also a special column called __time that refers to the time column of the segment. This will hopefully become less and less special as the code evolves, but for now it’s as special as my Mommy always told me I am. In the codebase, segments have an internal format version. The current segment format version is v9. Format of a column Each column is stored as two parts: A Jackson-serialized ColumnDescriptor The rest of the binary for the column A ColumnDescriptor is essentially an object that allows us to use Jackson's polymorphic deserialization to add new and interesting methods of serialization with minimal impact to the code. It consists of some metadata about the column (what type is it, is it multi-value, etc.) and then a list of serialization/deserialization logic that can deserialize the rest of the binary. Compression Druid compresses blocks of values for string, long, float, and double columns, using LZ4 by default, and bitmaps for string columns and numeric null values are compressed using Roaring. We recommend sticking with these defaults unless experimental verification with your own data and query patterns suggest that non-default options will perform better in your specific case. For example, for bitmap in string columns, the differences between using Roaring and CONCISE are most pronounced for high cardinality columns. In this case, Roaring is substantially faster on filters that match a lot of values, but in some cases CONCISE can have a lower footprint due to the overhead of the Roaring format (but is still slower when lots of values are matched). Currently, compression is configured on at the segment level rather than individual columns, see IndexSpec for more details. Sharding Data to Create Segments Sharding Multiple segments may exist for the same interval of time for the same datasource. These segments form a block for an interval. Depending on the type of shardSpec that is used to shard the data, Druid queries may only complete if a block is complete. That is to say, if a block consists of 3 segments, such as: sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_0 sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_1 sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_2 All 3 segments must be loaded before a query for the interval 2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z completes. The exception to this rule is with using linear shard specs. Linear shard specs do not force 'completeness' and queries can complete even if shards are not loaded in the system. For example, if your real-time ingestion creates 3 segments that were sharded with linear shard spec, and only two of the segments were loaded in the system, queries would return results only for those 2 segments. Schema changes Replacing segments Druid uniquely identifies segments using the datasource, interval, version, and partition number. The partition number is only visible in the segment id if there are multiple segments created for some granularity of time. For example, if you have hourly segments, but you have more data in an hour than a single segment can hold, you can create multiple segments for the same hour. These segments will share the same datasource, interval, and version, but have linearly increasing partition numbers. foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-01/2015-01-02_v1_1 foo_2015-01-01/2015-01-02_v1_2 In the example segments above, the dataSource = foo, interval = 2015-01-01/2015-01-02, version = v1, and partitionNum = 0. If at some later point in time, you reindex the data with a new schema, the newly created segments will have a higher version id. foo_2015-01-01/2015-01-02_v2_0 foo_2015-01-01/2015-01-02_v2_1 foo_2015-01-01/2015-01-02_v2_2 Druid batch indexing (either Hadoop-based or IndexTask-based) guarantees atomic updates on an interval-by-interval basis. In our example, until all v2 segments for 2015-01-01/2015-01-02 are loaded in a Druid cluster, queries exclusively use v1 segments. Once all v2 segments are loaded and queryable, all queries ignore v1 segments and switch to the v2 segments. Shortly afterwards, the v1 segments are unloaded from the cluster. Note that updates that span multiple segment intervals are only atomic within each interval. They are not atomic across the entire update. For example, you have segments such as the following: foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-02/2015-01-03_v1_1 foo_2015-01-03/2015-01-04_v1_2 v2 segments will be loaded into the cluster as soon as they are built and replace v1 segments for the period of time the segments overlap. Before v2 segments are completely loaded, your cluster may have a mixture of v1 and v2 segments. foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-02/2015-01-03_v2_1 foo_2015-01-03/2015-01-04_v1_2 In this case, queries may hit a mixture of v1 and v2 segments. Different schemas among segments Druid segments for the same datasource may have different schemas. If a string column (dimension) exists in one segment but not another, queries that involve both segments still work. Queries for the segment missing the dimension will behave as if the dimension has only null values. Similarly, if one segment has a numeric column (metric) but another does not, queries on the segment missing the metric will generally \"do the right thing\". Aggregations over this missing metric behave as if the metric were missing. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"design/processes.html":{"url":"design/processes.html","title":"进程与服务","keywords":"","body":" Process types Druid has several process types: Coordinator Overlord Broker Historical MiddleManager and Peons Indexer (Optional) Router (Optional) Server types Druid processes can be deployed any way you like, but for ease of deployment we suggest organizing them into three server types: Master Query Data This section describes the Druid processes and the suggested Master/Query/Data server organization, as shown in the architecture diagram above. Master server A Master server manages data ingestion and availability: it is responsible for starting new ingestion jobs and coordinating availability of data on the \"Data servers\" described below. Within a Master server, functionality is split between two processes, the Coordinator and Overlord. Coordinator process Coordinator processes watch over the Historical processes on the Data servers. They are responsible for assigning segments to specific servers, and for ensuring segments are well-balanced across Historicals. Overlord process Overlord processes watch over the MiddleManager processes on the Data servers and are the controllers of data ingestion into Druid. They are responsible for assigning ingestion tasks to MiddleManagers and for coordinating segment publishing. Query server A Query server provides the endpoints that users and client applications interact with, routing queries to Data servers or other Query servers (and optionally proxied Master server requests as well). Within a Query server, functionality is split between two processes, the Broker and Router. Broker process Broker processes receive queries from external clients and forward those queries to Data servers. When Brokers receive results from those subqueries, they merge those results and return them to the caller. End users typically query Brokers rather than querying Historicals or MiddleManagers processes on Data servers directly. Router process (optional) Router processes are optional processes that provide a unified API gateway in front of Druid Brokers, Overlords, and Coordinators. They are optional since you can also simply contact the Druid Brokers, Overlords, and Coordinators directly. The Router also runs the Druid Console, a management UI for datasources, segments, tasks, data processes (Historicals and MiddleManagers), and coordinator dynamic configuration. The user can also run SQL and native Druid queries within the console. Data server A Data server executes ingestion jobs and stores queryable data. Within a Data server, functionality is split between two processes, the Historical and MiddleManager. Historical process Historical processes are the workhorses that handle storage and querying on \"historical\" data (including any streaming data that has been in the system long enough to be committed). Historical processes download segments from deep storage and respond to queries about these segments. They don't accept writes. Middle Manager process MiddleManager processes handle ingestion of new data into the cluster. They are responsible for reading from external data sources and publishing new Druid segments. Peon processes Peon processes are task execution engines spawned by MiddleManagers. Each Peon runs a separate JVM and is responsible for executing a single task. Peons always run on the same host as the MiddleManager that spawned them. Indexer process (optional) Indexer processes are an alternative to MiddleManagers and Peons. Instead of forking separate JVM processes per-task, the Indexer runs tasks as individual threads within a single JVM process. The Indexer is designed to be easier to configure and deploy compared to the MiddleManager + Peon system and to better enable resource sharing across tasks. The Indexer is a newer feature and is currently designated experimental due to the fact that its memory management system is still under development. It will continue to mature in future versions of Druid. Typically, you would deploy either MiddleManagers or Indexers, but not both. Pros and cons of colocation Druid processes can be colocated based on the Master/Data/Query server organization as described above. This organization generally results in better utilization of hardware resources for most clusters. For very large scale clusters, however, it can be desirable to split the Druid processes such that they run on individual servers to avoid resource contention. This section describes guidelines and configuration parameters related to process colocation. Coordinators and Overlords The workload on the Coordinator process tends to increase with the number of segments in the cluster. The Overlord's workload also increases based on the number of segments in the cluster, but to a lesser degree than the Coordinator. In clusters with very high segment counts, it can make sense to separate the Coordinator and Overlord processes to provide more resources for the Coordinator's segment balancing workload. Unified Process The Coordinator and Overlord processes can be run as a single combined process by setting the druid.coordinator.asOverlord.enabled property. Please see Coordinator Configuration: Operation for details. Historicals and MiddleManagers With higher levels of ingestion or query load, it can make sense to deploy the Historical and MiddleManager processes on separate hosts to to avoid CPU and memory contention. The Historical also benefits from having free memory for memory mapped segments, which can be another reason to deploy the Historical and MiddleManager processes separately. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"dependencies/deep-storage.html":{"url":"dependencies/deep-storage.html","title":"深度存储","keywords":"","body":" Deep storage is where segments are stored. It is a storage mechanism that Apache Druid does not provide. This deep storage infrastructure defines the level of durability of your data, as long as Druid processes can see this storage infrastructure and get at the segments stored on it, you will not lose data no matter how many Druid nodes you lose. If segments disappear from this storage layer, then you will lose whatever data those segments represented. Local Mount A local mount can be used for storage of segments as well. This allows you to use just your local file system or anything else that can be mount locally like NFS, Ceph, etc. This is the default deep storage implementation. In order to use a local mount for deep storage, you need to set the following configuration in your common configs. Property Possible Values Description Default druid.storage.type local Must be set. druid.storage.storageDirectory Directory for storing segments. Must be set. Note that you should generally set druid.storage.storageDirectory to something different from druid.segmentCache.locations and druid.segmentCache.infoDir. If you are using the Hadoop indexer in local mode, then just give it a local file as your output directory and it will work. S3-compatible See druid-s3-extensions extension documentation. HDFS See druid-hdfs-storage extension documentation. Additional Deep Stores For additional deep stores, please see our extensions list. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"dependencies/metadata-storage.html":{"url":"dependencies/metadata-storage.html","title":"元数据存储","keywords":"","body":" The Metadata Storage is an external dependency of Apache Druid. Druid uses it to store various metadata about the system, but not to store the actual data. There are a number of tables used for various purposes described below. Derby is the default metadata store for Druid, however, it is not suitable for production. MySQL and PostgreSQL are more production suitable metadata stores. The Metadata Storage stores the entire metadata which is essential for a Druid cluster to work. For production clusters, consider using MySQL or PostgreSQL instead of Derby. Also, it's highly recommended to set up a high availability environment because there is no way to restore if you lose any metadata. Using Derby Add the following to your Druid configuration. druid.metadata.storage.type=derby druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527//opt/var/druid_state/derby;create=true MySQL See mysql-metadata-storage extension documentation. PostgreSQL See postgresql-metadata-storage. Adding custom dbcp properties NOTE: These properties are not settable through the druid.metadata.storage.connector.dbcp properties: username, password, connectURI, validationQuery, testOnBorrow. These must be set through druid.metadata.storage.connector properties. Example supported properties: druid.metadata.storage.connector.dbcp.maxConnLifetimeMillis=1200000 druid.metadata.storage.connector.dbcp.defaultQueryTimeout=30000 See BasicDataSource Configuration for full list. Metadata storage tables Segments table This is dictated by the druid.metadata.storage.tables.segments property. This table stores metadata about the segments that should be available in the system. (This set of segments is called \"used segments\" elsewhere in the documentation and throughout the project.) The table is polled by the Coordinator to determine the set of segments that should be available for querying in the system. The table has two main functional columns, the other columns are for indexing purposes. Value 1 in the used column means that the segment should be \"used\" by the cluster (i.e., it should be loaded and available for requests). Value 0 means that the segment should not be loaded into the cluster. We do this as a means of unloading segments from the cluster without actually removing their metadata (which allows for simpler rolling back if that is ever an issue). The payload column stores a JSON blob that has all of the metadata for the segment (some of the data stored in this payload is redundant with some of the columns in the table, that is intentional). This looks something like { \"dataSource\":\"wikipedia\", \"interval\":\"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z\", \"version\":\"2012-05-24T00:10:00.046Z\", \"loadSpec\":{ \"type\":\"s3_zip\", \"bucket\":\"bucket_for_segment\", \"key\":\"path/to/segment/on/s3\" }, \"dimensions\":\"comma-delimited-list-of-dimension-names\", \"metrics\":\"comma-delimited-list-of-metric-names\", \"shardSpec\":{\"type\":\"none\"}, \"binaryVersion\":9, \"size\":size_of_segment, \"identifier\":\"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z\" } Note that the format of this blob can and will change from time-to-time. Rule table The rule table is used to store the various rules about where segments should land. These rules are used by the Coordinator when making segment (re-)allocation decisions about the cluster. Config table The config table is used to store runtime configuration objects. We do not have many of these yet and we are not sure if we will keep this mechanism going forward, but it is the beginnings of a method of changing some configuration parameters across the cluster at runtime. Task-related tables There are also a number of tables created and used by the Overlord and MiddleManager when managing tasks. Audit table The Audit table is used to store the audit history for configuration changes e.g rule changes done by Coordinator and other config changes. Accessed by: The Metadata Storage is accessed only by: Indexing Service Processes (if any) Realtime Processes (if any) Coordinator Processes Thus you need to give permissions (e.g., in AWS Security Groups) only for these machines to access the Metadata storage. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"dependencies/zookeeper.html":{"url":"dependencies/zookeeper.html","title":"Zookeeper","keywords":"","body":" Apache Druid uses Apache ZooKeeper (ZK) for management of current cluster state. The operations that happen over ZK are Coordinator leader election Segment \"publishing\" protocol from Historical Segment load/drop protocol between Coordinator and Historical Overlord leader election Overlord and MiddleManager task management Coordinator Leader Election We use the Curator LeadershipLatch recipe to do leader election at path ${druid.zk.paths.coordinatorPath}/_COORDINATOR Segment \"publishing\" protocol from Historical and Realtime The announcementsPath and servedSegmentsPath are used for this. All Historical processes publish themselves on the announcementsPath, specifically, they will create an ephemeral znode at ${druid.zk.paths.announcementsPath}/${druid.host} Which signifies that they exist. They will also subsequently create a permanent znode at ${druid.zk.paths.servedSegmentsPath}/${druid.host} And as they load up segments, they will attach ephemeral znodes that look like ${druid.zk.paths.servedSegmentsPath}/${druid.host}/_segment_identifier_ Processes like the Coordinator and Broker can then watch these paths to see which processes are currently serving which segments. Segment load/drop protocol between Coordinator and Historical The loadQueuePath is used for this. When the Coordinator decides that a Historical process should load or drop a segment, it writes an ephemeral znode to ${druid.zk.paths.loadQueuePath}/_host_of_historical_process/_segment_identifier This znode will contain a payload that indicates to the Historical process what it should do with the given segment. When the Historical process is done with the work, it will delete the znode in order to signify to the Coordinator that it is complete. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"ingestion/":{"url":"ingestion/","title":"概览","keywords":"","body":" Overview All data in Druid is organized into segments, which are data files that generally have up to a few million rows each. Loading data in Druid is called ingestion or indexing and consists of reading data from a source system and creating segments based on that data. In most ingestion methods, the work of loading data is done by Druid MiddleManager processes (or the Indexer processes). One exception is Hadoop-based ingestion, where this work is instead done using a Hadoop MapReduce job on YARN (although MiddleManager or Indexer processes are still involved in starting and monitoring the Hadoop jobs). Once segments have been generated and stored in deep storage, they will be loaded by Historical processes. For more details on how this works under the hood, see the Storage design section of Druid's design documentation. How to use this documentation This page you are currently reading provides information about universal Druid ingestion concepts, and about configurations that are common to all ingestion methods. The individual pages for each ingestion method provide additional information about concepts and configurations that are unique to each ingestion method. We recommend reading (or at least skimming) this universal page first, and then referring to the page for the ingestion method or methods that you have chosen. Ingestion methods The table below lists Druid's most common data ingestion methods, along with comparisons to help you choose the best one for your situation. Each ingestion method supports its own set of source systems to pull from. For details about how each method works, as well as configuration properties specific to that method, check out its documentation page. Streaming The most recommended, and most popular, method of streaming ingestion is the Kafka indexing service that reads directly from Kafka. The Kinesis indexing service also works well if you prefer Kinesis. This table compares the major available options: Method Kafka Kinesis Tranquility Supervisor type kafka kinesis N/A How it works Druid reads directly from Apache Kafka. Druid reads directly from Amazon Kinesis. Tranquility, a library that ships separately from Druid, is used to push data into Druid. Can ingest late data? Yes Yes No (late data is dropped based on the windowPeriod config) Exactly-once guarantees? Yes Yes No Batch When doing batch loads from files, you should use one-time tasks, and you have three options: index_parallel (native batch; parallel), index_hadoop (Hadoop-based), or index (native batch; single-task). In general, we recommend native batch whenever it meets your needs, since the setup is simpler (it does not depend on an external Hadoop cluster). However, there are still scenarios where Hadoop-based batch ingestion might be a better choice, for example when you already have a running Hadoop cluster and want to use the cluster resource of the existing cluster for batch ingestion. This table compares the three available options: Method Native batch (parallel) Hadoop-based Native batch (simple) Task type index_parallel index_hadoop index Parallel? Yes, if inputFormat is splittable and maxNumConcurrentSubTasks > 1 in tuningConfig. See data format documentation for details. Yes, always. No. Each task is single-threaded. Can append or overwrite? Yes, both. Overwrite only. Yes, both. External dependencies None. Hadoop cluster (Druid submits Map/Reduce jobs). None. Input locations Any inputSource. Any Hadoop FileSystem or Druid datasource. Any inputSource. File formats Any inputFormat. Any Hadoop InputFormat. Any inputFormat. Rollup modes Perfect if forceGuaranteedRollup = true in the tuningConfig. Always perfect. Perfect if forceGuaranteedRollup = true in the tuningConfig. Partitioning options Dynamic, hash-based, and range-based partitioning methods are available. See Partitions Spec for details. Hash-based or range-based partitioning via partitionsSpec. Dynamic and hash-based partitioning methods are available. See Partitions Spec for details. Druid's data model Datasources Druid data is stored in datasources, which are similar to tables in a traditional RDBMS. Druid offers a unique data modeling system that bears similarity to both relational and timeseries models. Primary timestamp Druid schemas must always include a primary timestamp. The primary timestamp is used for partitioning and sorting your data. Druid queries are able to rapidly identify and retrieve data corresponding to time ranges of the primary timestamp column. Druid is also able to use the primary timestamp column for time-based data management operations such as dropping time chunks, overwriting time chunks, and time-based retention rules. The primary timestamp is parsed based on the timestampSpec. In addition, the granularitySpec controls other important operations that are based on the primary timestamp. Regardless of which input field the primary timestamp is read from, it will always be stored as a column named __time in your Druid datasource. If you have more than one timestamp column, you can store the others as secondary timestamps. Dimensions Dimensions are columns that are stored as-is and can be used for any purpose. You can group, filter, or apply aggregators to dimensions at query time in an ad-hoc manner. If you run with rollup disabled, then the set of dimensions is simply treated like a set of columns to ingest, and behaves exactly as you would expect from a typical database that does not support a rollup feature. Dimensions are configured through the dimensionsSpec. Metrics Metrics are columns that are stored in an aggregated form. They are most useful when rollup is enabled. Specifying a metric allows you to choose an aggregation function for Druid to apply to each row during ingestion. This has two benefits: If rollup is enabled, multiple rows can be collapsed into one row even while retaining summary information. In the rollup tutorial, this is used to collapse netflow data to a single row per (minute, srcIP, dstIP) tuple, while retaining aggregate information about total packet and byte counts. Some aggregators, especially approximate ones, can be computed faster at query time even on non-rolled-up data if they are partially computed at ingestion time. Metrics are configured through the metricsSpec. Rollup What is rollup? Druid can roll up data as it is ingested to minimize the amount of raw data that needs to be stored. Rollup is a form of summarization or pre-aggregation. In practice, rolling up data can dramatically reduce the size of data that needs to be stored, reducing row counts by potentially orders of magnitude. This storage reduction does come at a cost: as we roll up data, we lose the ability to query individual events. When rollup is disabled, Druid loads each row as-is without doing any form of pre-aggregation. This mode is similar to what you would expect from a typical database that does not support a rollup feature. When rollup is enabled, then any rows that have identical dimensions and timestamp to each other (after queryGranularity-based truncation) can be collapsed, or rolled up, into a single row in Druid. By default, rollup is enabled. Enabling or disabling rollup Rollup is controlled by the rollup setting in the granularitySpec. By default, it is true (enabled). Set this to false if you want Druid to store each record as-is, without any rollup summarization. Example of rollup For an example of how to configure rollup, and of how the feature will modify your data, check out the rollup tutorial. Maximizing rollup ratio You can measure the rollup ratio of a datasource by comparing the number of rows in Druid with the number of ingested events. The higher this number, the more benefit you are gaining from rollup. One way to do this is with a Druid SQL query like: SELECT SUM(\"cnt\") / COUNT(*) * 1.0 FROM datasource In this query, cnt should refer to a \"count\" type metric specified at ingestion time. See Counting the number of ingested events on the \"Schema design\" page for more details about how counting works when rollup is enabled. Tips for maximizing rollup: Generally, the fewer dimensions you have, and the lower the cardinality of your dimensions, the better rollup ratios you will achieve. Use sketches to avoid storing high cardinality dimensions, which harm rollup ratios. Adjusting queryGranularity at ingestion time (for example, using PT5M instead of PT1M) increases the likelihood of two rows in Druid having matching timestamps, and can improve your rollup ratios. It can be beneficial to load the same data into more than one Druid datasource. Some users choose to create a \"full\" datasource that has rollup disabled (or enabled, but with a minimal rollup ratio) and an \"abbreviated\" datasource that has fewer dimensions and a higher rollup ratio. When queries only involve dimensions in the \"abbreviated\" set, using that datasource leads to much faster query times. This can often be done with just a small increase in storage footprint, since abbreviated datasources tend to be substantially smaller. If you are using a best-effort rollup ingestion configuration that does not guarantee perfect rollup, you can potentially improve your rollup ratio by switching to a guaranteed perfect rollup option, or by reindexing your data in the background after initial ingestion. Perfect rollup vs Best-effort rollup Some Druid ingestion methods guarantee perfect rollup, meaning that input data are perfectly aggregated at ingestion time. Others offer best-effort rollup, meaning that input data might not be perfectly aggregated and thus there could be multiple segments holding rows with the same timestamp and dimension values. In general, ingestion methods that offer best-effort rollup do this because they are either parallelizing ingestion without a shuffling step (which would be required for perfect rollup), or because they are finalizing and publishing segments before all data for a time chunk has been received, which we call incremental publishing. In both of these cases, records that could theoretically be rolled up may end up in different segments. All types of streaming ingestion run in this mode. Ingestion methods that guarantee perfect rollup do it with an additional preprocessing step to determine intervals and partitioning before the actual data ingestion stage. This preprocessing step scans the entire input dataset, which generally increases the time required for ingestion, but provides information necessary for perfect rollup. The following table shows how each method handles rollup: Method How it works Native batch index_parallel and index type may be either perfect or best-effort, based on configuration. Hadoop Always perfect. Kafka indexing service Always best-effort. Kinesis indexing service Always best-effort. Partitioning Why partition? Optimal partitioning and sorting of segments within your datasources can have substantial impact on footprint and performance. Druid datasources are always partitioned by time into time chunks, and each time chunk contains one or more segments. This partitioning happens for all ingestion methods, and is based on the segmentGranularity parameter of your ingestion spec's dataSchema. The segments within a particular time chunk may also be partitioned further, using options that vary based on the ingestion type you have chosen. In general, doing this secondary partitioning using a particular dimension will improve locality, meaning that rows with the same value for that dimension are stored together and can be accessed quickly. You will usually get the best performance and smallest overall footprint by partitioning your data on some \"natural\" dimension that you often filter by, if one exists. This will often improve compression - users have reported threefold storage size decreases - and it also tends to improve query performance as well. Partitioning and sorting are best friends! If you do have a \"natural\" partitioning dimension, you should also consider placing it first in the dimensions list of your dimensionsSpec, which tells Druid to sort rows within each segment by that column. This will often improve compression even more, beyond the improvement gained by partitioning alone. However, note that currently, Druid always sorts rows within a segment by timestamp first, even before the first dimension listed in your dimensionsSpec. This can prevent dimension sorting from being maximally effective. If necessary, you can work around this limitation by setting queryGranularity equal to segmentGranularity in your granularitySpec, which will set all timestamps within the segment to the same value, and by saving your \"real\" timestamp as a secondary timestamp. This limitation may be removed in a future version of Druid. How to set up partitioning Not all ingestion methods support an explicit partitioning configuration, and not all have equivalent levels of flexibility. As of current Druid versions, If you are doing initial ingestion through a less-flexible method (like Kafka) then you can use reindexing techniques to repartition your data after it is initially ingested. This is a powerful technique: you can use it to ensure that any data older than a certain threshold is optimally partitioned, even as you continuously add new data from a stream. The following table shows how each ingestion method handles partitioning: Method How it works Native batch Configured using partitionsSpec inside the tuningConfig. Hadoop Configured using partitionsSpec inside the tuningConfig. Kafka indexing service Partitioning in Druid is guided by how your Kafka topic is partitioned. You can also reindex to repartition after initial ingestion. Kinesis indexing service Partitioning in Druid is guided by how your Kinesis stream is sharded. You can also reindex to repartition after initial ingestion. Note that, of course, one way to partition data is to load it into separate datasources. This is a perfectly viable approach and works very well when the number of datasources does not lead to excessive per-datasource overheads. If you go with this approach, then you can ignore this section, since it is describing how to set up partitioning within a single datasource. For more details on splitting data up into separate datasources, and potential operational considerations, refer to the Multitenancy considerations page. Ingestion specs No matter what ingestion method you use, data is loaded into Druid using either one-time tasks or ongoing \"supervisors\" (which run and supervise a set of tasks over time). In any case, part of the task or supervisor definition is an ingestion spec. Ingestion specs consists of three main components: dataSchema, which configures the datasource name, primary timestamp, dimensions, metrics, and transforms and filters (if needed). ioConfig, which tells Druid how to connect to the source system and how to parse data. For more information, see the documentation for each ingestion method. tuningConfig, which controls various tuning parameters specific to each ingestion method. Example ingestion spec for task type index_parallel (native batch): { \"type\": \"index_parallel\", \"spec\": { \"dataSchema\": { \"dataSource\": \"wikipedia\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ { \"type\": \"string\", \"page\" }, { \"type\": \"string\", \"language\" }, { \"type\": \"long\", \"name\": \"userId\" } ] }, \"metricsSpec\": [ { \"type\": \"count\", \"name\": \"count\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_added_sum\", \"fieldName\": \"bytes_added\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_deleted_sum\", \"fieldName\": \"bytes_deleted\" } ], \"granularitySpec\": { \"segmentGranularity\": \"day\", \"queryGranularity\": \"none\", \"intervals\": [ \"2013-08-31/2013-09-01\" ] } }, \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"local\", \"baseDir\": \"examples/indexing/\", \"filter\": \"wikipedia_data.json\" }, \"inputFormat\": { \"type\": \"json\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"userId\", \"expr\": \"$.user.id\" } ] } } }, \"tuningConfig\": { \"type\": \"index_parallel\" } } } The specific options supported by these sections will depend on the ingestion method you have chosen. For more examples, refer to the documentation for each ingestion method. You can also load data visually, without the need to write an ingestion spec, using the \"Load data\" functionality available in Druid's web console. Druid's visual data loader supports Kafka, Kinesis, and native batch mode. dataSchema The dataSchema spec has been changed in 0.17.0. The new spec is supported by all ingestion methods except for Hadoop ingestion. See the Legacy dataSchema spec for the old spec. The dataSchema is a holder for the following components: datasource name, primary timestamp, dimensions, metrics, and transforms and filters (if needed). An example dataSchema is: \"dataSchema\": { \"dataSource\": \"wikipedia\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ { \"type\": \"string\", \"page\" }, { \"type\": \"string\", \"language\" }, { \"type\": \"long\", \"name\": \"userId\" } ] }, \"metricsSpec\": [ { \"type\": \"count\", \"name\": \"count\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_added_sum\", \"fieldName\": \"bytes_added\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_deleted_sum\", \"fieldName\": \"bytes_deleted\" } ], \"granularitySpec\": { \"segmentGranularity\": \"day\", \"queryGranularity\": \"none\", \"intervals\": [ \"2013-08-31/2013-09-01\" ] } } dataSource The dataSource is located in dataSchema → dataSource and is simply the name of the datasource that data will be written to. An example dataSource is: \"dataSource\": \"my-first-datasource\" timestampSpec The timestampSpec is located in dataSchema → timestampSpec and is responsible for configuring the primary timestamp. An example timestampSpec is: \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" } Conceptually, after input data records are read, Druid applies ingestion spec components in a particular order: first flattenSpec (if any), then timestampSpec, then transformSpec, and finally dimensionsSpec and metricsSpec. Keep this in mind when writing your ingestion spec. A timestampSpec can have the following components: Field Description Default column Input row field to read the primary timestamp from.Regardless of the name of this input field, the primary timestamp will always be stored as a column named __time in your Druid datasource. timestamp format Timestamp format. Options are: `iso`: ISO8601 with 'T' separator, like \"2000-01-01T01:02:03.456\"`posix`: seconds since epoch`millis`: milliseconds since epoch`micro`: microseconds since epoch`nano`: nanoseconds since epoch`auto`: automatically detects ISO (either 'T' or space separator) or millis formatany [Joda DateTimeFormat string](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html) auto missingValue Timestamp to use for input records that have a null or missing timestamp column. Should be in ISO8601 format, like \"2000-01-01T01:02:03.456\", even if you have specified something else for format. Since Druid requires a primary timestamp, this setting can be useful for ingesting datasets that do not have any per-record timestamps at all. none dimensionsSpec The dimensionsSpec is located in dataSchema → dimensionsSpec and is responsible for configuring dimensions. An example dimensionsSpec is: \"dimensionsSpec\" : { \"dimensions\": [ \"page\", \"language\", { \"type\": \"long\", \"name\": \"userId\" } ], \"dimensionExclusions\" : [], \"spatialDimensions\" : [] } Conceptually, after input data records are read, Druid applies ingestion spec components in a particular order: first flattenSpec (if any), then timestampSpec, then transformSpec, and finally dimensionsSpec and metricsSpec. Keep this in mind when writing your ingestion spec. A dimensionsSpec can have the following components: Field Description Default dimensions A list of dimension names or objects. Cannot have the same column in both dimensions and dimensionExclusions.If this and spatialDimensions are both null or empty arrays, Druid will treat all non-timestamp, non-metric columns that do not appear in dimensionExclusions as String-typed dimension columns. See inclusions and exclusions below for details. [] dimensionExclusions The names of dimensions to exclude from ingestion. Only names are supported here, not objects.This list is only used if the dimensions and spatialDimensions lists are both null or empty arrays; otherwise it is ignored. See inclusions and exclusions below for details. [] spatialDimensions An array of spatial dimensions. [] Dimension objects Each dimension in the dimensions list can either be a name or an object. Providing a name is equivalent to providing a string type dimension object with the given name, e.g. \"page\" is equivalent to {\"name\": \"page\", \"type\": \"string\"}. Dimension objects can have the following components: Field Description Default type Either string, long, float, or double. string name The name of the dimension. This will be used as the field name to read from input records, as well as the column name stored in generated segments.Note that you can use a transformSpec if you want to rename columns during ingestion time. none (required) createBitmapIndex For string typed dimensions, whether or not bitmap indexes should be created for the column in generated segments. Creating a bitmap index requires more storage, but speeds up certain kinds of filtering (especially equality and prefix filtering). Only supported for string typed dimensions. true Inclusions and exclusions Druid will interpret a dimensionsSpec in two possible ways: normal or schemaless. Normal interpretation occurs when either dimensions or spatialDimensions is non-empty. In this case, the combination of the two lists will be taken as the set of dimensions to be ingested, and the list of dimensionExclusions will be ignored. Schemaless interpretation occurs when both dimensions and spatialDimensions are empty or null. In this case, the set of dimensions is determined in the following way: First, start from the set of all input fields from the inputFormat (or the flattenSpec, if one is being used). Any field listed in dimensionExclusions is excluded. The field listed as column in the timestampSpec is excluded. Any field used as an input to an aggregator from the metricsSpec is excluded. Any field with the same name as an aggregator from the metricsSpec is excluded. All other fields are ingested as string typed dimensions with the default settings. Note: Fields generated by a transformSpec are not currently considered candidates for schemaless dimension interpretation. metricsSpec The metricsSpec is located in dataSchema → metricsSpec and is a list of aggregators to apply at ingestion time. This is most useful when rollup is enabled, since it's how you configure ingestion-time aggregation. An example metricsSpec is: \"metricsSpec\": [ { \"type\": \"count\", \"name\": \"count\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_added_sum\", \"fieldName\": \"bytes_added\" }, { \"type\": \"doubleSum\", \"name\": \"bytes_deleted_sum\", \"fieldName\": \"bytes_deleted\" } ] Generally, when rollup is disabled, you should have an empty metricsSpec (because without rollup, Druid does not do any ingestion-time aggregation, so there is little reason to include an ingestion-time aggregator). However, in some cases, it can still make sense to define metrics: for example, if you want to create a complex column as a way of pre-computing part of an approximate aggregation, this can only be done by defining a metric in a metricsSpec. granularitySpec The granularitySpec is located in dataSchema → granularitySpec and is responsible for configuring the following operations: Partitioning a datasource into time chunks (via segmentGranularity). Truncating the timestamp, if desired (via queryGranularity). Specifying which time chunks of segments should be created, for batch ingestion (via intervals). Specifying whether ingestion-time rollup should be used or not (via rollup). Other than rollup, these operations are all based on the primary timestamp. An example granularitySpec is: \"granularitySpec\": { \"segmentGranularity\": \"day\", \"queryGranularity\": \"none\", \"intervals\": [ \"2013-08-31/2013-09-01\" ], \"rollup\": true } A granularitySpec can have the following components: Field Description Default type Either uniform or arbitrary. In most cases you want to use uniform. uniform segmentGranularity Time chunking granularity for this datasource. Multiple segments can be created per time chunk. For example, when set to day, the events of the same day fall into the same time chunk which can be optionally further partitioned into multiple segments based on other configurations and input size. Any granularity can be provided here. Note that all segments in the same time chunk should have the same segment granularity.Ignored if type is set to arbitrary. day queryGranularity The resolution of timestamp storage within each segment. This must be equal to, or finer, than segmentGranularity. This will be the finest granularity that you can query at and still receive sensible results, but note that you can still query at anything coarser than this granularity. E.g., a value of minute will mean that records will be stored at minutely granularity, and can be sensibly queried at any multiple of minutes (including minutely, 5-minutely, hourly, etc).Any granularity can be provided here. Use none to store timestamps as-is, without any truncation. Note that rollup will be applied if it is set even when the queryGranularity is set to none. none rollup Whether to use ingestion-time rollup or not. Note that rollup is still effective even when queryGranularity is set to none. Your data will be rolled up if they have the exactly same timestamp. true intervals A list of intervals describing what time chunks of segments should be created. If type is set to uniform, this list will be broken up and rounded-off based on the segmentGranularity. If type is set to arbitrary, this list will be used as-is.If null or not provided, batch ingestion tasks will generally determine which time chunks to output based on what timestamps are found in the input data.If specified, batch ingestion tasks may be able to skip a determining-partitions phase, which can result in faster ingestion. Batch ingestion tasks may also be able to request all their locks up-front instead of one by one. Batch ingestion tasks will throw away any records with timestamps outside of the specified intervals.Ignored for any form of streaming ingestion. null transformSpec The transformSpec is located in dataSchema → transformSpec and is responsible for transforming and filtering records during ingestion time. It is optional. An example transformSpec is: \"transformSpec\": { \"transforms\": [ { \"type\": \"expression\", \"name\": \"countryUpper\", \"expression\": \"upper(country)\" } ], \"filter\": { \"type\": \"selector\", \"dimension\": \"country\", \"value\": \"San Serriffe\" } } Conceptually, after input data records are read, Druid applies ingestion spec components in a particular order: first flattenSpec (if any), then timestampSpec, then transformSpec, and finally dimensionsSpec and metricsSpec. Keep this in mind when writing your ingestion spec. Transforms The transforms list allows you to specify a set of expressions to evaluate on top of input data. Each transform has a \"name\" which can be referred to by your dimensionsSpec, metricsSpec, etc. If a transform has the same name as a field in an input row, then it will shadow the original field. Transforms that shadow fields may still refer to the fields they shadow. This can be used to transform a field \"in-place\". Transforms do have some limitations. They can only refer to fields present in the actual input rows; in particular, they cannot refer to other transforms. And they cannot remove fields, only add them. However, they can shadow a field with another field containing all nulls, which will act similarly to removing the field. Transforms can refer to the timestamp of an input row by referring to __time as part of the expression. They can also replace the timestamp if you set their \"name\" to __time. In both cases, __time should be treated as a millisecond timestamp (number of milliseconds since Jan 1, 1970 at midnight UTC). Transforms are applied after the timestampSpec. Druid currently includes one kind of built-in transform, the expression transform. It has the following syntax: { \"type\": \"expression\", \"name\": \"\", \"expression\": \"\" } The expression is a Druid query expression. Conceptually, after input data records are read, Druid applies ingestion spec components in a particular order: first flattenSpec (if any), then timestampSpec, then transformSpec, and finally dimensionsSpec and metricsSpec. Keep this in mind when writing your ingestion spec. Filter The filter conditionally filters input rows during ingestion. Only rows that pass the filter will be ingested. Any of Druid's standard query filters can be used. Note that within a transformSpec, the transforms are applied before the filter, so the filter can refer to a transform. Legacy dataSchema spec The dataSchema spec has been changed in 0.17.0. The new spec is supported by all ingestion methods except for Hadoop ingestion. See dataSchema for the new spec. The legacy dataSchema spec has below two more components in addition to the ones listed in the dataSchema section above. input row parser, flattening of nested data (if needed) parser (Deprecated) In legacy dataSchema, the parser is located in the dataSchema → parser and is responsible for configuring a wide variety of items related to parsing input records. The parser is deprecated and it is highly recommended to use inputFormat instead. For details about inputFormat and supported parser types, see the \"Data formats\" page. For details about major components of the parseSpec, refer to their subsections: timestampSpec, responsible for configuring the primary timestamp. dimensionsSpec, responsible for configuring dimensions. flattenSpec, responsible for flattening nested data formats. An example parser is: \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"userId\", \"expr\": \"$.user.id\" } ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ { \"type\": \"string\", \"page\" }, { \"type\": \"string\", \"language\" }, { \"type\": \"long\", \"name\": \"userId\" } ] } } } flattenSpec In the legacy dataSchema, the flattenSpec is located in dataSchema → parser → parseSpec → flattenSpec and is responsible for bridging the gap between potentially nested input data (such as JSON, Avro, etc) and Druid's flat data model. See Flatten spec for more details. ioConfig The ioConfig influences how data is read from a source system, such as Apache Kafka, Amazon S3, a mounted filesystem, or any other supported source system. The inputFormat property applies to all ingestion method except for Hadoop ingestion. The Hadoop ingestion still uses the parser in the legacy dataSchema. The rest of ioConfig is specific to each individual ingestion method. An example ioConfig to read JSON data is: \"ioConfig\": { \"type\": \"\", \"inputFormat\": { \"type\": \"json\" }, ... } For more details, see the documentation provided by each ingestion method. tuningConfig Tuning properties are specified in a tuningConfig, which goes at the top level of an ingestion spec. Some properties apply to all ingestion methods, but most are specific to each individual ingestion method. An example tuningConfig that sets all of the shared, common properties to their defaults is: \"tuningConfig\": { \"type\": \"\", \"maxRowsInMemory\": 1000000, \"maxBytesInMemory\": , \"indexSpec\": { \"bitmap\": { \"type\": \"roaring\" }, \"dimensionCompression\": \"lz4\", \"metricCompression\": \"lz4\", \"longEncoding\": \"longs\" }, } Field Description Default type Each ingestion method has its own tuning type code. You must specify the type code that matches your ingestion method. Common options are index, hadoop, kafka, and kinesis. maxRowsInMemory The maximum number of records to store in memory before persisting to disk. Note that this is the number of rows post-rollup, and so it may not be equal to the number of input records. Ingested records will be persisted to disk when either maxRowsInMemory or maxBytesInMemory are reached (whichever happens first). 1000000 maxBytesInMemory The maximum aggregate size of records, in bytes, to store in the JVM heap before persisting. This is based on a rough estimate of memory usage. Ingested records will be persisted to disk when either maxRowsInMemory or maxBytesInMemory are reached (whichever happens first).Setting maxBytesInMemory to -1 disables this check, meaning Druid will rely entirely on maxRowsInMemory to control memory usage. Setting it to zero means the default value will be used (one-sixth of JVM heap size).Note that the estimate of memory usage is designed to be an overestimate, and can be especially high when using complex ingest-time aggregators, including sketches. If this causes your indexing workloads to persist to disk too often, you can set maxBytesInMemory to -1 and rely on maxRowsInMemory instead. One-sixth of max JVM heap size indexSpec Tune how data is indexed. See below for more information. See table below Other properties Each ingestion method has its own list of additional tuning properties. See the documentation for each method for a full list: Kafka indexing service, Kinesis indexing service, Native batch, and Hadoop-based. indexSpec The indexSpec object can include the following properties: Field Description Default bitmap Compression format for bitmap indexes. Should be a JSON object with type set to roaring or concise. For type roaring, the boolean property compressRunOnSerialization (defaults to true) controls whether or not run-length encoding will be used when it is determined to be more space-efficient. {\"type\": \"concise\"} dimensionCompression Compression format for dimension columns. Options are lz4, lzf, or uncompressed. lz4 metricCompression Compression format for primitive type metric columns. Options are lz4, lzf, uncompressed, or none (which is more efficient than uncompressed, but not supported by older versions of Druid). lz4 longEncoding Encoding format for long-typed columns. Applies regardless of whether they are dimensions or metrics. Options are auto or longs. auto encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. longs stores the value as-is with 8 bytes each. longs Beyond these properties, each ingestion method has its own specific tuning properties. See the documentation for each ingestion method for details. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/data-formats.html":{"url":"ingestion/data-formats.html","title":"数据格式","keywords":"","body":" Apache Druid can ingest denormalized data in JSON, CSV, or a delimited form such as TSV, or any custom format. While most examples in the documentation use data in JSON format, it is not difficult to configure Druid to ingest any other delimited data. We welcome any contributions to new formats. This page lists all default and core extension data formats supported by Druid. For additional data formats supported with community extensions, please see our community extensions list. Formatting the Data The following samples show data formats that are natively supported in Druid: JSON {\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"Gypsy Danger\", \"language\" : \"en\", \"user\" : \"nuclear\", \"unpatrolled\" : \"true\", \"newPage\" : \"true\", \"robot\": \"false\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"North America\", \"country\":\"United States\", \"region\":\"Bay Area\", \"city\":\"San Francisco\", \"added\": 57, \"deleted\": 200, \"delta\": -143} {\"timestamp\": \"2013-08-31T03:32:45Z\", \"page\": \"Striker Eureka\", \"language\" : \"en\", \"user\" : \"speed\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Australia\", \"country\":\"Australia\", \"region\":\"Cantebury\", \"city\":\"Syndey\", \"added\": 459, \"deleted\": 129, \"delta\": 330} {\"timestamp\": \"2013-08-31T07:11:21Z\", \"page\": \"Cherno Alpha\", \"language\" : \"ru\", \"user\" : \"masterYi\", \"unpatrolled\" : \"false\", \"newPage\" : \"true\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"article\", \"continent\":\"Asia\", \"country\":\"Russia\", \"region\":\"Oblast\", \"city\":\"Moscow\", \"added\": 123, \"deleted\": 12, \"delta\": 111} {\"timestamp\": \"2013-08-31T11:58:39Z\", \"page\": \"Crimson Typhoon\", \"language\" : \"zh\", \"user\" : \"triplets\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"China\", \"region\":\"Shanxi\", \"city\":\"Taiyuan\", \"added\": 905, \"deleted\": 5, \"delta\": 900} {\"timestamp\": \"2013-08-31T12:41:27Z\", \"page\": \"Coyote Tango\", \"language\" : \"ja\", \"user\" : \"cancer\", \"unpatrolled\" : \"true\", \"newPage\" : \"false\", \"robot\": \"true\", \"anonymous\": \"false\", \"namespace\":\"wikipedia\", \"continent\":\"Asia\", \"country\":\"Japan\", \"region\":\"Kanto\", \"city\":\"Tokyo\", \"added\": 1, \"deleted\": 10, \"delta\": -9} CSV 2013-08-31T01:02:33Z,\"Gypsy Danger\",\"en\",\"nuclear\",\"true\",\"true\",\"false\",\"false\",\"article\",\"North America\",\"United States\",\"Bay Area\",\"San Francisco\",57,200,-143 2013-08-31T03:32:45Z,\"Striker Eureka\",\"en\",\"speed\",\"false\",\"true\",\"true\",\"false\",\"wikipedia\",\"Australia\",\"Australia\",\"Cantebury\",\"Syndey\",459,129,330 2013-08-31T07:11:21Z,\"Cherno Alpha\",\"ru\",\"masterYi\",\"false\",\"true\",\"true\",\"false\",\"article\",\"Asia\",\"Russia\",\"Oblast\",\"Moscow\",123,12,111 2013-08-31T11:58:39Z,\"Crimson Typhoon\",\"zh\",\"triplets\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"China\",\"Shanxi\",\"Taiyuan\",905,5,900 2013-08-31T12:41:27Z,\"Coyote Tango\",\"ja\",\"cancer\",\"true\",\"false\",\"true\",\"false\",\"wikipedia\",\"Asia\",\"Japan\",\"Kanto\",\"Tokyo\",1,10,-9 TSV (Delimited) 2013-08-31T01:02:33Z \"Gypsy Danger\" \"en\" \"nuclear\" \"true\" \"true\" \"false\" \"false\" \"article\" \"North America\" \"United States\" \"Bay Area\" \"San Francisco\" 57 200 -143 2013-08-31T03:32:45Z \"Striker Eureka\" \"en\" \"speed\" \"false\" \"true\" \"true\" \"false\" \"wikipedia\" \"Australia\" \"Australia\" \"Cantebury\" \"Syndey\" 459 129 330 2013-08-31T07:11:21Z \"Cherno Alpha\" \"ru\" \"masterYi\" \"false\" \"true\" \"true\" \"false\" \"article\" \"Asia\" \"Russia\" \"Oblast\" \"Moscow\" 123 12 111 2013-08-31T11:58:39Z \"Crimson Typhoon\" \"zh\" \"triplets\" \"true\" \"false\" \"true\" \"false\" \"wikipedia\" \"Asia\" \"China\" \"Shanxi\" \"Taiyuan\" 905 5 900 2013-08-31T12:41:27Z \"Coyote Tango\" \"ja\" \"cancer\" \"true\" \"false\" \"true\" \"false\" \"wikipedia\" \"Asia\" \"Japan\" \"Kanto\" \"Tokyo\" 1 10 -9 Note that the CSV and TSV data do not contain column heads. This becomes important when you specify the data for ingesting. Besides text formats, Druid also supports binary formats such as Orc and Parquet formats. Custom Formats Druid supports custom data formats and can use the Regex parser or the JavaScript parsers to parse these formats. Please note that using any of these parsers for parsing data will not be as efficient as writing a native Java parser or using an external stream processor. We welcome contributions of new Parsers. Input Format The Input Format is a new way to specify the data format of your input data which was introduced in 0.17.0. Unfortunately, the Input Format doesn't support all data formats or ingestion methods supported by Druid yet. Especially if you want to use the Hadoop ingestion, you still need to use the Parser. If your data is formatted in some format not listed in this section, please consider using the Parser instead. All forms of Druid ingestion require some form of schema object. The format of the data to be ingested is specified using the inputFormat entry in your ioConfig. JSON The inputFormat to load data of JSON format. An example is: \"ioConfig\": { \"inputFormat\": { \"type\": \"json\" }, ... } The JSON inputFormat has the following components: Field Type Description Required type String This should say json. yes flattenSpec JSON Object Specifies flattening configuration for nested JSON data. See flattenSpec for more info. no featureSpec JSON Object JSON parser features supported by Jackson library. Those features will be applied when parsing the input JSON data. no CSV The inputFormat to load data of the CSV format. An example is: \"ioConfig\": { \"inputFormat\": { \"type\": \"csv\", \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"] }, ... } The CSV inputFormat has the following components: Field Type Description Required type String This should say csv. yes listDelimiter String A custom delimiter for multi-value dimensions. no (default = ctrl+A) columns JSON array Specifies the columns of the data. The columns should be in the same order with the columns of your data. yes if findColumnsFromHeader is false or missing findColumnsFromHeader Boolean If this is set, the task will find the column names from the header row. Note that skipHeaderRows will be applied before finding column names from the header. For example, if you set skipHeaderRows to 2 and findColumnsFromHeader to true, the task will skip the first two lines and then extract column information from the third line. columns will be ignored if this is set to true. no (default = false if columns is set; otherwise null) skipHeaderRows Integer If this is set, the task will skip the first skipHeaderRows rows. no (default = 0) TSV (Delimited) \"ioConfig\": { \"inputFormat\": { \"type\": \"tsv\", \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"], \"delimiter\":\"|\" }, ... } The inputFormat to load data of a delimited format. An example is: Field Type Description Required type String This should say tsv. yes delimiter String A custom delimiter for data values. no (default = \\t) listDelimiter String A custom delimiter for multi-value dimensions. no (default = ctrl+A) columns JSON array Specifies the columns of the data. The columns should be in the same order with the columns of your data. yes if findColumnsFromHeader is false or missing findColumnsFromHeader Boolean If this is set, the task will find the column names from the header row. Note that skipHeaderRows will be applied before finding column names from the header. For example, if you set skipHeaderRows to 2 and findColumnsFromHeader to true, the task will skip the first two lines and then extract column information from the third line. columns will be ignored if this is set to true. no (default = false if columns is set; otherwise null) skipHeaderRows Integer If this is set, the task will skip the first skipHeaderRows rows. no (default = 0) Be sure to change the delimiter to the appropriate delimiter for your data. Like CSV, you must specify the columns and which subset of the columns you want indexed. ORC You need to include the druid-orc-extensions as an extension to use the ORC input format. If you are considering upgrading from earlier than 0.15.0 to 0.15.0 or a higher version, please read Migration from 'contrib' extension carefully. The inputFormat to load data of ORC format. An example is: \"ioConfig\": { \"inputFormat\": { \"type\": \"orc\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"nested\", \"expr\": \"$.path.to.nested\" } ] }, \"binaryAsString\": false }, ... } The ORC inputFormat has the following components: Field Type Description Required type String This should say orc. yes flattenSpec JSON Object Specifies flattening configuration for nested ORC data. See flattenSpec for more info. no binaryAsString Boolean Specifies if the binary orc column which is not logically marked as a string should be treated as a UTF-8 encoded string. no (default = false) Parquet You need to include the druid-parquet-extensions as an extension to use the Parquet input format. The inputFormat to load data of Parquet format. An example is: \"ioConfig\": { \"inputFormat\": { \"type\": \"parquet\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"nested\", \"expr\": \"$.path.to.nested\" } ] }, \"binaryAsString\": false }, ... } The Parquet inputFormat has the following components: Field Type Description Required type String This should be set to parquet to read Parquet file yes flattenSpec JSON Object Define a flattenSpec to extract nested values from a Parquet file. Note that only 'path' expression are supported ('jq' is unavailable). no (default will auto-discover 'root' level properties) binaryAsString Boolean Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. no (default = false) Avro OCF You need to include the druid-avro-extensions as an extension to use the Avro OCF input format. See the Avro Types section for how Avro types are handled in Druid The inputFormat to load data of Avro OCF format. An example is: \"ioConfig\": { \"inputFormat\": { \"type\": \"avro_ocf\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"someRecord_subInt\", \"expr\": \"$.someRecord.subInt\" } ] }, \"schema\": { \"namespace\": \"org.apache.druid.data.input\", \"name\": \"SomeDatum\", \"type\": \"record\", \"fields\" : [ { \"name\": \"timestamp\", \"type\": \"long\" }, { \"name\": \"eventType\", \"type\": \"string\" }, { \"name\": \"id\", \"type\": \"long\" }, { \"name\": \"someRecord\", \"type\": { \"type\": \"record\", \"name\": \"MySubRecord\", \"fields\": [ { \"name\": \"subInt\", \"type\": \"int\"}, { \"name\": \"subLong\", \"type\": \"long\"} ] }}] }, \"binaryAsString\": false }, ... } Field Type Description Required type String This should be set to avro_ocf to read Avro OCF file yes flattenSpec JSON Object Define a flattenSpec to extract nested values from a Avro records. Note that only 'path' expression are supported ('jq' is unavailable). no (default will auto-discover 'root' level properties) schema JSON Object Define a reader schema to be used when parsing Avro records, this is useful when parsing multiple versions of Avro OCF file data no (default will decode using the writer schema contained in the OCF file) binaryAsString Boolean Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. no (default = false) FlattenSpec The flattenSpec is located in inputFormat → flattenSpec and is responsible for bridging the gap between potentially nested input data (such as JSON, Avro, etc) and Druid's flat data model. An example flattenSpec is: \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"name\": \"baz\", \"type\": \"root\" }, { \"name\": \"foo_bar\", \"type\": \"path\", \"expr\": \"$.foo.bar\" }, { \"name\": \"first_food\", \"type\": \"jq\", \"expr\": \".thing.food[1]\" } ] } Conceptually, after input data records are read, the flattenSpec is applied first before any other specs such as timestampSpec, transformSpec, dimensionsSpec, or metricsSpec. Keep this in mind when writing your ingestion spec. Flattening is only supported for data formats that support nesting, including avro, json, orc, and parquet. A flattenSpec can have the following components: Field Description Default useFieldDiscovery If true, interpret all root-level fields as available fields for usage by timestampSpec, transformSpec, dimensionsSpec, and metricsSpec.If false, only explicitly specified fields (see fields) will be available for use. true fields Specifies the fields of interest and how they are accessed. See below for details. [] Field flattening specifications Each entry in the fields list can have the following components: Field Description Default type Options are as follows:`root`, referring to a field at the root level of the record. Only really useful if `useFieldDiscovery` is false.`path`, referring to a field using [JsonPath](https://github.com/jayway/JsonPath) notation. Supported by most data formats that offer nesting, including `avro`, `json`, `orc`, and `parquet`.`jq`, referring to a field using [jackson-jq](https://github.com/eiiches/jackson-jq) notation. Only supported for the `json` format. none (required) name Name of the field after flattening. This name can be referred to by the timestampSpec, transformSpec, dimensionsSpec, and metricsSpec. none (required) expr Expression for accessing the field while flattening. For type path, this should be JsonPath. For type jq, this should be jackson-jq notation. For other types, this parameter is ignored. none (required for types path and jq) Notes on flattening For convenience, when defining a root-level field, it is possible to define only the field name, as a string, instead of a JSON object. For example, {\"name\": \"baz\", \"type\": \"root\"} is equivalent to \"baz\". Enabling useFieldDiscovery will only automatically detect \"simple\" fields at the root level that correspond to data types that Druid supports. This includes strings, numbers, and lists of strings or numbers. Other types will not be automatically detected, and must be specified explicitly in the fields list. Duplicate field names are not allowed. An exception will be thrown. If useFieldDiscovery is enabled, any discovered field with the same name as one already defined in the fields list will be skipped, rather than added twice. http://jsonpath.herokuapp.com/ is useful for testing path-type expressions. jackson-jq supports a subset of the full jq syntax. Please refer to the jackson-jq documentation for details. Parser The Parser is deprecated for native batch tasks, Kafka indexing service, and Kinesis indexing service. Consider using the input format instead for these types of ingestion. This section lists all default and core extension parsers. For community extension parsers, please see our community extensions list. String Parser string typed parsers operate on text based inputs that can be split into individual records by newlines. Each line can be further parsed using parseSpec. Field Type Description Required type String This should say string in general, or hadoopyString when used in a Hadoop indexing job. yes parseSpec JSON Object Specifies the format, timestamp, and dimensions of the data. yes Avro Hadoop Parser You need to include the druid-avro-extensions as an extension to use the Avro Hadoop Parser. See the Avro Types section for how Avro types are handled in Druid This parser is for Hadoop batch ingestion. The inputFormat of inputSpec in ioConfig must be set to \"org.apache.druid.data.input.avro.AvroValueInputFormat\". You may want to set Avro reader's schema in jobProperties in tuningConfig, e.g.: \"avro.schema.input.value.path\": \"/path/to/your/schema.avsc\" or \"avro.schema.input.value\": \"your_schema_JSON_object\". If the Avro reader's schema is not set, the schema in Avro object container file will be used. See Avro specification for more information. Field Type Description Required type String This should say avro_hadoop. yes parseSpec JSON Object Specifies the timestamp and dimensions of the data. Should be an \"avro\" parseSpec. yes fromPigAvroStorage Boolean Specifies whether the data file is stored using AvroStorage. no(default == false) An Avro parseSpec can contain a flattenSpec using either the \"root\" or \"path\" field types, which can be used to read nested Avro records. The \"jq\" field type is not currently supported for Avro. For example, using Avro Hadoop parser with custom reader's schema file: { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"\", \"parser\" : { \"type\" : \"avro_hadoop\", \"parseSpec\" : { \"format\": \"avro\", \"timestampSpec\": , \"dimensionsSpec\": , \"flattenSpec\": } } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"inputFormat\": \"org.apache.druid.data.input.avro.AvroValueInputFormat\", \"paths\" : \"\" } }, \"tuningConfig\" : { \"jobProperties\" : { \"avro.schema.input.value.path\" : \"/path/to/my/schema.avsc\" } } } } ORC Hadoop Parser You need to include the druid-orc-extensions as an extension to use the ORC Hadoop Parser. If you are considering upgrading from earlier than 0.15.0 to 0.15.0 or a higher version, please read Migration from 'contrib' extension carefully. This parser is for Hadoop batch ingestion. The inputFormat of inputSpec in ioConfig must be set to \"org.apache.orc.mapreduce.OrcInputFormat\". Field Type Description Required type String This should say orc yes parseSpec JSON Object Specifies the timestamp and dimensions of the data (timeAndDims and orc format) and a flattenSpec (orc format) yes The parser supports two parseSpec formats: orc and timeAndDims. orc supports auto field discovery and flattening, if specified with a flattenSpec. If no flattenSpec is specified, useFieldDiscovery will be enabled by default. Specifying a dimensionSpec is optional if useFieldDiscovery is enabled: if a dimensionSpec is supplied, the list of dimensions it defines will be the set of ingested dimensions, if missing the discovered fields will make up the list. timeAndDims parse spec must specify which fields will be extracted as dimensions through the dimensionSpec. All column types are supported, with the exception of union types. Columns of list type, if filled with primitives, may be used as a multi-value dimension, or specific elements can be extracted with flattenSpec expressions. Likewise, primitive fields may be extracted from map and struct types in the same manner. Auto field discovery will automatically create a string dimension for every (non-timestamp) primitive or list of primitives, as well as any flatten expressions defined in the flattenSpec. Hadoop job properties Like most Hadoop jobs, the best outcomes will add \"mapreduce.job.user.classpath.first\": \"true\" or \"mapreduce.job.classloader\": \"true\" to the jobProperties section of tuningConfig. Note that it is likely if using \"mapreduce.job.classloader\": \"true\" that you will need to set mapreduce.job.classloader.system.classes to include -org.apache.hadoop.hive. to instruct Hadoop to load org.apache.hadoop.hive classes from the application jars instead of system jars, e.g. ... \"mapreduce.job.classloader\": \"true\", \"mapreduce.job.classloader.system.classes\" : \"java., javax.accessibility., javax.activation., javax.activity., javax.annotation., javax.annotation.processing., javax.crypto., javax.imageio., javax.jws., javax.lang.model., -javax.management.j2ee., javax.management., javax.naming., javax.net., javax.print., javax.rmi., javax.script., -javax.security.auth.message., javax.security.auth., javax.security.cert., javax.security.sasl., javax.sound., javax.sql., javax.swing., javax.tools., javax.transaction., -javax.xml.registry., -javax.xml.rpc., javax.xml., org.w3c.dom., org.xml.sax., org.apache.commons.logging., org.apache.log4j., -org.apache.hadoop.hbase., -org.apache.hadoop.hive., org.apache.hadoop., core-default.xml, hdfs-default.xml, mapred-default.xml, yarn-default.xml\", ... This is due to the hive-storage-api dependency of the orc-mapreduce library, which provides some classes under the org.apache.hadoop.hive package. If instead using the setting \"mapreduce.job.user.classpath.first\": \"true\", then this will not be an issue. Examples orc parser, orc parseSpec, auto field discovery, flatten expressions { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\", \"paths\": \"path/to/file.orc\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"orc\", \"parseSpec\": { \"format\": \"orc\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"nestedDim\", \"expr\": \"$.nestedData.dim1\" }, { \"type\": \"path\", \"name\": \"listDimFirstItem\", \"expr\": \"$.listDim[1]\" } ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"millis\" } } }, ... }, \"tuningConfig\": } } } orc parser, orc parseSpec, field discovery with no flattenSpec or dimensionSpec { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\", \"paths\": \"path/to/file.orc\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"orc\", \"parseSpec\": { \"format\": \"orc\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"millis\" } } }, ... }, \"tuningConfig\": } } } orc parser, orc parseSpec, no autodiscovery { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\", \"paths\": \"path/to/file.orc\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"orc\", \"parseSpec\": { \"format\": \"orc\", \"flattenSpec\": { \"useFieldDiscovery\": false, \"fields\": [ { \"type\": \"path\", \"name\": \"nestedDim\", \"expr\": \"$.nestedData.dim1\" }, { \"type\": \"path\", \"name\": \"listDimFirstItem\", \"expr\": \"$.listDim[1]\" } ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"millis\" }, \"dimensionsSpec\": { \"dimensions\": [ \"dim1\", \"dim3\", \"nestedDim\", \"listDimFirstItem\" ], \"dimensionExclusions\": [], \"spatialDimensions\": [] } } }, ... }, \"tuningConfig\": } } } orc parser, timeAndDims parseSpec { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.orc.mapreduce.OrcInputFormat\", \"paths\": \"path/to/file.orc\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"orc\", \"parseSpec\": { \"format\": \"timeAndDims\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"dim1\", \"dim2\", \"dim3\", \"listDim\" ], \"dimensionExclusions\": [], \"spatialDimensions\": [] } } }, ... }, \"tuningConfig\": } } Parquet Hadoop Parser You need to include the druid-parquet-extensions as an extension to use the Parquet Hadoop Parser. The Parquet Hadoop parser is for Hadoop batch ingestion and parses Parquet files directly. The inputFormat of inputSpec in ioConfig must be set to org.apache.druid.data.input.parquet.DruidParquetInputFormat. The Parquet Hadoop Parser supports auto field discovery and flattening if provided with a flattenSpec with the parquet parseSpec. Parquet nested list and map logical types should operate correctly with JSON path expressions for all supported types. Field Type Description Required type String This should say parquet. yes parseSpec JSON Object Specifies the timestamp and dimensions of the data, and optionally, a flatten spec. Valid parseSpec formats are timeAndDims and parquet yes binaryAsString Boolean Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. no(default = false) When the time dimension is a DateType column, a format should not be supplied. When the format is UTF8 (String), either auto or a explicitly defined format is required. Parquet Hadoop Parser vs Parquet Avro Hadoop Parser Both parsers read from Parquet files, but slightly differently. The main differences are: The Parquet Hadoop Parser uses a simple conversion while the Parquet Avro Hadoop Parser converts Parquet data into avro records first with the parquet-avro library and then parses avro data using the druid-avro-extensions module to ingest into Druid. The Parquet Hadoop Parser sets a hadoop job property parquet.avro.add-list-element-records to false (which normally defaults to true), in order to 'unwrap' primitive list elements into multi-value dimensions. The Parquet Hadoop Parser supports int96 Parquet values, while the Parquet Avro Hadoop Parser does not. There may also be some subtle differences in the behavior of JSON path expression evaluation of flattenSpec. Based on those differences, we suggest using the Parquet Hadoop Parser over the Parquet Avro Hadoop Parser to allow ingesting data beyond the schema constraints of Avro conversion. However, the Parquet Avro Hadoop Parser was the original basis for supporting the Parquet format, and as such it is a bit more mature. Examples parquet parser, parquet parseSpec { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\", \"paths\": \"path/to/file.parquet\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"parquet\", \"parseSpec\": { \"format\": \"parquet\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"nestedDim\", \"expr\": \"$.nestedData.dim1\" }, { \"type\": \"path\", \"name\": \"listDimFirstItem\", \"expr\": \"$.listDim[1]\" } ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [], \"dimensionExclusions\": [], \"spatialDimensions\": [] } } }, ... }, \"tuningConfig\": } } } parquet parser, timeAndDims parseSpec { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetInputFormat\", \"paths\": \"path/to/file.parquet\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"parquet\", \"parseSpec\": { \"format\": \"timeAndDims\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"dim1\", \"dim2\", \"dim3\", \"listDim\" ], \"dimensionExclusions\": [], \"spatialDimensions\": [] } } }, ... }, \"tuningConfig\": } } Parquet Avro Hadoop Parser Consider using the Parquet Hadoop Parser over this parser to ingest Parquet files. See Parquet Hadoop Parser vs Parquet Avro Hadoop Parser for the differences between those parsers. You need to include both the druid-parquet-extensions [druid-avro-extensions] as extensions to use the Parquet Avro Hadoop Parser. The Parquet Avro Hadoop Parser is for Hadoop batch ingestion. This parser first converts the Parquet data into Avro records, and then parses them to ingest into Druid. The inputFormat of inputSpec in ioConfig must be set to org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat. The Parquet Avro Hadoop Parser supports auto field discovery and flattening if provided with a flattenSpec with the avro parseSpec. Parquet nested list and map logical types should operate correctly with JSON path expressions for all supported types. This parser sets a hadoop job property parquet.avro.add-list-element-records to false (which normally defaults to true), in order to 'unwrap' primitive list elements into multi-value dimensions. Note that the int96 Parquet value type is not supported with this parser. Field Type Description Required type String This should say parquet-avro. yes parseSpec JSON Object Specifies the timestamp and dimensions of the data, and optionally, a flatten spec. Should be avro. yes binaryAsString Boolean Specifies if the bytes parquet column which is not logically marked as a string or enum type should be treated as a UTF-8 encoded string. no(default = false) When the time dimension is a DateType column, a format should not be supplied. When the format is UTF8 (String), either auto or an explicitly defined format is required. Example { \"type\": \"index_hadoop\", \"spec\": { \"ioConfig\": { \"type\": \"hadoop\", \"inputSpec\": { \"type\": \"static\", \"inputFormat\": \"org.apache.druid.data.input.parquet.DruidParquetAvroInputFormat\", \"paths\": \"path/to/file.parquet\" }, ... }, \"dataSchema\": { \"dataSource\": \"example\", \"parser\": { \"type\": \"parquet-avro\", \"parseSpec\": { \"format\": \"avro\", \"flattenSpec\": { \"useFieldDiscovery\": true, \"fields\": [ { \"type\": \"path\", \"name\": \"nestedDim\", \"expr\": \"$.nestedData.dim1\" }, { \"type\": \"path\", \"name\": \"listDimFirstItem\", \"expr\": \"$.listDim[1]\" } ] }, \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [], \"dimensionExclusions\": [], \"spatialDimensions\": [] } } }, ... }, \"tuningConfig\": } } } Avro Stream Parser You need to include the druid-avro-extensions as an extension to use the Avro Stream Parser. See the Avro Types section for how Avro types are handled in Druid This parser is for stream ingestion and reads Avro data from a stream directly. Field Type Description Required type String This should say avro_stream. no avroBytesDecoder JSON Object Specifies how to decode bytes to Avro record. yes parseSpec JSON Object Specifies the timestamp and dimensions of the data. Should be an \"avro\" parseSpec. yes An Avro parseSpec can contain a flattenSpec using either the \"root\" or \"path\" field types, which can be used to read nested Avro records. The \"jq\" field type is not currently supported for Avro. For example, using Avro stream parser with schema repo Avro bytes decoder: \"parser\" : { \"type\" : \"avro_stream\", \"avroBytesDecoder\" : { \"type\" : \"schema_repo\", \"subjectAndIdConverter\" : { \"type\" : \"avro_1124\", \"topic\" : \"${YOUR_TOPIC}\" }, \"schemaRepository\" : { \"type\" : \"avro_1124_rest_client\", \"url\" : \"${YOUR_SCHEMA_REPO_END_POINT}\", } }, \"parseSpec\" : { \"format\": \"avro\", \"timestampSpec\": , \"dimensionsSpec\": , \"flattenSpec\": } } Avro Bytes Decoder If type is not included, the avroBytesDecoder defaults to schema_repo. Inline Schema Based Avro Bytes Decoder The \"schema_inline\" decoder reads Avro records using a fixed schema and does not support schema migration. If you may need to migrate schemas in the future, consider one of the other decoders, all of which use a message header that allows the parser to identify the proper Avro schema for reading records. This decoder can be used if all the input events can be read using the same schema. In this case, specify the schema in the input task JSON itself, as described below. ... \"avroBytesDecoder\": { \"type\": \"schema_inline\", \"schema\": { //your schema goes here, for example \"namespace\": \"org.apache.druid.data\", \"name\": \"User\", \"type\": \"record\", \"fields\": [ { \"name\": \"FullName\", \"type\": \"string\" }, { \"name\": \"Country\", \"type\": \"string\" } ] } } ... Multiple Inline Schemas Based Avro Bytes Decoder Use this decoder if different input events can have different read schemas. In this case, specify the schema in the input task JSON itself, as described below. ... \"avroBytesDecoder\": { \"type\": \"multiple_schemas_inline\", \"schemas\": { //your id -> schema map goes here, for example \"1\": { \"namespace\": \"org.apache.druid.data\", \"name\": \"User\", \"type\": \"record\", \"fields\": [ { \"name\": \"FullName\", \"type\": \"string\" }, { \"name\": \"Country\", \"type\": \"string\" } ] }, \"2\": { \"namespace\": \"org.apache.druid.otherdata\", \"name\": \"UserIdentity\", \"type\": \"record\", \"fields\": [ { \"name\": \"Name\", \"type\": \"string\" }, { \"name\": \"Location\", \"type\": \"string\" } ] }, ... ... } } ... Note that it is essentially a map of integer schema ID to avro schema object. This parser assumes that record has following format. first 1 byte is version and must always be 1. next 4 bytes are integer schema ID serialized using big-endian byte order. remaining bytes contain serialized avro message. SchemaRepo Based Avro Bytes Decoder This Avro bytes decoder first extracts subject and id from the input message bytes, and then uses them to look up the Avro schema used to decode the Avro record from bytes. For details, see the schema repo and AVRO-1124. You will need an http service like schema repo to hold the avro schema. For information on registering a schema on the message producer side, see org.apache.druid.data.input.AvroStreamInputRowParserTest#testParse(). Field Type Description Required type String This should say schema_repo. no subjectAndIdConverter JSON Object Specifies how to extract the subject and id from message bytes. yes schemaRepository JSON Object Specifies how to look up the Avro schema from subject and id. yes Avro-1124 Subject And Id Converter This section describes the format of the subjectAndIdConverter object for the schema_repo Avro bytes decoder. Field Type Description Required type String This should say avro_1124. no topic String Specifies the topic of your Kafka stream. yes Avro-1124 Schema Repository This section describes the format of the schemaRepository object for the schema_repo Avro bytes decoder. Field Type Description Required type String This should say avro_1124_rest_client. no url String Specifies the endpoint url of your Avro-1124 schema repository. yes Confluent Schema Registry-based Avro Bytes Decoder This Avro bytes decoder first extracts a unique id from input message bytes, and then uses it to look up the schema in the Schema Registry used to decode the Avro record from bytes. For details, see the Schema Registry documentation and repository. Field Type Description Required type String This should say schema_registry. no url String Specifies the url endpoint of the Schema Registry. yes capacity Integer Specifies the max size of the cache (default = Integer.MAX_VALUE). no ... \"avroBytesDecoder\" : { \"type\" : \"schema_registry\", \"url\" : } ... Protobuf Parser You need to include the druid-protobuf-extensions as an extension to use the Protobuf Parser. This parser is for stream ingestion and reads Protocol buffer data from a stream directly. Field Type Description Required type String This should say protobuf. yes descriptor String Protobuf descriptor file name in the classpath or URL. yes protoMessageType String Protobuf message type in the descriptor. Both short name and fully qualified name are accepted. The parser uses the first message type found in the descriptor if not specified. no parseSpec JSON Object Specifies the timestamp and dimensions of the data. The format must be JSON. See JSON ParseSpec for more configuration options. Note that timeAndDims parseSpec is no longer supported. yes Sample spec: \"parser\": { \"type\": \"protobuf\", \"descriptor\": \"file:///tmp/metrics.desc\", \"protoMessageType\": \"Metrics\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"unit\", \"http_method\", \"http_code\", \"page\", \"metricType\", \"server\" ], \"dimensionExclusions\": [ \"timestamp\", \"value\" ] } } } See the extension description for more details and examples. ParseSpec The Parser is deprecated for native batch tasks, Kafka indexing service, and Kinesis indexing service. Consider using the input format instead for these types of ingestion. ParseSpecs serve two purposes: The String Parser use them to determine the format (i.e., JSON, CSV, TSV) of incoming rows. All Parsers use them to determine the timestamp and dimensions of incoming rows. If format is not included, the parseSpec defaults to tsv. JSON ParseSpec Use this with the String Parser to load JSON. Field Type Description Required format String This should say json. no timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes flattenSpec JSON Object Specifies flattening configuration for nested JSON data. See flattenSpec for more info. no Sample spec: \"parseSpec\": { \"format\" : \"json\", \"timestampSpec\" : { \"column\" : \"timestamp\" }, \"dimensionSpec\" : { \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"] } } JSON Lowercase ParseSpec The jsonLowercase parser is deprecated and may be removed in a future version of Druid. This is a special variation of the JSON ParseSpec that lower cases all the column names in the incoming JSON data. This parseSpec is required if you are updating to Druid 0.7.x from Druid 0.6.x, are directly ingesting JSON with mixed case column names, do not have any ETL in place to lower case those column names, and would like to make queries that include the data you created using 0.6.x and 0.7.x. Field Type Description Required format String This should say jsonLowercase. yes timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes CSV ParseSpec Use this with the String Parser to load CSV. Strings are parsed using the com.opencsv library. Field Type Description Required format String This should say csv. yes timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes listDelimiter String A custom delimiter for multi-value dimensions. no (default = ctrl+A) columns JSON array Specifies the columns of the data. yes Sample spec: \"parseSpec\": { \"format\" : \"csv\", \"timestampSpec\" : { \"column\" : \"timestamp\" }, \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"], \"dimensionsSpec\" : { \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"] } } CSV Index Tasks If your input files contain a header, the columns field is optional and you don't need to set. Instead, you can set the hasHeaderRow field to true, which makes Druid automatically extract the column information from the header. Otherwise, you must set the columns field and ensure that field must match the columns of your input data in the same order. Also, you can skip some header rows by setting skipHeaderRows in your parseSpec. If both skipHeaderRows and hasHeaderRow options are set, skipHeaderRows is first applied. For example, if you set skipHeaderRows to 2 and hasHeaderRow to true, Druid will skip the first two lines and then extract column information from the third line. Note that hasHeaderRow and skipHeaderRows are effective only for non-Hadoop batch index tasks. Other types of index tasks will fail with an exception. Other CSV Ingestion Tasks The columns field must be included and and ensure that the order of the fields matches the columns of your input data in the same order. TSV / Delimited ParseSpec Use this with the String Parser to load any delimited text that does not require special escaping. By default, the delimiter is a tab, so this will load TSV. Field Type Description Required format String This should say tsv. yes timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes delimiter String A custom delimiter for data values. no (default = \\t) listDelimiter String A custom delimiter for multi-value dimensions. no (default = ctrl+A) columns JSON String array Specifies the columns of the data. yes Sample spec: \"parseSpec\": { \"format\" : \"tsv\", \"timestampSpec\" : { \"column\" : \"timestamp\" }, \"columns\" : [\"timestamp\",\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\",\"added\",\"deleted\",\"delta\"], \"delimiter\":\"|\", \"dimensionsSpec\" : { \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"] } } Be sure to change the delimiter to the appropriate delimiter for your data. Like CSV, you must specify the columns and which subset of the columns you want indexed. TSV (Delimited) Index Tasks If your input files contain a header, the columns field is optional and doesn't need to be set. Instead, you can set the hasHeaderRow field to true, which makes Druid automatically extract the column information from the header. Otherwise, you must set the columns field and ensure that field must match the columns of your input data in the same order. Also, you can skip some header rows by setting skipHeaderRows in your parseSpec. If both skipHeaderRows and hasHeaderRow options are set, skipHeaderRows is first applied. For example, if you set skipHeaderRows to 2 and hasHeaderRow to true, Druid will skip the first two lines and then extract column information from the third line. Note that hasHeaderRow and skipHeaderRows are effective only for non-Hadoop batch index tasks. Other types of index tasks will fail with an exception. Other TSV (Delimited) Ingestion Tasks The columns field must be included and and ensure that the order of the fields matches the columns of your input data in the same order. Multi-value dimensions Dimensions can have multiple values for TSV and CSV data. To specify the delimiter for a multi-value dimension, set the listDelimiter in the parseSpec. JSON data can contain multi-value dimensions as well. The multiple values for a dimension must be formatted as a JSON array in the ingested data. No additional parseSpec configuration is needed. Regex ParseSpec \"parseSpec\":{ \"format\" : \"regex\", \"timestampSpec\" : { \"column\" : \"timestamp\" }, \"dimensionsSpec\" : { \"dimensions\" : [] }, \"columns\" : [], \"pattern\" : } The columns field must match the columns of your regex matching groups in the same order. If columns are not provided, default columns names (\"column_1\", \"column2\", ... \"column_n\") will be assigned. Ensure that your column names include all your dimensions. JavaScript ParseSpec \"parseSpec\":{ \"format\" : \"javascript\", \"timestampSpec\" : { \"column\" : \"timestamp\" }, \"dimensionsSpec\" : { \"dimensions\" : [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"] }, \"function\" : \"function(str) { var parts = str.split(\\\"-\\\"); return { one: parts[0], two: parts[1] } }\" } Note with the JavaScript parser that data must be fully parsed and returned as a {key:value} format in the JS logic. This means any flattening or parsing multi-dimensional values must be done here. JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. TimeAndDims ParseSpec Use this with non-String Parsers to provide them with timestamp and dimensions information. Non-String Parsers handle all formatting decisions on their own, without using the ParseSpec. Field Type Description Required format String This should say timeAndDims. yes timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes Orc ParseSpec Use this with the Hadoop ORC Parser to load ORC files. Field Type Description Required format String This should say orc. no timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes flattenSpec JSON Object Specifies flattening configuration for nested JSON data. See flattenSpec for more info. no Parquet ParseSpec Use this with the Hadoop Parquet Parser to load Parquet files. Field Type Description Required format String This should say parquet. no timestampSpec JSON Object Specifies the column and format of the timestamp. yes dimensionsSpec JSON Object Specifies the dimensions of the data. yes flattenSpec JSON Object Specifies flattening configuration for nested JSON data. See flattenSpec for more info. no 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/schema-design.html":{"url":"ingestion/schema-design.html","title":"Schema定义","keywords":"","body":" Druid's data model For general information, check out the documentation on Druid's data model on the main ingestion overview page. The rest of this page discusses tips for users coming from other kinds of systems, as well as general tips and common practices. Druid data is stored in datasources, which are similar to tables in a traditional RDBMS. Druid datasources can be ingested with or without rollup. With rollup enabled, Druid partially aggregates your data during ingestion, potentially reducing its row count, decreasing storage footprint, and improving query performance. With rollup disabled, Druid stores one row for each row in your input data, without any pre-aggregation. Every row in Druid must have a timestamp. Data is always partitioned by time, and every query has a time filter. Query results can also be broken down by time buckets like minutes, hours, days, and so on. All columns in Druid datasources, other than the timestamp column, are either dimensions or metrics. This follows the standard naming convention of OLAP data. Typical production datasources have tens to hundreds of columns. Dimension columns are stored as-is, so they can be filtered on, grouped by, or aggregated at query time. They are always single Strings, arrays of Strings, single Longs, single Doubles or single Floats. Metric columns are stored pre-aggregated, so they can only be aggregated at query time (not filtered or grouped by). They are often stored as numbers (integers or floats) but can also be stored as complex objects like HyperLogLog sketches or approximate quantile sketches. Metrics can be configured at ingestion time even when rollup is disabled, but are most useful when rollup is enabled. If you're coming from a... Relational model (Like Hive or PostgreSQL.) Druid datasources are generally equivalent to tables in a relational database. Druid lookups can act similarly to data-warehouse-style dimension tables, but as you'll see below, denormalization is often recommended if you can get away with it. Common practice for relational data modeling involves normalization: the idea of splitting up data into multiple tables such that data redundancy is reduced or eliminated. For example, in a \"sales\" table, best-practices relational modeling calls for a \"product id\" column that is a foreign key into a separate \"products\" table, which in turn has \"product id\", \"product name\", and \"product category\" columns. This prevents the product name and category from needing to be repeated on different rows in the \"sales\" table that refer to the same product. In Druid, on the other hand, it is common to use totally flat datasources that do not require joins at query time. In the example of the \"sales\" table, in Druid it would be typical to store \"productid\", \"product_name\", and \"product_category\" as dimensions directly in a Druid \"sales\" datasource, without using a separate \"products\" table. Totally flat schemas substantially increase performance, since the need for joins is eliminated at query time. As an an added speed boost, this also allows Druid's query layer to operate directly on compressed dictionary-encoded data. Perhaps counter-intuitively, this does _not substantially increase storage footprint relative to normalized schemas, since Druid uses dictionary encoding to effectively store just a single integer per row for string columns. If necessary, Druid datasources can be partially normalized through the use of lookups, which are the rough equivalent of dimension tables in a relational database. At query time, you would use Druid's SQL LOOKUP function, or native lookup extraction functions, instead of using the JOIN keyword like you would in a relational database. Since lookup tables impose an increase in memory footprint and incur more computational overhead at query time, it is only recommended to do this if you need the ability to update a lookup table and have the changes reflected immediately for already-ingested rows in your main table. Tips for modeling relational data in Druid: Druid datasources do not have primary or unique keys, so skip those. Denormalize if possible. If you need to be able to update dimension / lookup tables periodically and have those changes reflected in already-ingested data, consider partial normalization with lookups. If you need to join two large distributed tables with each other, you must do this before loading the data into Druid. Druid does not support query-time joins of two datasources. Lookups do not help here, since a full copy of each lookup table is stored on each Druid server, so they are not a good choice for large tables. Consider whether you want to enable rollup for pre-aggregation, or whether you want to disable rollup and load your existing data as-is. Rollup in Druid is similar to creating a summary table in a relational model. Time series model (Like OpenTSDB or InfluxDB.) Similar to time series databases, Druid's data model requires a timestamp. Druid is not a timeseries database, but it is a natural choice for storing timeseries data. Its flexible data model allows it to store both timeseries and non-timeseries data, even in the same datasource. To achieve best-case compression and query performance in Druid for timeseries data, it is important to partition and sort by metric name, like timeseries databases often do. See Partitioning and sorting for more details. Tips for modeling timeseries data in Druid: Druid does not think of data points as being part of a \"time series\". Instead, Druid treats each point separately for ingestion and aggregation. Create a dimension that indicates the name of the series that a data point belongs to. This dimension is often called \"metric\" or \"name\". Do not get the dimension named \"metric\" confused with the concept of Druid metrics. Place this first in the list of dimensions in your \"dimensionsSpec\" for best performance (this helps because it improves locality; see partitioning and sorting below for details). Create other dimensions for attributes attached to your data points. These are often called \"tags\" in timeseries database systems. Create metrics corresponding to the types of aggregations that you want to be able to query. Typically this includes \"sum\", \"min\", and \"max\" (in one of the long, float, or double flavors). If you want to be able to compute percentiles or quantiles, use Druid's approximate aggregators. Consider enabling rollup, which will allow Druid to potentially combine multiple points into one row in your Druid datasource. This can be useful if you want to store data at a different time granularity than it is naturally emitted. It is also useful if you want to combine timeseries and non-timeseries data in the same datasource. If you don't know ahead of time what columns you'll want to ingest, use an empty dimensions list to trigger automatic detection of dimension columns. Log aggregation model (Like Elasticsearch or Splunk.) Similar to log aggregation systems, Druid offers inverted indexes for fast searching and filtering. Druid's search capabilities are generally less developed than these systems, and its analytical capabilities are generally more developed. The main data modeling differences between Druid and these systems are that when ingesting data into Druid, you must be more explicit. Druid columns have types specific upfront and Druid does not, at this time, natively support nested data. Tips for modeling log data in Druid: If you don't know ahead of time what columns you'll want to ingest, use an empty dimensions list to trigger automatic detection of dimension columns. If you have nested data, flatten it using a flattenSpec. Consider enabling rollup if you have mainly analytical use cases for your log data. This will mean you lose the ability to retrieve individual events from Druid, but you potentially gain substantial compression and query performance boosts. General tips and best practices Rollup Druid can roll up data as it is ingested to minimize the amount of raw data that needs to be stored. This is a form of summarization or pre-aggregation. For more details, see the Rollup section of the ingestion documentation. Partitioning and sorting Optimally partitioning and sorting your data can have substantial impact on footprint and performance. For more details, see the Partitioning section of the ingestion documentation. Sketches for high cardinality columns When dealing with high cardinality columns like user IDs or other unique IDs, consider using sketches for approximate analysis rather than operating on the actual values. When you ingest data using a sketch, Druid does not store the original raw data, but instead stores a \"sketch\" of it that it can feed into a later computation at query time. Popular use cases for sketches include count-distinct and quantile computation. Each sketch is designed for just one particular kind of computation. In general using sketches serves two main purposes: improving rollup, and reducing memory footprint at query time. Sketches improve rollup ratios because they allow you to collapse multiple distinct values into the same sketch. For example, if you have two rows that are identical except for a user ID (perhaps two users did the same action at the same time), storing them in a count-distinct sketch instead of as-is means you can store the data in one row instead of two. You won't be able to retrieve the user IDs or compute exact distinct counts, but you'll still be able to compute approximate distinct counts, and you'll reduce your storage footprint. Sketches reduce memory footprint at query time because they limit the amount of data that needs to be shuffled between servers. For example, in a quantile computation, instead of needing to send all data points to a central location so they can be sorted and the quantile can be computed, Druid instead only needs to send a sketch of the points. This can reduce data transfer needs to mere kilobytes. For details about the sketches available in Druid, see the approximate aggregators page. If you prefer videos, take a look at Not exactly!, a conference talk about sketches in Druid. String vs numeric dimensions If the user wishes to ingest a column as a numeric-typed dimension (Long, Double or Float), it is necessary to specify the type of the column in the dimensions section of the dimensionsSpec. If the type is omitted, Druid will ingest a column as the default String type. There are performance tradeoffs between string and numeric columns. Numeric columns are generally faster to group on than string columns. But unlike string columns, numeric columns don't have indexes, so they can be slower to filter on. You may want to experiment to find the optimal choice for your use case. For details about how to configure numeric dimensions, see the dimensionsSpec documentation. Secondary timestamps Druid schemas must always include a primary timestamp. The primary timestamp is used for partitioning and sorting your data, so it should be the timestamp that you will most often filter on. Druid is able to rapidly identify and retrieve data corresponding to time ranges of the primary timestamp column. If your data has more than one timestamp, you can ingest the others as secondary timestamps. The best way to do this is to ingest them as long-typed dimensions in milliseconds format. If necessary, you can get them into this format using a transformSpec and expressions like timestamp_parse, which returns millisecond timestamps. At query time, you can query secondary timestamps with SQL time functions like MILLIS_TO_TIMESTAMP, TIME_FLOOR, and others. If you're using native Druid queries, you can use expressions. Nested dimensions At the time of this writing, Druid does not support nested dimensions. Nested dimensions need to be flattened. For example, if you have data of the following form: {\"foo\":{\"bar\": 3}} then before indexing it, you should transform it to: {\"foo_bar\": 3} Druid is capable of flattening JSON, Avro, or Parquet input data. Please read about flattenSpec for more details. Counting the number of ingested events When rollup is enabled, count aggregators at query time do not actually tell you the number of rows that have been ingested. They tell you the number of rows in the Druid datasource, which may be smaller than the number of rows ingested. In this case, a count aggregator at ingestion time can be used to count the number of events. However, it is important to note that when you query for this metric, you should use a longSum aggregator. A count aggregator at query time will return the number of Druid rows for the time interval, which can be used to determine what the roll-up ratio was. To clarify with an example, if your ingestion spec contains: ... \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, ... You should query for the number of ingested rows with: ... \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"numIngestedEvents\", \"fieldName\": \"count\" }, ... Schema-less dimensions If the dimensions field is left empty in your ingestion spec, Druid will treat every column that is not the timestamp column, a dimension that has been excluded, or a metric column as a dimension. Note that when using schema-less ingestion, all dimensions will be ingested as String-typed dimensions. Including the same column as a dimension and a metric One workflow with unique IDs is to be able to filter on a particular ID, while still being able to do fast unique counts on the ID column. If you are not using schema-less dimensions, this use case is supported by setting the name of the metric to something different than the dimension. If you are using schema-less dimensions, the best practice here is to include the same column twice, once as a dimension, and as a hyperUnique metric. This may involve some work at ETL time. As an example, for schema-less dimensions, repeat the same column: {\"device_id_dim\":123, \"device_id_met\":123} and in your metricsSpec, include: { \"type\" : \"hyperUnique\", \"name\" : \"devices\", \"fieldName\" : \"device_id_met\" } device_id_dim should automatically get picked up as a dimension. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"ingestion/data-management.html":{"url":"ingestion/data-management.html","title":"数据管理","keywords":"","body":" Schema changes Schemas for datasources can change at any time and Apache Druid supports different schemas among segments. Replacing segments Druid uniquely identifies segments using the datasource, interval, version, and partition number. The partition number is only visible in the segment id if there are multiple segments created for some granularity of time. For example, if you have hourly segments, but you have more data in an hour than a single segment can hold, you can create multiple segments for the same hour. These segments will share the same datasource, interval, and version, but have linearly increasing partition numbers. foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-01/2015-01-02_v1_1 foo_2015-01-01/2015-01-02_v1_2 In the example segments above, the dataSource = foo, interval = 2015-01-01/2015-01-02, version = v1, partitionNum = 0. If at some later point in time, you reindex the data with a new schema, the newly created segments will have a higher version id. foo_2015-01-01/2015-01-02_v2_0 foo_2015-01-01/2015-01-02_v2_1 foo_2015-01-01/2015-01-02_v2_2 Druid batch indexing (either Hadoop-based or IndexTask-based) guarantees atomic updates on an interval-by-interval basis. In our example, until all v2 segments for 2015-01-01/2015-01-02 are loaded in a Druid cluster, queries exclusively use v1 segments. Once all v2 segments are loaded and queryable, all queries ignore v1 segments and switch to the v2 segments. Shortly afterwards, the v1 segments are unloaded from the cluster. Note that updates that span multiple segment intervals are only atomic within each interval. They are not atomic across the entire update. For example, you have segments such as the following: foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-02/2015-01-03_v1_1 foo_2015-01-03/2015-01-04_v1_2 v2 segments will be loaded into the cluster as soon as they are built and replace v1 segments for the period of time the segments overlap. Before v2 segments are completely loaded, your cluster may have a mixture of v1 and v2 segments. foo_2015-01-01/2015-01-02_v1_0 foo_2015-01-02/2015-01-03_v2_1 foo_2015-01-03/2015-01-04_v1_2 In this case, queries may hit a mixture of v1 and v2 segments. Different schemas among segments Druid segments for the same datasource may have different schemas. If a string column (dimension) exists in one segment but not another, queries that involve both segments still work. Queries for the segment missing the dimension will behave as if the dimension has only null values. Similarly, if one segment has a numeric column (metric) but another does not, queries on the segment missing the metric will generally \"do the right thing\". Aggregations over this missing metric behave as if the metric were missing. Compaction and reindexing Compaction is a type of overwrite operation, which reads an existing set of segments, combines them into a new set with larger but fewer segments, and overwrites the original set with the new compacted set, without changing the data that is stored. For performance reasons, it is sometimes beneficial to compact a set of segments into a set of larger but fewer segments, as there is some per-segment processing and memory overhead in both the ingestion and querying paths. Compaction tasks merge all segments of the given interval. The syntax is: { \"type\": \"compact\", \"id\": , \"dataSource\": , \"ioConfig\": , \"dimensionsSpec\" , \"metricsSpec\" , \"segmentGranularity\": , \"tuningConfig\" , \"context\": } Field Description Required type Task type. Should be compact Yes id Task id No dataSource DataSource name to be compacted Yes ioConfig ioConfig for compaction task. See Compaction IOConfig for details. Yes dimensionsSpec Custom dimensionsSpec. Compaction task will use this dimensionsSpec if exist instead of generating one. See below for more details. No metricsSpec Custom metricsSpec. Compaction task will use this metricsSpec if specified rather than generating one. No segmentGranularity If this is set, compactionTask will change the segment granularity for the given interval. See segmentGranularity of granularitySpec for more details. See the below table for the behavior. No tuningConfig Parallel indexing task tuningConfig No context Task context No An example of compaction task is { \"type\" : \"compact\", \"dataSource\" : \"wikipedia\", \"ioConfig\" : { \"type\": \"compact\", \"inputSpec\": { \"type\": \"interval\", \"interval\": \"2017-01-01/2018-01-01\" } } } This compaction task reads all segments of the interval 2017-01-01/2018-01-01 and results in new segments. Since segmentGranularity is null, the original segment granularity will be remained and not changed after compaction. To control the number of result segments per time chunk, you can set maxRowsPerSegment or numShards. Please note that you can run multiple compactionTasks at the same time. For example, you can run 12 compactionTasks per month instead of running a single task for the entire year. A compaction task internally generates an index task spec for performing compaction work with some fixed parameters. For example, its inputSource is always the DruidInputSource, and dimensionsSpec and metricsSpec include all dimensions and metrics of the input segments by default. Compaction tasks will exit with a failure status code, without doing anything, if the interval you specify has no data segments loaded in it (or if the interval you specify is empty). The output segment can have different metadata from the input segments unless all input segments have the same metadata. Dimensions: since Apache Druid supports schema change, the dimensions can be different across segments even if they are a part of the same dataSource. If the input segments have different dimensions, the output segment basically includes all dimensions of the input segments. However, even if the input segments have the same set of dimensions, the dimension order or the data type of dimensions can be different. For example, the data type of some dimensions can be changed from string to primitive types, or the order of dimensions can be changed for better locality. In this case, the dimensions of recent segments precede that of old segments in terms of data types and the ordering. This is because more recent segments are more likely to have the new desired order and data types. If you want to use your own ordering and types, you can specify a custom dimensionsSpec in the compaction task spec. Roll-up: the output segment is rolled up only when rollup is set for all input segments. See Roll-up for more details. You can check that your segments are rolled up or not by using Segment Metadata Queries. Compaction IOConfig The compaction IOConfig requires specifying inputSpec as seen below. Field Description Required type Task type. Should be compact Yes inputSpec Input specification Yes There are two supported inputSpecs for now. The interval inputSpec is: Field Description Required type Task type. Should be interval Yes interval Interval to compact Yes The segments inputSpec is: Field Description Required type Task type. Should be segments Yes segments A list of segment IDs Yes Adding new data Druid can insert new data to an existing datasource by appending new segments to existing segment sets. It can also add new data by merging an existing set of segments with new data and overwriting the original set. Druid does not support single-record updates by primary key. Updating existing data Once you ingest some data in a dataSource for an interval and create Apache Druid segments, you might want to make changes to the ingested data. There are several ways this can be done. Using lookups If you have a dimension where values need to be updated frequently, try first using lookups. A classic use case of lookups is when you have an ID dimension stored in a Druid segment, and want to map the ID dimension to a human-readable String value that may need to be updated periodically. Reingesting data If lookup-based techniques are not sufficient, you will need to reingest data into Druid for the time chunks that you want to update. This can be done using one of the batch ingestion methods in overwrite mode (the default mode). It can also be done using streaming ingestion, provided you drop data for the relevant time chunks first. If you do the reingestion in batch mode, Druid's atomic update mechanism means that queries will flip seamlessly from the old data to the new data. We recommend keeping a copy of your raw data around in case you ever need to reingest it. With Hadoop-based ingestion This section assumes the reader understands how to do batch ingestion using Hadoop. See Hadoop batch ingestion for more information. Hadoop batch-ingestion can be used for reindexing and delta ingestion. Druid uses an inputSpec in the ioConfig to know where the data to be ingested is located and how to read it. For simple Hadoop batch ingestion, static or granularity spec types allow you to read data stored in deep storage. There are other types of inputSpec to enable reindexing and delta ingestion. Reindexing with Native Batch Ingestion This section assumes the reader understands how to do batch ingestion without Hadoop using native batch indexing, which uses an inputSource to know where and how to read the input data. The DruidInputSource can be used to read data from segments inside Druid. Note that IndexTask is to be used for prototyping purposes only as it has to do all processing inside a single process and can't scale. Please use Hadoop batch ingestion for production scenarios dealing with more than 1GB of data. Deleting data Druid supports permanent deletion of segments that are in an \"unused\" state (see the Segment lifecycle section of the Architecture page). The Kill Task deletes unused segments within a specified interval from metadata storage and deep storage. For more information, please see Kill Task. Permanent deletion of a segment in Apache Druid has two steps: The segment must first be marked as \"unused\". This occurs when a segment is dropped by retention rules, and when a user manually disables a segment through the Coordinator API. After segments have been marked as \"unused\", a Kill Task will delete any \"unused\" segments from Druid's metadata store as well as deep storage. For documentation on retention rules, please see Data Retention. For documentation on disabling segments using the Coordinator API, please see the Coordinator Datasources API reference. A data deletion tutorial is available at Tutorial: Deleting data Kill Task Kill tasks delete all information about a segment and removes it from deep storage. Segments to kill must be unused (used==0) in the Druid segment table. The available grammar is: { \"type\": \"kill\", \"id\": , \"dataSource\": , \"interval\" : , \"context\": } Retention Druid supports retention rules, which are used to define intervals of time where data should be preserved, and intervals where data should be discarded. Druid also supports separating Historical processes into tiers, and the retention rules can be configured to assign data for specific intervals to specific tiers. These features are useful for performance/cost management; a common use case is separating Historical processes into a \"hot\" tier and a \"cold\" tier. For more information, please see Load rules. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"development/extensions-core/kafka-ingestion.html":{"url":"development/extensions-core/kafka-ingestion.html","title":"从Kafka摄入","keywords":"","body":" The Kafka indexing service enables the configuration of supervisors on the Overlord, which facilitate ingestion from Kafka by managing the creation and lifetime of Kafka indexing tasks. These indexing tasks read events using Kafka's own partition and offset mechanism and are therefore able to provide guarantees of exactly-once ingestion. The supervisor oversees the state of the indexing tasks to coordinate handoffs, manage failures, and ensure that the scalability and replication requirements are maintained. This service is provided in the druid-kafka-indexing-service core Apache Druid extension (see Including Extensions). The Kafka indexing service supports transactional topics which were introduced in Kafka 0.11.x. It is the default behavior of Druid and make the Kafka consumer that Druid uses incompatible with older brokers. Ensure that your Kafka brokers are version 0.11.x or better before using this functionality. Refer Kafka upgrade guide if you are using older version of Kafka brokers. In addition, users could set isolation.level read_uncommitted in consumerProperties, if don't need Druid to consume transactional topics or need Druid to consume older versions of Kafka. Make sure offsets are sequential, since there is no offset gap check in Druid anymore. Tutorial This page contains reference documentation for Apache Kafka-based ingestion. For a walk-through instead, check out the Loading from Apache Kafka tutorial. Submitting a Supervisor Spec The Kafka indexing service requires that the druid-kafka-indexing-service extension be loaded on both the Overlord and the MiddleManagers. A supervisor for a dataSource is started by submitting a supervisor spec via HTTP POST to http://:/druid/indexer/v1/supervisor, for example: curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor A sample supervisor spec is shown below: { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"metrics-kafka\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [], \"dimensionExclusions\": [ \"timestamp\", \"value\" ] }, \"metricsSpec\": [ { \"name\": \"count\", \"type\": \"count\" }, { \"name\": \"value_sum\", \"fieldName\": \"value\", \"type\": \"doubleSum\" }, { \"name\": \"value_min\", \"fieldName\": \"value\", \"type\": \"doubleMin\" }, { \"name\": \"value_max\", \"fieldName\": \"value\", \"type\": \"doubleMax\" } ], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"HOUR\", \"queryGranularity\": \"NONE\" } }, \"ioConfig\": { \"topic\": \"metrics\", \"inputFormat\": { \"type\": \"json\" }, \"consumerProperties\": { \"bootstrap.servers\": \"localhost:9092\" }, \"taskCount\": 1, \"replicas\": 1, \"taskDuration\": \"PT1H\" }, \"tuningConfig\": { \"type\": \"kafka\", \"maxRowsPerSegment\": 5000000 } } Supervisor Configuration Field Description Required type The supervisor type, this should always be kafka. yes dataSchema The schema that will be used by the Kafka indexing task during ingestion. See dataSchema for details. yes ioConfig A KafkaSupervisorIOConfig object for configuring Kafka connection and I/O-related settings for the supervisor and indexing task. See KafkaSupervisorIOConfig below. yes tuningConfig A KafkaSupervisorTuningConfig object for configuring performance-related settings for the supervisor and indexing tasks. See KafkaSupervisorTuningConfig below. no KafkaSupervisorIOConfig Field Type Description Required topic String The Kafka topic to read from. This must be a specific topic as topic patterns are not supported. yes inputFormat Object inputFormat to specify how to parse input data. See the below section for details about specifying the input format. yes consumerProperties Map A map of properties to be passed to the Kafka consumer. See next section for more information. yes pollTimeout Long The length of time to wait for the Kafka consumer to poll records, in milliseconds no (default == 100) replicas Integer The number of replica sets, where 1 means a single set of tasks (no replication). Replica tasks will always be assigned to different workers to provide resiliency against process failure. no (default == 1) taskCount Integer The maximum number of reading tasks in a replica set. This means that the maximum number of reading tasks will be taskCount * replicas and the total number of tasks (reading + publishing) will be higher than this. See Capacity Planning below for more details. The number of reading tasks will be less than taskCount if taskCount > {numKafkaPartitions}. no (default == 1) taskDuration ISO8601 Period The length of time before tasks stop reading and begin publishing their segment. no (default == PT1H) startDelay ISO8601 Period The period to wait before the supervisor starts managing tasks. no (default == PT5S) period ISO8601 Period How often the supervisor will execute its management logic. Note that the supervisor will also run in response to certain events (such as tasks succeeding, failing, and reaching their taskDuration) so this value specifies the maximum time between iterations. no (default == PT30S) useEarliestOffset Boolean If a supervisor is managing a dataSource for the first time, it will obtain a set of starting offsets from Kafka. This flag determines whether it retrieves the earliest or latest offsets in Kafka. Under normal circumstances, subsequent tasks will start from where the previous segments ended so this flag will only be used on first run. no (default == false) completionTimeout ISO8601 Period The length of time to wait before declaring a publishing task as failed and terminating it. If this is set too low, your tasks may never publish. The publishing clock for a task begins roughly after taskDuration elapses. no (default == PT30M) lateMessageRejectionStartDateTime ISO8601 DateTime Configure tasks to reject messages with timestamps earlier than this date time; for example if this is set to 2016-01-01T11:00Z and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps earlier than 2016-01-01T11:00Z will be dropped. This may help prevent concurrency issues if your data stream has late messages and you have multiple pipelines that need to operate on the same segments (e.g. a realtime and a nightly batch ingestion pipeline). no (default == none) lateMessageRejectionPeriod ISO8601 Period Configure tasks to reject messages with timestamps earlier than this period before the task was created; for example if this is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps earlier than 2016-01-01T11:00Z will be dropped. This may help prevent concurrency issues if your data stream has late messages and you have multiple pipelines that need to operate on the same segments (e.g. a realtime and a nightly batch ingestion pipeline). Please note that only one of lateMessageRejectionPeriod or lateMessageRejectionStartDateTime can be specified. no (default == none) earlyMessageRejectionPeriod ISO8601 Period Configure tasks to reject messages with timestamps later than this period after the task reached its taskDuration; for example if this is set to PT1H, the taskDuration is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps later than 2016-01-01T14:00Z will be dropped. Note: Tasks sometimes run past their task duration, for example, in cases of supervisor failover. Setting earlyMessageRejectionPeriod too low may cause messages to be dropped unexpectedly whenever a task runs past its originally configured task duration. no (default == none) More on consumerProperties This must contain a property bootstrap.servers with a list of Kafka brokers in the form: :,:,.... By default, isolation.level is set to read_committed. It should be set to read_uncommitted if you don't want Druid to consume only committed transactions or working with older versions of Kafka servers with no Transactions support. There are few cases that require fetching few/all of consumer properties at runtime e.g. when bootstrap.servers is not known upfront or not static, to enable SSL connections users might have to provide passwords for keystore, truststore and key secretly. For such consumer properties, user can implement a DynamicConfigProvider to supply them at runtime, by adding druid.dynamic.config.provider={\"type\": \"\", ...} in consumerProperties map. Note: In 0.20.0 or older Druid versions, for SSL connections, the keystore, truststore and key passwords can also be provided as a Password Provider. This is deprecated. Specifying data format Kafka indexing service supports both inputFormat and parser to specify the data format. The inputFormat is a new and recommended way to specify the data format for Kafka indexing service, but unfortunately, it doesn't support all data formats supported by the legacy parser. (They will be supported in the future.) The supported inputFormats include csv, delimited, and json. You can also read avro_stream, protobuf, and thrift formats using parser. KafkaSupervisorTuningConfig The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. Field Type Description Required type String The indexing task type, this should always be kafka. yes maxRowsInMemory Integer The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set. no (default == 1000000) maxBytesInMemory Long The number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. Normally this is computed internally and user does not need to set it. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists). no (default == One-sixth of max JVM memory) maxRowsPerSegment Integer The number of rows to aggregate into a segment; this number is post-aggregation rows. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == 5000000) maxTotalRows Long The number of rows to aggregate across all segments; this number is post-aggregation rows. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == unlimited) intermediatePersistPeriod ISO8601 Period The period that determines the rate at which intermediate persists occur. no (default == PT10M) maxPendingPersists Integer Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). no (default == 0, meaning one persist can be running concurrently with ingestion, and none can be queued up) indexSpec Object Tune how data is indexed. See IndexSpec for more information. no indexSpecForIntermediatePersists Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. This can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. However, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see IndexSpec for possible values. no (default = same as indexSpec) reportParseExceptions Boolean DEPRECATED. If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped. Setting reportParseExceptions to true will override existing configurations for maxParseExceptions and maxSavedParseExceptions, setting maxParseExceptions to 0 and limiting maxSavedParseExceptions to no more than 1. no (default == false) handoffConditionTimeout Long Milliseconds to wait for segment handoff. It must be >= 0, where 0 means to wait forever. no (default == 0) resetOffsetAutomatically Boolean Controls behavior when Druid needs to read Kafka messages that are no longer available (i.e. when OffsetOutOfRangeException is encountered).If false, the exception will bubble up, which will cause your tasks to fail and ingestion to halt. If this occurs, manual intervention is required to correct the situation; potentially using the Reset Supervisor API. This mode is useful for production, since it will make you aware of issues with ingestion.If true, Druid will automatically reset to the earlier or latest offset available in Kafka, based on the value of the useEarliestOffset property (earliest if true, latest if false). Please note that this can lead to data being DROPPED (if useEarliestOffset is false) or DUPLICATED (if useEarliestOffset is true) without your knowledge. Messages will be logged indicating that a reset has occurred, but ingestion will continue. This mode is useful for non-production situations, since it will make Druid attempt to recover from problems automatically, even if they lead to quiet dropping or duplicating of data.This feature behaves similarly to the Kafka auto.offset.reset consumer property. no (default == false) workerThreads Integer The number of threads that the supervisor uses to handle requests/responses for worker tasks, along with any other internal asynchronous operation. no (default == min(10, taskCount)) chatThreads Integer The number of threads that will be used for communicating with indexing tasks. no (default == min(10, taskCount * replicas)) chatRetries Integer The number of times HTTP requests to indexing tasks will be retried before considering tasks unresponsive. no (default == 8) httpTimeout ISO8601 Period How long to wait for a HTTP response from an indexing task. no (default == PT10S) shutdownTimeout ISO8601 Period How long to wait for the supervisor to attempt a graceful shutdown of tasks before exiting. no (default == PT80S) offsetFetchPeriod ISO8601 Period How often the supervisor queries Kafka and the indexing tasks to fetch current offsets and calculate lag. no (default == PT30S, min == PT5S) segmentWriteOutMediumFactory Object Segment write-out medium to use when creating segments. See below for more information. no (not specified by default, the value from druid.peon.defaultSegmentWriteOutMediumFactory.type is used) intermediateHandoffPeriod ISO8601 Period How often the tasks should hand off segments. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == P2147483647D) logParseExceptions Boolean If true, log an error message when a parsing exception occurs, containing information about the row where the error occurred. no, default == false maxParseExceptions Integer The maximum number of parse exceptions that can occur before the task halts ingestion and fails. Overridden if reportParseExceptions is set. no, unlimited default maxSavedParseExceptions Integer When a parse exception occurs, Druid can keep track of the most recent parse exceptions. \"maxSavedParseExceptions\" limits how many exception instances will be saved. These saved exceptions will be made available after the task finishes in the task completion report. Overridden if reportParseExceptions is set. no, default == 0 IndexSpec Field Type Description Required bitmap Object Compression format for bitmap indexes. Should be a JSON object. See Bitmap types below for options. no (defaults to Roaring) dimensionCompression String Compression format for dimension columns. Choose from LZ4, LZF, or uncompressed. no (default == LZ4) metricCompression String Compression format for primitive type metric columns. Choose from LZ4, LZF, uncompressed, or none. no (default == LZ4) longEncoding String Encoding format for metric and dimension columns with type long. Choose from auto or longs. auto encodes the values using offset or lookup table depending on column cardinality, and store them with variable size. longs stores the value as is with 8 bytes each. no (default == longs) Bitmap types For Roaring bitmaps: Field Type Description Required type String Must be roaring. yes compressRunOnSerialization Boolean Use a run-length encoding where it is estimated as more space efficient. no (default == true) For Concise bitmaps: Field Type Description Required type String Must be concise. yes SegmentWriteOutMediumFactory Field Type Description Required type String See Additional Peon Configuration: SegmentWriteOutMediumFactory for explanation and available options. yes Operations This section gives descriptions of how some supervisor APIs work specifically in Kafka Indexing Service. For all supervisor APIs, please check Supervisor APIs. Getting Supervisor Status Report GET /druid/indexer/v1/supervisor//status returns a snapshot report of the current state of the tasks managed by the given supervisor. This includes the latest offsets as reported by Kafka, the consumer lag per partition, as well as the aggregate lag of all partitions. The consumer lag per partition may be reported as negative values if the supervisor has not received a recent latest offset response from Kafka. The aggregate lag value will always be >= 0. The status report also contains the supervisor's state and a list of recently thrown exceptions (reported as recentErrors, whose max size can be controlled using the druid.supervisor.maxStoredExceptionEvents configuration). There are two fields related to the supervisor's state - state and detailedState. The state field will always be one of a small number of generic states that are applicable to any type of supervisor, while the detailedState field will contain a more descriptive, implementation-specific state that may provide more insight into the supervisor's activities than the generic state field. The list of possible state values are: [PENDING, RUNNING, SUSPENDED, STOPPING, UNHEALTHY_SUPERVISOR, UNHEALTHY_TASKS] The list of detailedState values and their corresponding state mapping is as follows: Detailed State Corresponding State Description UNHEALTHY_SUPERVISOR UNHEALTHY_SUPERVISOR The supervisor has encountered errors on the past druid.supervisor.unhealthinessThreshold iterations UNHEALTHY_TASKS UNHEALTHY_TASKS The last druid.supervisor.taskUnhealthinessThreshold tasks have all failed UNABLE_TO_CONNECT_TO_STREAM UNHEALTHY_SUPERVISOR The supervisor is encountering connectivity issues with Kafka and has not successfully connected in the past LOST_CONTACT_WITH_STREAM UNHEALTHY_SUPERVISOR The supervisor is encountering connectivity issues with Kafka but has successfully connected in the past PENDING (first iteration only) PENDING The supervisor has been initialized and hasn't started connecting to the stream CONNECTING_TO_STREAM (first iteration only) RUNNING The supervisor is trying to connect to the stream and update partition data DISCOVERING_INITIAL_TASKS (first iteration only) RUNNING The supervisor is discovering already-running tasks CREATING_TASKS (first iteration only) RUNNING The supervisor is creating tasks and discovering state RUNNING RUNNING The supervisor has started tasks and is waiting for taskDuration to elapse SUSPENDED SUSPENDED The supervisor has been suspended STOPPING STOPPING The supervisor is stopping On each iteration of the supervisor's run loop, the supervisor completes the following tasks in sequence: 1) Fetch the list of partitions from Kafka and determine the starting offset for each partition (either based on the last processed offset if continuing, or starting from the beginning or ending of the stream if this is a new topic). 2) Discover any running indexing tasks that are writing to the supervisor's datasource and adopt them if they match the supervisor's configuration, else signal them to stop. 3) Send a status request to each supervised task to update our view of the state of the tasks under our supervision. 4) Handle tasks that have exceeded taskDuration and should transition from the reading to publishing state. 5) Handle tasks that have finished publishing and signal redundant replica tasks to stop. 6) Handle tasks that have failed and clean up the supervisor's internal state. 7) Compare the list of healthy tasks to the requested taskCount and replicas configurations and create additional tasks if required. The detailedState field will show additional values (those marked with \"first iteration only\") the first time the supervisor executes this run loop after startup or after resuming from a suspension. This is intended to surface initialization-type issues, where the supervisor is unable to reach a stable state (perhaps because it can't connect to Kafka, it can't read from the Kafka topic, or it can't communicate with existing tasks). Once the supervisor is stable - that is, once it has completed a full execution without encountering any issues - detailedState will show a RUNNING state until it is stopped, suspended, or hits a failure threshold and transitions to an unhealthy state. Getting Supervisor Ingestion Stats Report GET /druid/indexer/v1/supervisor//stats returns a snapshot of the current ingestion row counters for each task being managed by the supervisor, along with moving averages for the row counters. See Task Reports: Row Stats for more information. Supervisor Health Check GET /druid/indexer/v1/supervisor//health returns 200 OK if the supervisor is healthy and 503 Service Unavailable if it is unhealthy. Healthiness is determined by the supervisor's state (as returned by the /status endpoint) and the druid.supervisor.* Overlord configuration thresholds. Updating Existing Supervisors POST /druid/indexer/v1/supervisor can be used to update existing supervisor spec. Calling this endpoint when there is already an existing supervisor for the same dataSource will cause: The running supervisor to signal its managed tasks to stop reading and begin publishing. The running supervisor to exit. A new supervisor to be created using the configuration provided in the request body. This supervisor will retain the existing publishing tasks and will create new tasks starting at the offsets the publishing tasks ended on. Seamless schema migrations can thus be achieved by simply submitting the new schema using this endpoint. Suspending and Resuming Supervisors You can suspend and resume a supervisor using POST /druid/indexer/v1/supervisor//suspend and POST /druid/indexer/v1/supervisor//resume, respectively. Note that the supervisor itself will still be operating and emitting logs and metrics, it will just ensure that no indexing tasks are running until the supervisor is resumed. Resetting Supervisors The POST /druid/indexer/v1/supervisor//reset operation clears stored offsets, causing the supervisor to start reading offsets from either the earliest or latest offsets in Kafka (depending on the value of useEarliestOffset). After clearing stored offsets, the supervisor kills and recreates any active tasks, so that tasks begin reading from valid offsets. Use care when using this operation! Resetting the supervisor may cause Kafka messages to be skipped or read twice, resulting in missing or duplicate data. The reason for using this operation is to recover from a state in which the supervisor ceases operating due to missing offsets. The indexing service keeps track of the latest persisted Kafka offsets in order to provide exactly-once ingestion guarantees across tasks. Subsequent tasks must start reading from where the previous task completed in order for the generated segments to be accepted. If the messages at the expected starting offsets are no longer available in Kafka (typically because the message retention period has elapsed or the topic was removed and re-created) the supervisor will refuse to start and in flight tasks will fail. This operation enables you to recover from this condition. Note that the supervisor must be running for this endpoint to be available. Terminating Supervisors The POST /druid/indexer/v1/supervisor//terminate operation terminates a supervisor and causes all associated indexing tasks managed by this supervisor to immediately stop and begin publishing their segments. This supervisor will still exist in the metadata store and it's history may be retrieved with the supervisor history API, but will not be listed in the 'get supervisors' API response nor can it's configuration or status report be retrieved. The only way this supervisor can start again is by submitting a functioning supervisor spec to the create API. Capacity Planning Kafka indexing tasks run on MiddleManagers and are thus limited by the resources available in the MiddleManager cluster. In particular, you should make sure that you have sufficient worker capacity (configured using the druid.worker.capacity property) to handle the configuration in the supervisor spec. Note that worker capacity is shared across all types of indexing tasks, so you should plan your worker capacity to handle your total indexing load (e.g. batch processing, realtime tasks, merging tasks, etc.). If your workers run out of capacity, Kafka indexing tasks will queue and wait for the next available worker. This may cause queries to return partial results but will not result in data loss (assuming the tasks run before Kafka purges those offsets). A running task will normally be in one of two states: reading or publishing. A task will remain in reading state for taskDuration, at which point it will transition to publishing state. A task will remain in publishing state for as long as it takes to generate segments, push segments to deep storage, and have them be loaded and served by a Historical process (or until completionTimeout elapses). The number of reading tasks is controlled by replicas and taskCount. In general, there will be replicas * taskCount reading tasks, the exception being if taskCount > {numKafkaPartitions} in which case {numKafkaPartitions} tasks will be used instead. When taskDuration elapses, these tasks will transition to publishing state and replicas * taskCount new reading tasks will be created. Therefore to allow for reading tasks and publishing tasks to run concurrently, there should be a minimum capacity of: workerCapacity = 2 * replicas * taskCount This value is for the ideal situation in which there is at most one set of tasks publishing while another set is reading. In some circumstances, it is possible to have multiple sets of tasks publishing simultaneously. This would happen if the time-to-publish (generate segment, push to deep storage, loaded on Historical) > taskDuration. This is a valid scenario (correctness-wise) but requires additional worker capacity to support. In general, it is a good idea to have taskDuration be large enough that the previous set of tasks finishes publishing before the current set begins. Supervisor Persistence When a supervisor spec is submitted via the POST /druid/indexer/v1/supervisor endpoint, it is persisted in the configured metadata database. There can only be a single supervisor per dataSource, and submitting a second spec for the same dataSource will overwrite the previous one. When an Overlord gains leadership, either by being started or as a result of another Overlord failing, it will spawn a supervisor for each supervisor spec in the metadata database. The supervisor will then discover running Kafka indexing tasks and will attempt to adopt them if they are compatible with the supervisor's configuration. If they are not compatible because they have a different ingestion spec or partition allocation, the tasks will be killed and the supervisor will create a new set of tasks. In this way, the supervisors are persistent across Overlord restarts and fail-overs. A supervisor is stopped via the POST /druid/indexer/v1/supervisor//terminate endpoint. This places a tombstone marker in the database (to prevent the supervisor from being reloaded on a restart) and then gracefully shuts down the currently running supervisor. When a supervisor is shut down in this way, it will instruct its managed tasks to stop reading and begin publishing their segments immediately. The call to the shutdown endpoint will return after all tasks have been signaled to stop but before the tasks finish publishing their segments. Schema/Configuration Changes Schema and configuration changes are handled by submitting the new supervisor spec via the same POST /druid/indexer/v1/supervisor endpoint used to initially create the supervisor. The Overlord will initiate a graceful shutdown of the existing supervisor which will cause the tasks being managed by that supervisor to stop reading and begin publishing their segments. A new supervisor will then be started which will create a new set of tasks that will start reading from the offsets where the previous now-publishing tasks left off, but using the updated schema. In this way, configuration changes can be applied without requiring any pause in ingestion. Deployment Notes On the Subject of Segments Each Kafka Indexing Task puts events consumed from Kafka partitions assigned to it in a single segment for each segment granular interval until maxRowsPerSegment, maxTotalRows or intermediateHandoffPeriod limit is reached, at this point a new partition for this segment granularity is created for further events. Kafka Indexing Task also does incremental hand-offs which means that all the segments created by a task will not be held up till the task duration is over. As soon as maxRowsPerSegment, maxTotalRows or intermediateHandoffPeriod limit is hit, all the segments held by the task at that point in time will be handed-off and new set of segments will be created for further events. This means that the task can run for longer durations of time without accumulating old segments locally on Middle Manager processes and it is encouraged to do so. Kafka Indexing Service may still produce some small segments. Lets say the task duration is 4 hours, segment granularity is set to an HOUR and Supervisor was started at 9:10 then after 4 hours at 13:10, new set of tasks will be started and events for the interval 13:00 - 14:00 may be split across previous and new set of tasks. If you see it becoming a problem then one can schedule re-indexing tasks be run to merge segments together into new segments of an ideal size (in the range of ~500-700 MB per segment). Details on how to optimize the segment size can be found on Segment size optimization. There is also ongoing work to support automatic segment compaction of sharded segments as well as compaction not requiring Hadoop (see here). 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"development/extensions-core/kinesis-ingestion.html":{"url":"development/extensions-core/kinesis-ingestion.html","title":"从Amazon Kinesis摄入","keywords":"","body":" Similar to the Kafka indexing service, the Kinesis indexing service enables the configuration of supervisors on the Overlord, which facilitate ingestion from Kinesis by managing the creation and lifetime of Kinesis indexing tasks. These indexing tasks read events using Kinesis's own Shards and Sequence Number mechanism and are therefore able to provide guarantees of exactly-once ingestion. The supervisor oversees the state of the indexing tasks to coordinate handoffs, manage failures, and ensure that the scalability and replication requirements are maintained. The Kinesis indexing service is provided as the druid-kinesis-indexing-service core Apache Druid extension (see Including Extensions). Please note that this is currently designated as an experimental feature and is subject to the usual experimental caveats. Submitting a Supervisor Spec The Kinesis indexing service requires that the druid-kinesis-indexing-service extension be loaded on both the Overlord and the MiddleManagers. A supervisor for a dataSource is started by submitting a supervisor spec via HTTP POST to http://:/druid/indexer/v1/supervisor, for example: curl -X POST -H 'Content-Type: application/json' -d @supervisor-spec.json http://localhost:8090/druid/indexer/v1/supervisor A sample supervisor spec is shown below: { \"type\": \"kinesis\", \"dataSchema\": { \"dataSource\": \"metrics-kinesis\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [], \"dimensionExclusions\": [ \"timestamp\", \"value\" ] }, \"metricsSpec\": [ { \"name\": \"count\", \"type\": \"count\" }, { \"name\": \"value_sum\", \"fieldName\": \"value\", \"type\": \"doubleSum\" }, { \"name\": \"value_min\", \"fieldName\": \"value\", \"type\": \"doubleMin\" }, { \"name\": \"value_max\", \"fieldName\": \"value\", \"type\": \"doubleMax\" } ], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"HOUR\", \"queryGranularity\": \"NONE\" } }, \"ioConfig\": { \"stream\": \"metrics\", \"inputFormat\": { \"type\": \"json\" }, \"endpoint\": \"kinesis.us-east-1.amazonaws.com\", \"taskCount\": 1, \"replicas\": 1, \"taskDuration\": \"PT1H\", \"recordsPerFetch\": 2000, \"fetchDelayMillis\": 1000 }, \"tuningConfig\": { \"type\": \"kinesis\", \"maxRowsPerSegment\": 5000000 } } Supervisor Spec Field Description Required type The supervisor type, this should always be kinesis. yes dataSchema The schema that will be used by the Kinesis indexing task during ingestion. See dataSchema. yes ioConfig A KinesisSupervisorIOConfig object for configuring Kafka connection and I/O-related settings for the supervisor and indexing task. See KinesisSupervisorIOConfig below. yes tuningConfig A KinesisSupervisorTuningConfig object for configuring performance-related settings for the supervisor and indexing tasks. See KinesisSupervisorTuningConfig below. no KinesisSupervisorIOConfig Field Type Description Required stream String The Kinesis stream to read. yes inputFormat Object inputFormat to specify how to parse input data. See the below section for details about specifying the input format. yes endpoint String The AWS Kinesis stream endpoint for a region. You can find a list of endpoints here. no (default == kinesis.us-east-1.amazonaws.com) replicas Integer The number of replica sets, where 1 means a single set of tasks (no replication). Replica tasks will always be assigned to different workers to provide resiliency against process failure. no (default == 1) taskCount Integer The maximum number of reading tasks in a replica set. This means that the maximum number of reading tasks will be taskCount * replicas and the total number of tasks (reading + publishing) will be higher than this. See Capacity Planning below for more details. The number of reading tasks will be less than taskCount if taskCount > {numKinesisShards}. no (default == 1) taskDuration ISO8601 Period The length of time before tasks stop reading and begin publishing their segment. no (default == PT1H) startDelay ISO8601 Period The period to wait before the supervisor starts managing tasks. no (default == PT5S) period ISO8601 Period How often the supervisor will execute its management logic. Note that the supervisor will also run in response to certain events (such as tasks succeeding, failing, and reaching their taskDuration) so this value specifies the maximum time between iterations. no (default == PT30S) useEarliestSequenceNumber Boolean If a supervisor is managing a dataSource for the first time, it will obtain a set of starting sequence numbers from Kinesis. This flag determines whether it retrieves the earliest or latest sequence numbers in Kinesis. Under normal circumstances, subsequent tasks will start from where the previous segments ended so this flag will only be used on first run. no (default == false) completionTimeout ISO8601 Period The length of time to wait before declaring a publishing task as failed and terminating it. If this is set too low, your tasks may never publish. The publishing clock for a task begins roughly after taskDuration elapses. no (default == PT6H) lateMessageRejectionPeriod ISO8601 Period Configure tasks to reject messages with timestamps earlier than this period before the task was created; for example if this is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps earlier than 2016-01-01T11:00Z will be dropped. This may help prevent concurrency issues if your data stream has late messages and you have multiple pipelines that need to operate on the same segments (e.g. a realtime and a nightly batch ingestion pipeline). no (default == none) earlyMessageRejectionPeriod ISO8601 Period Configure tasks to reject messages with timestamps later than this period after the task reached its taskDuration; for example if this is set to PT1H, the taskDuration is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps later than 2016-01-01T14:00Z will be dropped. Note: Tasks sometimes run past their task duration, for example, in cases of supervisor failover. Setting earlyMessageRejectionPeriod too low may cause messages to be dropped unexpectedly whenever a task runs past its originally configured task duration. no (default == none) recordsPerFetch Integer The number of records to request per GetRecords call to Kinesis. See 'Determining Fetch Settings' below. no (default == 2000) fetchDelayMillis Integer Time in milliseconds to wait between subsequent GetRecords calls to Kinesis. See 'Determining Fetch Settings' below. no (default == 1000) awsAssumedRoleArn String The AWS assumed role to use for additional permissions. no awsExternalId String The AWS external id to use for additional permissions. no deaggregate Boolean Whether to use the de-aggregate function of the KCL. See below for details. no Specifying data format Kinesis indexing service supports both inputFormat and parser to specify the data format. The inputFormat is a new and recommended way to specify the data format for Kinesis indexing service, but unfortunately, it doesn't support all data formats supported by the legacy parser. (They will be supported in the future.) The supported inputFormats include csv, delimited, and json. You can also read avro_stream, protobuf, and thrift formats using parser. KinesisSupervisorTuningConfig The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. Field Type Description Required type String The indexing task type, this should always be kinesis. yes maxRowsInMemory Integer The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). no (default == 100000) maxBytesInMemory Long The number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. Normally this is computed internally and user does not need to set it. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists). no (default == One-sixth of max JVM memory) maxRowsPerSegment Integer The number of rows to aggregate into a segment; this number is post-aggregation rows. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == 5000000) maxTotalRows Long The number of rows to aggregate across all segments; this number is post-aggregation rows. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == unlimited) intermediatePersistPeriod ISO8601 Period The period that determines the rate at which intermediate persists occur. no (default == PT10M) maxPendingPersists Integer Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). no (default == 0, meaning one persist can be running concurrently with ingestion, and none can be queued up) indexSpec Object Tune how data is indexed. See IndexSpec for more information. no indexSpecForIntermediatePersists Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. This can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. However, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see IndexSpec for possible values. no (default = same as indexSpec) reportParseExceptions Boolean If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped. no (default == false) handoffConditionTimeout Long Milliseconds to wait for segment handoff. It must be >= 0, where 0 means to wait forever. no (default == 0) resetOffsetAutomatically Boolean Controls behavior when Druid needs to read Kinesis messages that are no longer available.If false, the exception will bubble up, which will cause your tasks to fail and ingestion to halt. If this occurs, manual intervention is required to correct the situation; potentially using the Reset Supervisor API. This mode is useful for production, since it will make you aware of issues with ingestion.If true, Druid will automatically reset to the earlier or latest sequence number available in Kinesis, based on the value of the useEarliestSequenceNumber property (earliest if true, latest if false). Please note that this can lead to data being DROPPED (if useEarliestSequenceNumber is false) or DUPLICATED (if useEarliestSequenceNumber is true) without your knowledge. Messages will be logged indicating that a reset has occurred, but ingestion will continue. This mode is useful for non-production situations, since it will make Druid attempt to recover from problems automatically, even if they lead to quiet dropping or duplicating of data. no (default == false) skipSequenceNumberAvailabilityCheck Boolean Whether to enable checking if the current sequence number is still available in a particular Kinesis shard. If set to false, the indexing task will attempt to reset the current sequence number (or not), depending on the value of resetOffsetAutomatically. no (default == false) workerThreads Integer The number of threads that the supervisor uses to handle requests/responses for worker tasks, along with any other internal asynchronous operation. no (default == min(10, taskCount)) chatThreads Integer The number of threads that will be used for communicating with indexing tasks. no (default == min(10, taskCount * replicas)) chatRetries Integer The number of times HTTP requests to indexing tasks will be retried before considering tasks unresponsive. no (default == 8) httpTimeout ISO8601 Period How long to wait for a HTTP response from an indexing task. no (default == PT10S) shutdownTimeout ISO8601 Period How long to wait for the supervisor to attempt a graceful shutdown of tasks before exiting. no (default == PT80S) recordBufferSize Integer Size of the buffer (number of events) used between the Kinesis fetch threads and the main ingestion thread. no (default == 10000) recordBufferOfferTimeout Integer Length of time in milliseconds to wait for space to become available in the buffer before timing out. no (default == 5000) recordBufferFullWait Integer Length of time in milliseconds to wait for the buffer to drain before attempting to fetch records from Kinesis again. no (default == 5000) fetchSequenceNumberTimeout Integer Length of time in milliseconds to wait for Kinesis to return the earliest or latest sequence number for a shard. Kinesis will not return the latest sequence number if no data is actively being written to that shard. In this case, this fetch call will repeatedly timeout and retry until fresh data is written to the stream. no (default == 60000) fetchThreads Integer Size of the pool of threads fetching data from Kinesis. There is no benefit in having more threads than Kinesis shards. no (default == procs * 2, where \"procs\" is the number of processors on the server that the task is running on) segmentWriteOutMediumFactory Object Segment write-out medium to use when creating segments. See below for more information. no (not specified by default, the value from druid.peon.defaultSegmentWriteOutMediumFactory.type is used) intermediateHandoffPeriod ISO8601 Period How often the tasks should hand off segments. Handoff will happen either if maxRowsPerSegment or maxTotalRows is hit or every intermediateHandoffPeriod, whichever happens earlier. no (default == P2147483647D) logParseExceptions Boolean If true, log an error message when a parsing exception occurs, containing information about the row where the error occurred. no, default == false maxParseExceptions Integer The maximum number of parse exceptions that can occur before the task halts ingestion and fails. Overridden if reportParseExceptions is set. no, unlimited default maxSavedParseExceptions Integer When a parse exception occurs, Druid can keep track of the most recent parse exceptions. \"maxSavedParseExceptions\" limits how many exception instances will be saved. These saved exceptions will be made available after the task finishes in the task completion report. Overridden if reportParseExceptions is set. no, default == 0 maxRecordsPerPoll Integer The maximum number of records/events to be fetched from buffer per poll. The actual maximum will be Max(maxRecordsPerPoll, Max(bufferSize, 1)) no, default == 100 repartitionTransitionDuration ISO8601 Period When shards are split or merged, the supervisor will recompute shard -> task group mappings, and signal any running tasks created under the old mappings to stop early at (current time + repartitionTransitionDuration). Stopping the tasks early allows Druid to begin reading from the new shards more quickly. The repartition transition wait time controlled by this property gives the stream additional time to write records to the new shards after the split/merge, which helps avoid the issues with empty shard handling described at https://github.com/apache/druid/issues/7600. no, (default == PT2M) offsetFetchPeriod ISO8601 Period How often the supervisor queries Kinesis and the indexing tasks to fetch current offsets and calculate lag. no (default == PT30S, min == PT5S) IndexSpec Field Type Description Required bitmap Object Compression format for bitmap indexes. Should be a JSON object. See Bitmap types below for options. no (defaults to Roaring) dimensionCompression String Compression format for dimension columns. Choose from LZ4, LZF, or uncompressed. no (default == LZ4) metricCompression String Compression format for primitive type metric columns. Choose from LZ4, LZF, uncompressed, or none. no (default == LZ4) longEncoding String Encoding format for metric and dimension columns with type long. Choose from auto or longs. auto encodes the values using sequence number or lookup table depending on column cardinality, and store them with variable size. longs stores the value as is with 8 bytes each. no (default == longs) Bitmap types For Roaring bitmaps: Field Type Description Required type String Must be roaring. yes compressRunOnSerialization Boolean Use a run-length encoding where it is estimated as more space efficient. no (default == true) For Concise bitmaps: Field Type Description Required type String Must be concise. yes SegmentWriteOutMediumFactory Field Type Description Required type String See Additional Peon Configuration: SegmentWriteOutMediumFactory for explanation and available options. yes Operations This section gives descriptions of how some supervisor APIs work specifically in Kinesis Indexing Service. For all supervisor APIs, please check Supervisor APIs. AWS Authentication To authenticate with AWS, you must provide your AWS access key and AWS secret key via runtime.properties, for example: -Ddruid.kinesis.accessKey=123 -Ddruid.kinesis.secretKey=456 The AWS access key ID and secret access key are used for Kinesis API requests. If this is not provided, the service will look for credentials set in environment variables, in the default profile configuration file, and from the EC2 instance profile provider (in this order). Getting Supervisor Status Report GET /druid/indexer/v1/supervisor//status returns a snapshot report of the current state of the tasks managed by the given supervisor. This includes the latest sequence numbers as reported by Kinesis. Unlike the Kafka Indexing Service, Kinesis reports lag metrics measured in time difference in milliseconds between the current sequence number and latest sequence number, rather than message count. The status report also contains the supervisor's state and a list of recently thrown exceptions (reported as recentErrors, whose max size can be controlled using the druid.supervisor.maxStoredExceptionEvents configuration). There are two fields related to the supervisor's state - state and detailedState. The state field will always be one of a small number of generic states that are applicable to any type of supervisor, while the detailedState field will contain a more descriptive, implementation-specific state that may provide more insight into the supervisor's activities than the generic state field. The list of possible state values are: [PENDING, RUNNING, SUSPENDED, STOPPING, UNHEALTHY_SUPERVISOR, UNHEALTHY_TASKS] The list of detailedState values and their corresponding state mapping is as follows: Detailed State Corresponding State Description UNHEALTHY_SUPERVISOR UNHEALTHY_SUPERVISOR The supervisor has encountered errors on the past druid.supervisor.unhealthinessThreshold iterations UNHEALTHY_TASKS UNHEALTHY_TASKS The last druid.supervisor.taskUnhealthinessThreshold tasks have all failed UNABLE_TO_CONNECT_TO_STREAM UNHEALTHY_SUPERVISOR The supervisor is encountering connectivity issues with Kinesis and has not successfully connected in the past LOST_CONTACT_WITH_STREAM UNHEALTHY_SUPERVISOR The supervisor is encountering connectivity issues with Kinesis but has successfully connected in the past PENDING (first iteration only) PENDING The supervisor has been initialized and hasn't started connecting to the stream CONNECTING_TO_STREAM (first iteration only) RUNNING The supervisor is trying to connect to the stream and update partition data DISCOVERING_INITIAL_TASKS (first iteration only) RUNNING The supervisor is discovering already-running tasks CREATING_TASKS (first iteration only) RUNNING The supervisor is creating tasks and discovering state RUNNING RUNNING The supervisor has started tasks and is waiting for taskDuration to elapse SUSPENDED SUSPENDED The supervisor has been suspended STOPPING STOPPING The supervisor is stopping On each iteration of the supervisor's run loop, the supervisor completes the following tasks in sequence: 1) Fetch the list of shards from Kinesis and determine the starting sequence number for each shard (either based on the last processed sequence number if continuing, or starting from the beginning or ending of the stream if this is a new stream). 2) Discover any running indexing tasks that are writing to the supervisor's datasource and adopt them if they match the supervisor's configuration, else signal them to stop. 3) Send a status request to each supervised task to update our view of the state of the tasks under our supervision. 4) Handle tasks that have exceeded taskDuration and should transition from the reading to publishing state. 5) Handle tasks that have finished publishing and signal redundant replica tasks to stop. 6) Handle tasks that have failed and clean up the supervisor's internal state. 7) Compare the list of healthy tasks to the requested taskCount and replicas configurations and create additional tasks if required. The detailedState field will show additional values (those marked with \"first iteration only\") the first time the supervisor executes this run loop after startup or after resuming from a suspension. This is intended to surface initialization-type issues, where the supervisor is unable to reach a stable state (perhaps because it can't connect to Kinesis, it can't read from the stream, or it can't communicate with existing tasks). Once the supervisor is stable - that is, once it has completed a full execution without encountering any issues - detailedState will show a RUNNING state until it is stopped, suspended, or hits a failure threshold and transitions to an unhealthy state. Updating Existing Supervisors POST /druid/indexer/v1/supervisor can be used to update existing supervisor spec. Calling this endpoint when there is already an existing supervisor for the same dataSource will cause: The running supervisor to signal its managed tasks to stop reading and begin publishing. The running supervisor to exit. A new supervisor to be created using the configuration provided in the request body. This supervisor will retain the existing publishing tasks and will create new tasks starting at the sequence numbers the publishing tasks ended on. Seamless schema migrations can thus be achieved by simply submitting the new schema using this endpoint. Suspending and Resuming Supervisors You can suspend and resume a supervisor using POST /druid/indexer/v1/supervisor//suspend and POST /druid/indexer/v1/supervisor//resume, respectively. Note that the supervisor itself will still be operating and emitting logs and metrics, it will just ensure that no indexing tasks are running until the supervisor is resumed. Resetting Supervisors The POST /druid/indexer/v1/supervisor//reset operation clears stored sequence numbers, causing the supervisor to start reading from either the earliest or latest sequence numbers in Kinesis (depending on the value of useEarliestSequenceNumber). After clearing stored sequence numbers, the supervisor kills and recreates active tasks, so that tasks begin reading from valid sequence numbers. Use care when using this operation! Resetting the supervisor may cause Kinesis messages to be skipped or read twice, resulting in missing or duplicate data. The reason for using this operation is to recover from a state in which the supervisor ceases operating due to missing sequence numbers. The indexing service keeps track of the latest persisted sequence number in order to provide exactly-once ingestion guarantees across tasks. Subsequent tasks must start reading from where the previous task completed in order for the generated segments to be accepted. If the messages at the expected starting sequence numbers are no longer available in Kinesis (typically because the message retention period has elapsed or the topic was removed and re-created) the supervisor will refuse to start and in-flight tasks will fail. This operation enables you to recover from this condition. Note that the supervisor must be running for this endpoint to be available. Terminating Supervisors The POST /druid/indexer/v1/supervisor//terminate operation terminates a supervisor and causes all associated indexing tasks managed by this supervisor to immediately stop and begin publishing their segments. This supervisor will still exist in the metadata store and its history may be retrieved with the supervisor history API, but will not be listed in the 'get supervisors' API response nor can its configuration or status report be retrieved. The only way this supervisor can start again is by submitting a functioning supervisor spec to the create API. Capacity Planning Kinesis indexing tasks run on MiddleManagers and are thus limited by the resources available in the MiddleManager cluster. In particular, you should make sure that you have sufficient worker capacity (configured using the druid.worker.capacity property) to handle the configuration in the supervisor spec. Note that worker capacity is shared across all types of indexing tasks, so you should plan your worker capacity to handle your total indexing load (e.g. batch processing, realtime tasks, merging tasks, etc.). If your workers run out of capacity, Kinesis indexing tasks will queue and wait for the next available worker. This may cause queries to return partial results but will not result in data loss (assuming the tasks run before Kinesis purges those sequence numbers). A running task will normally be in one of two states: reading or publishing. A task will remain in reading state for taskDuration, at which point it will transition to publishing state. A task will remain in publishing state for as long as it takes to generate segments, push segments to deep storage, and have them be loaded and served by a Historical process (or until completionTimeout elapses). The number of reading tasks is controlled by replicas and taskCount. In general, there will be replicas * taskCount reading tasks, the exception being if taskCount > {numKinesisShards} in which case {numKinesisShards} tasks will be used instead. When taskDuration elapses, these tasks will transition to publishing state and replicas * taskCount new reading tasks will be created. Therefore to allow for reading tasks and publishing tasks to run concurrently, there should be a minimum capacity of: workerCapacity = 2 * replicas * taskCount This value is for the ideal situation in which there is at most one set of tasks publishing while another set is reading. In some circumstances, it is possible to have multiple sets of tasks publishing simultaneously. This would happen if the time-to-publish (generate segment, push to deep storage, loaded on Historical) > taskDuration. This is a valid scenario (correctness-wise) but requires additional worker capacity to support. In general, it is a good idea to have taskDuration be large enough that the previous set of tasks finishes publishing before the current set begins. Supervisor Persistence When a supervisor spec is submitted via the POST /druid/indexer/v1/supervisor endpoint, it is persisted in the configured metadata database. There can only be a single supervisor per dataSource, and submitting a second spec for the same dataSource will overwrite the previous one. When an Overlord gains leadership, either by being started or as a result of another Overlord failing, it will spawn a supervisor for each supervisor spec in the metadata database. The supervisor will then discover running Kinesis indexing tasks and will attempt to adopt them if they are compatible with the supervisor's configuration. If they are not compatible because they have a different ingestion spec or shard allocation, the tasks will be killed and the supervisor will create a new set of tasks. In this way, the supervisors are persistent across Overlord restarts and fail-overs. A supervisor is stopped via the POST /druid/indexer/v1/supervisor//terminate endpoint. This places a tombstone marker in the database (to prevent the supervisor from being reloaded on a restart) and then gracefully shuts down the currently running supervisor. When a supervisor is shut down in this way, it will instruct its managed tasks to stop reading and begin publishing their segments immediately. The call to the shutdown endpoint will return after all tasks have been signalled to stop but before the tasks finish publishing their segments. Schema/Configuration Changes Schema and configuration changes are handled by submitting the new supervisor spec via the same POST /druid/indexer/v1/supervisor endpoint used to initially create the supervisor. The Overlord will initiate a graceful shutdown of the existing supervisor which will cause the tasks being managed by that supervisor to stop reading and begin publishing their segments. A new supervisor will then be started which will create a new set of tasks that will start reading from the sequence numbers where the previous now-publishing tasks left off, but using the updated schema. In this way, configuration changes can be applied without requiring any pause in ingestion. Deployment Notes On the Subject of Segments Each Kinesis Indexing Task puts events consumed from Kinesis Shards assigned to it in a single segment for each segment granular interval until maxRowsPerSegment, maxTotalRows or intermediateHandoffPeriod limit is reached, at this point a new shard for this segment granularity is created for further events. Kinesis Indexing Task also does incremental hand-offs which means that all the segments created by a task will not be held up till the task duration is over. As soon as maxRowsPerSegment, maxTotalRows or intermediateHandoffPeriod limit is hit, all the segments held by the task at that point in time will be handed-off and new set of segments will be created for further events. This means that the task can run for longer durations of time without accumulating old segments locally on Middle Manager processes and it is encouraged to do so. Kinesis Indexing Service may still produce some small segments. Lets say the task duration is 4 hours, segment granularity is set to an HOUR and Supervisor was started at 9:10 then after 4 hours at 13:10, new set of tasks will be started and events for the interval 13:00 - 14:00 may be split across previous and new set of tasks. If you see it becoming a problem then one can schedule re-indexing tasks be run to merge segments together into new segments of an ideal size (in the range of ~500-700 MB per segment). Details on how to optimize the segment size can be found on Segment size optimization. There is also ongoing work to support automatic segment compaction of sharded segments as well as compaction not requiring Hadoop (see here). Determining Fetch Settings Internally, the Kinesis Indexing Service uses the Kinesis Record Supplier abstraction for fetching Kinesis data records and storing the records locally. The way the Kinesis Record Supplier fetches records is to have a separate thread run the fetching operation per each Kinesis Shard, the max number of threads is determined by fetchThreads. For example, a Kinesis stream with 3 shards will have 3 threads, each fetching from a shard separately. There is a delay between each fetching operation, which is controlled by fetchDelayMillis. The maximum number of records to be fetched per thread per operation is controlled by recordsPerFetch. Note that this is not the same as maxRecordsPerPoll. The records fetched by each thread will be pushed to a queue in the order that they are fetched. The records are stored in this queue until poll() is called by either the supervisor or the indexing task. poll() will attempt to drain the internal buffer queue up to a limit of max(maxRecordsPerPoll, q.size()). Here maxRecordsPerPoll controls the theoretical maximum records to drain out of the buffer queue, so setting this parameter to a reasonable value is essential in preventing the queue from overflowing or memory exceeding heap size. Kinesis places the following restrictions on calls to fetch records: Each data record can be up to 1 MB in size. Each shard can support up to 5 transactions per second for reads. Each shard can read up to 2 MB per second. The maximum size of data that GetRecords can return is 10 MB. Values for recordsPerFetch and fetchDelayMillis should be chosen to maximize throughput under the above constraints. The values that you choose will depend on the average size of a record and the number of consumers you have reading from a given shard (which will be replicas unless you have other consumers also reading from this Kinesis stream). If the above limits are violated, AWS will throw ProvisionedThroughputExceededException errors on subsequent calls to read data. When this happens, the Kinesis indexing service will pause by fetchDelayMillis and then attempt the call again. Internally, each indexing task maintains a buffer that stores the fetched but not yet processed record. recordsPerFetch and fetchDelayMillis control this behavior. The number of records that the indexing task fetch from the buffer is controlled by maxRecordsPerPoll, which determines the number of records to be processed per each ingestion loop in the task. Deaggregation See issue The Kinesis indexing service supports de-aggregation of multiple rows packed into a single record by the Kinesis Producer Library's aggregate method for more efficient data transfer. Currently, enabling the de-aggregate functionality requires the user to manually provide the Kinesis Client Library on the classpath, since this library has a license not compatible with Apache projects. To enable this feature, add the amazon-kinesis-client (tested on version 1.9.2) jar file (link) under dist/druid/extensions/druid-kinesis-indexing-service/. Then when submitting a supervisor-spec, set deaggregate to true. Resharding When changing the shard count for a Kinesis stream, there will be a window of time around the resharding operation with early shutdown of Kinesis ingestion tasks and possible task failures. The early shutdowns and task failures are expected, and they occur because the supervisor will update the shard -> task group mappings as shards are closed and fully read, to ensure that tasks are not running with an assignment of closed shards that have been fully read and to ensure a balanced distribution of active shards across tasks. This window with early task shutdowns and possible task failures will conclude when: All closed shards have been fully read and the Kinesis ingestion tasks have published the data from those shards, committing the \"closed\" state to metadata storage Any remaining tasks that had inactive shards in the assignment have been shutdown (these tasks would have been created before the closed shards were completely drained) 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/tranquility.html":{"url":"ingestion/tranquility.html","title":"从Tranqulity摄入","keywords":"","body":" Tranquility is a package for pushing streams to Druid in real-time. Druid does not come bundled with Tranquility; it is available as a separate download. Note that as of this writing, the latest available version of Tranquility is built against the rather old Druid 0.9.2 release. It will still work with the latest Druid servers, but not all features and functionality will be available due to limitations of older Druid APIs on the Tranquility side. For new projects that require streaming ingestion, we recommend using Druid's native support for Apache Kafka or Amazon Kinesis. For more details, check out the Tranquility GitHub page. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/standalone-realtime.html":{"url":"ingestion/standalone-realtime.html","title":"独立实时模式","keywords":"","body":" Older versions of Apache Druid supported a standalone 'Realtime' process to query and index 'stream pull' modes of real-time ingestion. These processes would periodically build segments for the data they had collected over some span of time and then set up hand-off to Historical servers. This processes could be invoked by org.apache.druid.cli.Main server realtime This model of stream pull ingestion was deprecated for a number of both operational and architectural reasons, and removed completely in Druid 0.16.0. Operationally, realtime nodes were difficult to configure, deploy, and scale because each node required an unique configuration. The design of the stream pull ingestion system for realtime nodes also suffered from limitations which made it not possible to achieve exactly once ingestion. The extensions druid-kafka-eight, druid-kafka-eight-simpleConsumer, druid-rabbitmq, and druid-rocketmq were also removed at this time, since they were built to operate on the realtime nodes. Please consider using the Kafka Indexing Service or Kinesis Indexing Service for stream pull ingestion instead. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/native-batch.html":{"url":"ingestion/native-batch.html","title":"本地模式","keywords":"","body":" Apache Druid currently has two types of native batch indexing tasks, index_parallel which can run multiple tasks in parallel, and index which will run a single indexing task. Please refer to our Hadoop-based vs. native batch comparison table for comparisons between Hadoop-based, native batch (simple), and native batch (parallel) ingestion. To run either kind of native batch indexing task, write an ingestion spec as specified below. Then POST it to the /druid/indexer/v1/task endpoint on the Overlord, or use the bin/post-index-task script included with Druid. Tutorial This page contains reference documentation for native batch ingestion. For a walk-through instead, check out the Loading a file tutorial, which demonstrates the \"simple\" (single-task) mode. Parallel task The Parallel task (type index_parallel) is a task for parallel batch indexing. This task only uses Druid's resource and doesn't depend on other external systems like Hadoop. The index_parallel task is a supervisor task that orchestrates the whole indexing process. The supervisor task splits the input data and creates worker tasks to process those splits. The created worker tasks are issued to the Overlord so that they can be scheduled and run on MiddleManagers or Indexers. Once a worker task successfully processes the assigned input split, it reports the generated segment list to the supervisor task. The supervisor task periodically checks the status of worker tasks. If one of them fails, it retries the failed task until the number of retries reaches the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalizes ingestion. The detailed behavior of the Parallel task is different depending on the partitionsSpec. See each partitionsSpec for more details. To use this task, the inputSource in the ioConfig should be splittable and maxNumConcurrentSubTasks should be set to larger than 1 in the tuningConfig. Otherwise, this task runs sequentially; the index_parallel task reads each input file one by one and creates segments by itself. The supported splittable input formats for now are: s3 reads data from AWS S3 storage. gs reads data from Google Cloud Storage. azure reads data from Azure Blob Storage. hdfs reads data from HDFS storage. http reads data from HTTP servers. local reads data from local storage. druid reads data from a Druid datasource. sql reads data from a RDBMS source. Some other cloud storage types are supported with the legacy firehose. The below firehose types are also splittable. Note that only text formats are supported with the firehose. Compression formats supported The supported compression formats for native batch ingestion are bz2, gz, xz, zip, sz (Snappy), and zst (ZSTD). static-cloudfiles You may want to consider the below things: You may want to control the amount of input data each worker task processes. This can be controlled using different configurations depending on the phase in parallel ingestion (see partitionsSpec for more details). For the tasks that read data from the inputSource, you can set the Split hint spec in the tuningConfig. For the tasks that merge shuffled segments, you can set the totalNumMergeTasks in the tuningConfig. The number of concurrent worker tasks in parallel ingestion is determined by maxNumConcurrentSubTasks in the tuningConfig. The supervisor task checks the number of current running worker tasks and creates more if it's smaller than maxNumConcurrentSubTasks no matter how many task slots are currently available. This may affect to other ingestion performance. See the below Capacity Planning section for more details. By default, batch ingestion replaces all data (in your granularitySpec's intervals) in any segment that it writes to. If you'd like to add to the segment instead, set the appendToExisting flag in the ioConfig. Note that it only replaces data in segments where it actively adds data: if there are segments in your granularitySpec's intervals that have no data written by this task, they will be left alone. If any existing segments partially overlap with the granularitySpec's intervals, the portion of those segments outside the new segments' intervals will still be visible. Task syntax A sample task is shown below: { \"type\": \"index_parallel\", \"spec\": { \"dataSchema\": { \"dataSource\": \"wikipedia_parallel_index_test\", \"timestampSpec\": { \"column\": \"timestamp\" }, \"dimensionsSpec\": { \"dimensions\": [ \"page\", \"language\", \"user\", \"unpatrolled\", \"newPage\", \"robot\", \"anonymous\", \"namespace\", \"continent\", \"country\", \"region\", \"city\" ] }, \"metricsSpec\": [ { \"type\": \"count\", \"name\": \"count\" }, { \"type\": \"doubleSum\", \"name\": \"added\", \"fieldName\": \"added\" }, { \"type\": \"doubleSum\", \"name\": \"deleted\", \"fieldName\": \"deleted\" }, { \"type\": \"doubleSum\", \"name\": \"delta\", \"fieldName\": \"delta\" } ], \"granularitySpec\": { \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"second\", \"intervals\" : [ \"2013-08-31/2013-09-02\" ] } }, \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"local\", \"baseDir\": \"examples/indexing/\", \"filter\": \"wikipedia_index_data*\" }, \"inputFormat\": { \"type\": \"json\" } }, \"tuningConfig\": { \"type\": \"index_parallel\", \"maxNumConcurrentSubTasks\": 2 } } } property description required? type The task type, this should always be index_parallel. yes id The task ID. If this is not explicitly specified, Druid generates the task ID using task type, data source name, interval, and date-time stamp. no spec The ingestion spec including the data schema, IOConfig, and TuningConfig. See below for more details. yes context Context containing various task configuration parameters. See below for more details. no dataSchema This field is required. See Ingestion Spec DataSchema If you specify intervals explicitly in your dataSchema's granularitySpec, batch ingestion will lock the full intervals specified when it starts up, and you will learn quickly if the specified interval overlaps with locks held by other tasks (e.g., Kafka ingestion). Otherwise, batch ingestion will lock each interval as it is discovered, so you may only learn that the task overlaps with a higher-priority task later in ingestion. If you specify intervals explicitly, any rows outside the specified intervals will be thrown away. We recommend setting intervals explicitly if you know the time range of the data so that locking failure happens faster, and so that you don't accidentally replace data outside that range if there's some stray data with unexpected timestamps. ioConfig property description default required? type The task type, this should always be index_parallel. none yes inputFormat inputFormat to specify how to parse input data. none yes appendToExisting Creates segments as additional shards of the latest version, effectively appending to the segment set instead of replacing it. The current limitation is that you can append to any datasources regardless of their original partitioning scheme, but the appended segments should be partitioned using the dynamic partitionsSpec. false no tuningConfig The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. See below for more details. property description default required? type The task type, this should always be index_parallel. none yes maxRowsPerSegment Deprecated. Use partitionsSpec instead. Used in sharding. Determines how many rows are in each segment. 5000000 no maxRowsInMemory Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set. 1000000 no maxBytesInMemory Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists) 1/6 of max JVM memory no maxColumnsToMerge A parameter that limits how many segments can be merged in a single phase when merging segments for publishing. This limit is imposed on the total number of columns present in a set of segments being merged. If the limit is exceeded, segment merging will occur in multiple phases. At least 2 segments will be merged in a single phase, regardless of this setting. -1 (unlimited) no maxTotalRows Deprecated. Use partitionsSpec instead. Total number of rows in segments waiting for being pushed. Used in determining when intermediate pushing should occur. 20000000 no numShards Deprecated. Use partitionsSpec instead. Directly specify the number of shards to create when using a hashed partitionsSpec. If this is specified and intervals is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. numShards cannot be specified if maxRowsPerSegment is set. null no splitHintSpec Used to give a hint to control the amount of data that each first phase task reads. This hint could be ignored depending on the implementation of the input source. See Split hint spec for more details. size-based split hint spec no partitionsSpec Defines how to partition data in each timeChunk, see PartitionsSpec dynamic if forceGuaranteedRollup = false, hashed or single_dim if forceGuaranteedRollup = true no indexSpec Defines segment storage format options to be used at indexing time, see IndexSpec null no indexSpecForIntermediatePersists Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. this can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. however, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see IndexSpec for possible values. same as indexSpec no maxPendingPersists Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). 0 (meaning one persist can be running concurrently with ingestion, and none can be queued up) no forceGuaranteedRollup Forces guaranteeing the perfect rollup. The perfect rollup optimizes the total size of generated segments and querying time while indexing time will be increased. If this is set to true, intervals in granularitySpec must be set and hashed or single_dim must be used for partitionsSpec. This flag cannot be used with appendToExisting of IOConfig. For more details, see the below Segment pushing modes section. false no reportParseExceptions If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped. false no pushTimeout Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever. 0 no segmentWriteOutMediumFactory Segment write-out medium to use when creating segments. See SegmentWriteOutMediumFactory. Not specified, the value from druid.peon.defaultSegmentWriteOutMediumFactory.type is used no maxNumConcurrentSubTasks Maximum number of worker tasks which can be run in parallel at the same time. The supervisor task would spawn worker tasks up to maxNumConcurrentSubTasks regardless of the current available task slots. If this value is set to 1, the supervisor task processes data ingestion on its own instead of spawning worker tasks. If this value is set to too large, too many worker tasks can be created which might block other ingestion. Check Capacity Planning for more details. 1 no maxRetry Maximum number of retries on task failures. 3 no maxNumSegmentsToMerge Max limit for the number of segments that a single task can merge at the same time in the second phase. Used only forceGuaranteedRollup is set. 100 no totalNumMergeTasks Total number of tasks to merge segments in the merge phase when partitionsSpec is set to hashed or single_dim. 10 no taskStatusCheckPeriodMs Polling period in milliseconds to check running task statuses. 1000 no chatHandlerTimeout Timeout for reporting the pushed segments in worker tasks. PT10S no chatHandlerNumRetries Retries for reporting the pushed segments in worker tasks. 5 no Split Hint Spec The split hint spec is used to give a hint when the supervisor task creates input splits. Note that each worker task processes a single input split. You can control the amount of data each worker task will read during the first phase. Size-based Split Hint Spec The size-based split hint spec is respected by all splittable input sources except for the HTTP input source and SQL input source. property description default required? type This should always be maxSize. none yes maxSplitSize Maximum number of bytes of input files to process in a single subtask. If a single file is larger than this number, it will be processed by itself in a single subtask (Files are never split across tasks yet). Note that one subtask will not process more files than maxNumFiles even when their total size is smaller than maxSplitSize. Human-readable format is supported. 1GiB no maxNumFiles Maximum number of input files to process in a single subtask. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (jute.maxbuffer) and the max packet size in MySQL (max_allowed_packet). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them. Note that one subtask will not process more data than maxSplitSize even when the total number of files is smaller than maxNumFiles. 1000 no Segments Split Hint Spec The segments split hint spec is used only for DruidInputSource (and legacy IngestSegmentFirehose). property description default required? type This should always be segments. none yes maxInputSegmentBytesPerTask Maximum number of bytes of input segments to process in a single subtask. If a single segment is larger than this number, it will be processed by itself in a single subtask (input segments are never split across tasks). Note that one subtask will not process more segments than maxNumSegments even when their total size is smaller than maxInputSegmentBytesPerTask. Human-readable format is supported. 1GiB no maxNumSegments Maximum number of input segments to process in a single subtask. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (jute.maxbuffer) and the max packet size in MySQL (max_allowed_packet). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them. Note that one subtask will not process more data than maxInputSegmentBytesPerTask even when the total number of segments is smaller than maxNumSegments. 1000 no partitionsSpec PartitionsSpec is used to describe the secondary partitioning method. You should use different partitionsSpec depending on the rollup mode you want. For perfect rollup, you should use either hashed (partitioning based on the hash of dimensions in each row) or single_dim (based on ranges of a single dimension). For best-effort rollup, you should use dynamic. The three partitionsSpec types have different characteristics. PartitionsSpec Ingestion speed Partitioning method Supported rollup mode Secondary partition pruning at query time dynamic Fastest Partitioning based on number of rows in segment. Best-effort rollup N/A hashed Moderate Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See Partitioning for more details. Perfect rollup The broker can use the partition information to prune segments early to speed up queries. Since the broker knows how to hash partitionDimensions values to locate a segment, given a query including a filter on all the partitionDimensions, the broker can pick up only the segments holding the rows satisfying the filter on partitionDimensions for query processing.Note that partitionDimensions must be set at ingestion time to enable secondary partition pruning at query time. single_dim Slowest Range partitioning based on the value of the partition dimension. Segment sizes may be skewed depending on the partition key distribution. This may reduce your datasource size and query latency by improving data locality. See Partitioning for more details. Perfect rollup The broker can use the partition information to prune segments early to speed up queries. Since the broker knows the range of partitionDimension values in each segment, given a query including a filter on the partitionDimension, the broker can pick up only the segments holding the rows satisfying the filter on partitionDimension for query processing. The recommended use case for each partitionsSpec is: If your data has a uniformly distributed column which is frequently used in your queries, consider using single_dim partitionsSpec to maximize the performance of most of your queries. If your data doesn't have a uniformly distributed column, but is expected to have a high rollup ratio when you roll up with some dimensions, consider using hashed partitionsSpec. It could reduce the size of datasource and query latency by improving data locality. If the above two scenarios are not the case or you don't need to roll up your datasource, consider using dynamic partitionsSpec. Dynamic partitioning property description default required? type This should always be dynamic none yes maxRowsPerSegment Used in sharding. Determines how many rows are in each segment. 5000000 no maxTotalRows Total number of rows across all segments waiting for being pushed. Used in determining when intermediate segment push should occur. 20000000 no With the Dynamic partitioning, the parallel index task runs in a single phase: it will spawn multiple worker tasks (type single_phase_sub_task), each of which creates segments. How the worker task creates segments is: The task creates a new segment whenever the number of rows in the current segment exceeds maxRowsPerSegment. Once the total number of rows in all segments across all time chunks reaches to maxTotalRows, the task pushes all segments created so far to the deep storage and creates new ones. Hash-based partitioning property description default required? type This should always be hashed none yes numShards Directly specify the number of shards to create. If this is specified and intervals is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. This property and targetRowsPerSegment cannot both be set. none no targetRowsPerSegment A target row count for each partition. If numShards is left unspecified, the Parallel task will determine a partition count automatically such that each partition has a row count close to the target, assuming evenly distributed keys in the input data. A target per-segment row count of 5 million is used if both numShards and targetRowsPerSegment are null. null (or 5,000,000 if both numShards and targetRowsPerSegment are null) no partitionDimensions The dimensions to partition on. Leave blank to select all dimensions. null no partitionFunction A function to compute hash of partition dimensions. See Hash partition function murmur3_32_abs no The Parallel task with hash-based partitioning is similar to MapReduce. The task runs in up to 3 phases: partial dimension cardinality, partial segment generation and partial segment merge. The partial dimension cardinality phase is an optional phase that only runs if numShards is not specified. The Parallel task splits the input data and assigns them to worker tasks based on the split hint spec. Each worker task (type partial_dimension_cardinality) gathers estimates of partitioning dimensions cardinality for each time chunk. The Parallel task will aggregate these estimates from the worker tasks and determine the highest cardinality across all of the time chunks in the input data, dividing this cardinality by targetRowsPerSegment to automatically determine numShards. In the partial segment generation phase, just like the Map phase in MapReduce, the Parallel task splits the input data based on the split hint spec and assigns each split to a worker task. Each worker task (type partial_index_generate) reads the assigned split, and partitions rows by the time chunk from segmentGranularity (primary partition key) in the granularitySpec and then by the hash value of partitionDimensions (secondary partition key) in the partitionsSpec. The partitioned data is stored in local storage of the middleManager or the indexer. The partial segment merge phase is similar to the Reduce phase in MapReduce. The Parallel task spawns a new set of worker tasks (type partial_index_generic_merge) to merge the partitioned data created in the previous phase. Here, the partitioned data is shuffled based on the time chunk and the hash value of partitionDimensions to be merged; each worker task reads the data falling in the same time chunk and the same hash value from multiple MiddleManager/Indexer processes and merges them to create the final segments. Finally, they push the final segments to the deep storage at once. Hash partition function In hash partitioning, the partition function is used to compute hash of partition dimensions. The partition dimension values are first serialized into a byte array as a whole, and then the partition function is applied to compute hash of the byte array. Druid currently supports only one partition function. name description murmur3_32_abs Applies an absolute value function to the result of murmur3_32). Single-dimension range partitioning Single dimension range partitioning is currently not supported in the sequential mode of the Parallel task. The Parallel task will use one subtask when you set maxNumConcurrentSubTasks to 1. property description default required? type This should always be single_dim none yes partitionDimension The dimension to partition on. Only rows with a single dimension value are allowed. none yes targetRowsPerSegment Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. none either this or maxRowsPerSegment maxRowsPerSegment Soft max for the number of rows to include in a partition. none either this or targetRowsPerSegment assumeGrouped Assume that input data has already been grouped on time and dimensions. Ingestion will run faster, but may choose sub-optimal partitions if this assumption is violated. false no With single-dim partitioning, the Parallel task runs in 3 phases, i.e., partial dimension distribution, partial segment generation, and partial segment merge. The first phase is to collect some statistics to find the best partitioning and the other 2 phases are to create partial segments and to merge them, respectively, as in hash-based partitioning. In the partial dimension distribution phase, the Parallel task splits the input data and assigns them to worker tasks based on the split hint spec. Each worker task (type partial_dimension_distribution) reads the assigned split and builds a histogram for partitionDimension. The Parallel task collects those histograms from worker tasks and finds the best range partitioning based on partitionDimension to evenly distribute rows across partitions. Note that either targetRowsPerSegment or maxRowsPerSegment will be used to find the best partitioning. In the partial segment generation phase, the Parallel task spawns new worker tasks (type partial_range_index_generate) to create partitioned data. Each worker task reads a split created as in the previous phase, partitions rows by the time chunk from the segmentGranularity (primary partition key) in the granularitySpec and then by the range partitioning found in the previous phase. The partitioned data is stored in local storage of the middleManager or the indexer. In the partial segment merge phase, the parallel index task spawns a new set of worker tasks (type partial_index_generic_merge) to merge the partitioned data created in the previous phase. Here, the partitioned data is shuffled based on the time chunk and the value of partitionDimension; each worker task reads the segments falling in the same partition of the same range from multiple MiddleManager/Indexer processes and merges them to create the final segments. Finally, they push the final segments to the deep storage. Because the task with single-dimension range partitioning makes two passes over the input in partial dimension distribution and partial segment generation phases, the task may fail if the input changes in between the two passes. HTTP status endpoints The supervisor task provides some HTTP endpoints to get running status. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/mode Returns 'parallel' if the indexing task is running in parallel. Otherwise, it returns 'sequential'. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/phase Returns the name of the current phase if the task running in the parallel mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/progress Returns the estimated progress of the current phase if the supervisor task is running in the parallel mode. An example of the result is { \"running\":10, \"succeeded\":0, \"failed\":0, \"complete\":0, \"total\":10, \"estimatedExpectedSucceeded\":10 } http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtasks/running Returns the task IDs of running worker tasks, or an empty list if the supervisor task is running in the sequential mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspecs Returns all worker task specs, or an empty list if the supervisor task is running in the sequential mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspecs/running Returns running worker task specs, or an empty list if the supervisor task is running in the sequential mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspecs/complete Returns complete worker task specs, or an empty list if the supervisor task is running in the sequential mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspec/{SUB_TASK_SPEC_ID} Returns the worker task spec of the given id, or HTTP 404 Not Found error if the supervisor task is running in the sequential mode. http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspec/{SUB_TASK_SPEC_ID}/state Returns the state of the worker task spec of the given id, or HTTP 404 Not Found error if the supervisor task is running in the sequential mode. The returned result contains the worker task spec, a current task status if exists, and task attempt history. An example of the result is { \"spec\": { \"id\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z_2\", \"groupId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\", \"supervisorTaskId\": \"index_parallel_lineitem_2018-04-20T22:12:43.610Z\", \"context\": null, \"inputSplit\": { \"split\": \"/path/to/data/lineitem.tbl.5\" }, \"ingestionSpec\": { \"dataSchema\": { \"dataSource\": \"lineitem\", \"timestampSpec\": { \"column\": \"l_shipdate\", \"format\": \"yyyy-MM-dd\" }, \"dimensionsSpec\": { \"dimensions\": [ \"l_orderkey\", \"l_partkey\", \"l_suppkey\", \"l_linenumber\", \"l_returnflag\", \"l_linestatus\", \"l_shipdate\", \"l_commitdate\", \"l_receiptdate\", \"l_shipinstruct\", \"l_shipmode\", \"l_comment\" ] }, \"metricsSpec\": [ { \"type\": \"count\", \"name\": \"count\" }, { \"type\": \"longSum\", \"name\": \"l_quantity\", \"fieldName\": \"l_quantity\", \"expression\": null }, { \"type\": \"doubleSum\", \"name\": \"l_extendedprice\", \"fieldName\": \"l_extendedprice\", \"expression\": null }, { \"type\": \"doubleSum\", \"name\": \"l_discount\", \"fieldName\": \"l_discount\", \"expression\": null }, { \"type\": \"doubleSum\", \"name\": \"l_tax\", \"fieldName\": \"l_tax\", \"expression\": null } ], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"YEAR\", \"queryGranularity\": { \"type\": \"none\" }, \"rollup\": true, \"intervals\": [ \"1980-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z\" ] }, \"transformSpec\": { \"filter\": null, \"transforms\": [] } }, \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"local\", \"baseDir\": \"/path/to/data/\", \"filter\": \"lineitem.tbl.5\" }, \"inputFormat\": { \"format\": \"tsv\", \"delimiter\": \"|\", \"columns\": [ \"l_orderkey\", \"l_partkey\", \"l_suppkey\", \"l_linenumber\", \"l_quantity\", \"l_extendedprice\", \"l_discount\", \"l_tax\", \"l_returnflag\", \"l_linestatus\", \"l_shipdate\", \"l_commitdate\", \"l_receiptdate\", \"l_shipinstruct\", \"l_shipmode\", \"l_comment\" ] }, \"appendToExisting\": false }, \"tuningConfig\": { \"type\": \"index_parallel\", \"maxRowsPerSegment\": 5000000, \"maxRowsInMemory\": 1000000, \"maxTotalRows\": 20000000, \"numShards\": null, \"indexSpec\": { \"bitmap\": { \"type\": \"roaring\" }, \"dimensionCompression\": \"lz4\", \"metricCompression\": \"lz4\", \"longEncoding\": \"longs\" }, \"indexSpecForIntermediatePersists\": { \"bitmap\": { \"type\": \"roaring\" }, \"dimensionCompression\": \"lz4\", \"metricCompression\": \"lz4\", \"longEncoding\": \"longs\" }, \"maxPendingPersists\": 0, \"reportParseExceptions\": false, \"pushTimeout\": 0, \"segmentWriteOutMediumFactory\": null, \"maxNumConcurrentSubTasks\": 4, \"maxRetry\": 3, \"taskStatusCheckPeriodMs\": 1000, \"chatHandlerTimeout\": \"PT10S\", \"chatHandlerNumRetries\": 5, \"logParseExceptions\": false, \"maxParseExceptions\": 2147483647, \"maxSavedParseExceptions\": 0, \"forceGuaranteedRollup\": false, \"buildV9Directly\": true } } }, \"currentStatus\": { \"id\": \"index_sub_lineitem_2018-04-20T22:16:29.922Z\", \"type\": \"index_sub\", \"createdTime\": \"2018-04-20T22:16:29.925Z\", \"queueInsertionTime\": \"2018-04-20T22:16:29.929Z\", \"statusCode\": \"RUNNING\", \"duration\": -1, \"location\": { \"host\": null, \"port\": -1, \"tlsPort\": -1 }, \"dataSource\": \"lineitem\", \"errorMsg\": null }, \"taskHistory\": [] } http://{PEON_IP}:{PEON_PORT}/druid/worker/v1/chat/{SUPERVISOR_TASK_ID}/subtaskspec/{SUB_TASK_SPEC_ID}/history Returns the task attempt history of the worker task spec of the given id, or HTTP 404 Not Found error if the supervisor task is running in the sequential mode. Capacity planning The supervisor task can create up to maxNumConcurrentSubTasks worker tasks no matter how many task slots are currently available. As a result, total number of tasks which can be run at the same time is (maxNumConcurrentSubTasks + 1) (including the supervisor task). Please note that this can be even larger than total number of task slots (sum of the capacity of all workers). If maxNumConcurrentSubTasks is larger than n (available task slots), then maxNumConcurrentSubTasks tasks are created by the supervisor task, but only n tasks would be started. Others will wait in the pending state until any running task is finished. If you are using the Parallel Index Task with stream ingestion together, we would recommend to limit the max capacity for batch ingestion to prevent stream ingestion from being blocked by batch ingestion. Suppose you have t Parallel Index Tasks to run at the same time, but want to limit the max number of tasks for batch ingestion to b. Then, (sum of maxNumConcurrentSubTasks of all Parallel Index Tasks + t (for supervisor tasks)) must be smaller than b. If you have some tasks of a higher priority than others, you may set their maxNumConcurrentSubTasks to a higher value than lower priority tasks. This may help the higher priority tasks to finish earlier than lower priority tasks by assigning more task slots to them. Simple task The simple task (type index) is designed to be used for smaller data sets. The task executes within the indexing service. Task syntax A sample task is shown below: { \"type\" : \"index\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"timestampSpec\" : { \"column\" : \"timestamp\", \"format\" : \"auto\" }, \"dimensionsSpec\" : { \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"], \"dimensionExclusions\" : [] }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"doubleSum\", \"name\" : \"added\", \"fieldName\" : \"added\" }, { \"type\" : \"doubleSum\", \"name\" : \"deleted\", \"fieldName\" : \"deleted\" }, { \"type\" : \"doubleSum\", \"name\" : \"delta\", \"fieldName\" : \"delta\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"DAY\", \"queryGranularity\" : \"NONE\", \"intervals\" : [ \"2013-08-31/2013-09-01\" ] } }, \"ioConfig\" : { \"type\" : \"index\", \"inputSource\" : { \"type\" : \"local\", \"baseDir\" : \"examples/indexing/\", \"filter\" : \"wikipedia_data.json\" }, \"inputFormat\": { \"type\": \"json\" } }, \"tuningConfig\" : { \"type\" : \"index\", \"maxRowsPerSegment\" : 5000000, \"maxRowsInMemory\" : 1000000 } } } property description required? type The task type, this should always be index. yes id The task ID. If this is not explicitly specified, Druid generates the task ID using task type, data source name, interval, and date-time stamp. no spec The ingestion spec including the data schema, IOConfig, and TuningConfig. See below for more details. yes context Context containing various task configuration parameters. See below for more details. no dataSchema This field is required. See the dataSchema section of the ingestion docs for details. If you do not specify intervals explicitly in your dataSchema's granularitySpec, the Local Index Task will do an extra pass over the data to determine the range to lock when it starts up. If you specify intervals explicitly, any rows outside the specified intervals will be thrown away. We recommend setting intervals explicitly if you know the time range of the data because it allows the task to skip the extra pass, and so that you don't accidentally replace data outside that range if there's some stray data with unexpected timestamps. ioConfig property description default required? type The task type, this should always be \"index\". none yes inputFormat inputFormat to specify how to parse input data. none yes appendToExisting Creates segments as additional shards of the latest version, effectively appending to the segment set instead of replacing it. The current limitation is that you can append to any datasources regardless of their original partitioning scheme, but the appended segments should be partitioned using the dynamic partitionsSpec. false no tuningConfig The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. See below for more details. property description default required? type The task type, this should always be \"index\". none yes maxRowsPerSegment Deprecated. Use partitionsSpec instead. Used in sharding. Determines how many rows are in each segment. 5000000 no maxRowsInMemory Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set. 1000000 no maxBytesInMemory Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists) 1/6 of max JVM memory no maxTotalRows Deprecated. Use partitionsSpec instead. Total number of rows in segments waiting for being pushed. Used in determining when intermediate pushing should occur. 20000000 no numShards Deprecated. Use partitionsSpec instead. Directly specify the number of shards to create. If this is specified and intervals is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. numShards cannot be specified if maxRowsPerSegment is set. null no partitionDimensions Deprecated. Use partitionsSpec instead. The dimensions to partition on. Leave blank to select all dimensions. Only used with forceGuaranteedRollup = true, will be ignored otherwise. null no partitionsSpec Defines how to partition data in each timeChunk, see PartitionsSpec dynamic if forceGuaranteedRollup = false, hashed if forceGuaranteedRollup = true no indexSpec Defines segment storage format options to be used at indexing time, see IndexSpec null no indexSpecForIntermediatePersists Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. this can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. however, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see IndexSpec for possible values. same as indexSpec no maxPendingPersists Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). 0 (meaning one persist can be running concurrently with ingestion, and none can be queued up) no forceGuaranteedRollup Forces guaranteeing the perfect rollup. The perfect rollup optimizes the total size of generated segments and querying time while indexing time will be increased. If this is set to true, the index task will read the entire input data twice: one for finding the optimal number of partitions per time chunk and one for generating segments. Note that the result segments would be hash-partitioned. This flag cannot be used with appendToExisting of IOConfig. For more details, see the below Segment pushing modes section. false no reportParseExceptions DEPRECATED. If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped. Setting reportParseExceptions to true will override existing configurations for maxParseExceptions and maxSavedParseExceptions, setting maxParseExceptions to 0 and limiting maxSavedParseExceptions to no more than 1. false no pushTimeout Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever. 0 no segmentWriteOutMediumFactory Segment write-out medium to use when creating segments. See SegmentWriteOutMediumFactory. Not specified, the value from druid.peon.defaultSegmentWriteOutMediumFactory.type is used no logParseExceptions If true, log an error message when a parsing exception occurs, containing information about the row where the error occurred. false no maxParseExceptions The maximum number of parse exceptions that can occur before the task halts ingestion and fails. Overridden if reportParseExceptions is set. unlimited no maxSavedParseExceptions When a parse exception occurs, Druid can keep track of the most recent parse exceptions. \"maxSavedParseExceptions\" limits how many exception instances will be saved. These saved exceptions will be made available after the task finishes in the task completion report. Overridden if reportParseExceptions is set. 0 no partitionsSpec PartitionsSpec is to describe the secondary partitioning method. You should use different partitionsSpec depending on the rollup mode you want. For perfect rollup, you should use hashed. property description default required? type This should always be hashed none yes maxRowsPerSegment Used in sharding. Determines how many rows are in each segment. 5000000 no numShards Directly specify the number of shards to create. If this is specified and intervals is specified in the granularitySpec, the index task can skip the determine intervals/partitions pass through the data. numShards cannot be specified if maxRowsPerSegment is set. null no partitionDimensions The dimensions to partition on. Leave blank to select all dimensions. null no partitionFunction A function to compute hash of partition dimensions. See Hash partition function murmur3_32_abs no For best-effort rollup, you should use dynamic. property description default required? type This should always be dynamic none yes maxRowsPerSegment Used in sharding. Determines how many rows are in each segment. 5000000 no maxTotalRows Total number of rows in segments waiting for being pushed. 20000000 no segmentWriteOutMediumFactory Field Type Description Required type String See Additional Peon Configuration: SegmentWriteOutMediumFactory for explanation and available options. yes Segment pushing modes While ingesting data using the Index task, it creates segments from the input data and pushes them. For segment pushing, the Index task supports two segment pushing modes, i.e., bulk pushing mode and incremental pushing mode for perfect rollup and best-effort rollup, respectively. In the bulk pushing mode, every segment is pushed at the very end of the index task. Until then, created segments are stored in the memory and local storage of the process running the index task. As a result, this mode might cause a problem due to limited storage capacity, and is not recommended to use in production. On the contrary, in the incremental pushing mode, segments are incrementally pushed, that is they can be pushed in the middle of the index task. More precisely, the index task collects data and stores created segments in the memory and disks of the process running that task until the total number of collected rows exceeds maxTotalRows. Once it exceeds, the index task immediately pushes all segments created until that moment, cleans all pushed segments up, and continues to ingest remaining data. To enable bulk pushing mode, forceGuaranteedRollup should be set in the TuningConfig. Note that this option cannot be used with appendToExisting of IOConfig. Input Sources The input source is the place to define from where your index task reads data. Only the native Parallel task and Simple task support the input source. S3 Input Source You need to include the druid-s3-extensions as an extension to use the S3 input source. The S3 input source is to support reading objects directly from S3. Objects can be specified either via a list of S3 URI strings or a list of S3 location prefixes, which will attempt to list the contents and ingest all objects contained in the locations. The S3 input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read one or multiple objects. Sample specs: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"s3\", \"uris\": [\"s3://foo/bar/file.json\", \"s3://bar/foo/file2.json\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"s3\", \"prefixes\": [\"s3://foo/bar\", \"s3://bar/foo\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"s3\", \"objects\": [ { \"bucket\": \"foo\", \"path\": \"bar/file1.json\"}, { \"bucket\": \"bar\", \"path\": \"foo/file2.json\"} ] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... property description default required? type This should be s3. None yes uris JSON array of URIs where S3 objects to be ingested are located. None uris or prefixes or objects must be set prefixes JSON array of URI prefixes for the locations of S3 objects to be ingested. Empty objects starting with one of the given prefixes will be skipped. None uris or prefixes or objects must be set objects JSON array of S3 Objects to be ingested. None uris or prefixes or objects must be set properties Properties Object for overriding the default S3 configuration. See below for more information. None No (defaults will be used if not given) Note that the S3 input source will skip all empty objects only when prefixes is specified. S3 Object: property description default required? bucket Name of the S3 bucket None yes path The path where data is located. None yes Properties Object: property description default required? accessKeyId The Password Provider or plain text string of this S3 InputSource's access key None yes if secretAccessKey is given secretAccessKey The Password Provider or plain text string of this S3 InputSource's secret key None yes if accessKeyId is given Note : If accessKeyId and secretAccessKey are not given, the default S3 credentials provider chain is used. Google Cloud Storage Input Source You need to include the druid-google-extensions as an extension to use the Google Cloud Storage input source. The Google Cloud Storage input source is to support reading objects directly from Google Cloud Storage. Objects can be specified as list of Google Cloud Storage URI strings. The Google Cloud Storage input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read one or multiple objects. Sample specs: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"google\", \"uris\": [\"gs://foo/bar/file.json\", \"gs://bar/foo/file2.json\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"google\", \"prefixes\": [\"gs://foo/bar\", \"gs://bar/foo\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"google\", \"objects\": [ { \"bucket\": \"foo\", \"path\": \"bar/file1.json\"}, { \"bucket\": \"bar\", \"path\": \"foo/file2.json\"} ] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... property description default required? type This should be google. None yes uris JSON array of URIs where Google Cloud Storage objects to be ingested are located. None uris or prefixes or objects must be set prefixes JSON array of URI prefixes for the locations of Google Cloud Storage objects to be ingested. Empty objects starting with one of the given prefixes will be skipped. None uris or prefixes or objects must be set objects JSON array of Google Cloud Storage objects to be ingested. None uris or prefixes or objects must be set Note that the Google Cloud Storage input source will skip all empty objects only when prefixes is specified. Google Cloud Storage object: property description default required? bucket Name of the Google Cloud Storage bucket None yes path The path where data is located. None yes Azure Input Source You need to include the druid-azure-extensions as an extension to use the Azure input source. The Azure input source is to support reading objects directly from Azure Blob store. Objects can be specified as list of Azure Blob store URI strings. The Azure input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read a single object. Sample specs: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"azure\", \"uris\": [\"azure://container/prefix1/file.json\", \"azure://container/prefix2/file2.json\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"azure\", \"prefixes\": [\"azure://container/prefix1\", \"azure://container/prefix2\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"azure\", \"objects\": [ { \"bucket\": \"container\", \"path\": \"prefix1/file1.json\"}, { \"bucket\": \"container\", \"path\": \"prefix2/file2.json\"} ] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... property description default required? type This should be azure. None yes uris JSON array of URIs where Azure Blob objects to be ingested are located. Should be in form \"azure://\\/\\\" None uris or prefixes or objects must be set prefixes JSON array of URI prefixes for the locations of Azure Blob objects to be ingested. Should be in the form \"azure://\\/\\\". Empty objects starting with one of the given prefixes will be skipped. None uris or prefixes or objects must be set objects JSON array of Azure Blob objects to be ingested. None uris or prefixes or objects must be set Note that the Azure input source will skip all empty objects only when prefixes is specified. Azure Blob object: property description default required? bucket Name of the Azure Blob Storage container None yes path The path where data is located. None yes HDFS Input Source You need to include the druid-hdfs-storage as an extension to use the HDFS input source. The HDFS input source is to support reading files directly from HDFS storage. File paths can be specified as an HDFS URI string or a list of HDFS URI strings. The HDFS input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read one or multiple files. Sample specs: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"hdfs\", \"paths\": \"hdfs://foo/bar/\", \"hdfs://bar/foo\" }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"hdfs\", \"paths\": [\"hdfs://foo/bar\", \"hdfs://bar/foo\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"hdfs\", \"paths\": \"hdfs://foo/bar/file.json\", \"hdfs://bar/foo/file2.json\" }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"hdfs\", \"paths\": [\"hdfs://foo/bar/file.json\", \"hdfs://bar/foo/file2.json\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... property description default required? type This should be hdfs. None yes paths HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like * are supported in these paths. Empty files located under one of the given paths will be skipped. None yes You can also ingest from cloud storage using the HDFS input source. However, if you want to read from AWS S3 or Google Cloud Storage, consider using the S3 input source or the Google Cloud Storage input source instead. HTTP Input Source The HTTP input source is to support reading files directly from remote sites via HTTP. The HTTP input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read only one file. This input source does not support Split Hint Spec. Sample specs: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"] }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... Example with authentication fields using the DefaultPassword provider (this requires the password to be in the ingestion spec): ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"], \"httpAuthenticationUsername\": \"username\", \"httpAuthenticationPassword\": \"password123\" }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... You can also use the other existing Druid PasswordProviders. Here is an example using the EnvironmentVariablePasswordProvider: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"], \"httpAuthenticationUsername\": \"username\", \"httpAuthenticationPassword\": { \"type\": \"environment\", \"variable\": \"HTTP_INPUT_SOURCE_PW\" } }, \"inputFormat\": { \"type\": \"json\" }, ... }, ... } property description default required? type This should be http None yes uris URIs of the input files. None yes httpAuthenticationUsername Username to use for authentication with specified URIs. Can be optionally used if the URIs specified in the spec require a Basic Authentication Header. None no httpAuthenticationPassword PasswordProvider to use with specified URIs. Can be optionally used if the URIs specified in the spec require a Basic Authentication Header. None no Inline Input Source The Inline input source can be used to read the data inlined in its own spec. It can be used for demos or for quickly testing out parsing and schema. Sample spec: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"inline\", \"data\": \"0,values,formatted\\n1,as,CSV\" }, \"inputFormat\": { \"type\": \"csv\" }, ... }, ... property description required? type This should be \"inline\". yes data Inlined data to ingest. yes Local Input Source The Local input source is to support reading files directly from local storage, and is mainly intended for proof-of-concept testing. The Local input source is splittable and can be used by the Parallel task, where each worker task of index_parallel will read one or multiple files. Sample spec: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"local\", \"filter\" : \"*.csv\", \"baseDir\": \"/data/directory\", \"files\": [\"/bar/foo\", \"/foo/bar\"] }, \"inputFormat\": { \"type\": \"csv\" }, ... }, ... property description required? type This should be \"local\". yes filter A wildcard filter for files. See here for more information. yes if baseDir is specified baseDir Directory to search recursively for files to be ingested. Empty files under the baseDir will be skipped. At least one of baseDir or files should be specified files File paths to ingest. Some files can be ignored to avoid ingesting duplicate files if they are located under the specified baseDir. Empty files will be skipped. At least one of baseDir or files should be specified Druid Input Source The Druid input source is to support reading data directly from existing Druid segments, potentially using a new schema and changing the name, dimensions, metrics, rollup, etc. of the segment. The Druid input source is splittable and can be used by the Parallel task. This input source has a fixed input format for reading from Druid segments; no inputFormat field needs to be specified in the ingestion spec when using this input source. property description required? type This should be \"druid\". yes dataSource A String defining the Druid datasource to fetch rows from yes interval A String representing an ISO-8601 interval, which defines the time range to fetch the data over. yes dimensions A list of Strings containing the names of dimension columns to select from the Druid datasource. If the list is empty, no dimensions are returned. If null, all dimensions are returned. no metrics The list of Strings containing the names of metric columns to select. If the list is empty, no metrics are returned. If null, all metrics are returned. no filter See Filters. Only rows that match the filter, if specified, will be returned. no A minimal example DruidInputSource spec is shown below: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"druid\", \"dataSource\": \"wikipedia\", \"interval\": \"2013-01-01/2013-01-02\" } ... }, ... The spec above will read all existing dimension and metric columns from the wikipedia datasource, including all rows with a timestamp (the __time column) within the interval 2013-01-01/2013-01-02. A spec that applies a filter and reads a subset of the original datasource's columns is shown below. ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"druid\", \"dataSource\": \"wikipedia\", \"interval\": \"2013-01-01/2013-01-02\", \"dimensions\": [ \"page\", \"user\" ], \"metrics\": [ \"added\" ], \"filter\": { \"type\": \"selector\", \"dimension\": \"page\", \"value\": \"Druid\" } } ... }, ... This spec above will only return the page, user dimensions and added metric. Only rows where page = Druid will be returned. SQL Input Source The SQL input source is used to read data directly from RDBMS. The SQL input source is splittable and can be used by the Parallel task, where each worker task will read from one SQL query from the list of queries. This input source does not support Split Hint Spec. Since this input source has a fixed input format for reading events, no inputFormat field needs to be specified in the ingestion spec when using this input source. Please refer to the Recommended practices section below before using this input source. property description required? type This should be \"sql\". Yes database Specifies the database connection details. The database type corresponds to the extension that supplies the connectorConfig support. The specified extension must be loaded into Druid:[mysql-metadata-storage](../development/extensions-core/mysql.md) for `mysql` [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extension for `postgresql`.You can selectively allow JDBC properties in connectURI. See JDBC connections security config for more details. Yes foldCase Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results. No sqls List of SQL queries where each SQL query would retrieve the data to be indexed. Yes An example SqlInputSource spec is shown below: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"sql\", \"database\": { \"type\": \"mysql\", \"connectorConfig\": { \"connectURI\": \"jdbc:mysql://host:port/schema\", \"user\": \"user\", \"password\": \"password\" } }, \"sqls\": [\"SELECT * FROM table1 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\", \"SELECT * FROM table2 WHERE timestamp BETWEEN '2013-01-01 00:00:00' AND '2013-01-01 11:59:59'\"] } }, ... The spec above will read all events from two separate SQLs for the interval 2013-01-01/2013-01-02. Each of the SQL queries will be run in its own sub-task and thus for the above example, there would be two sub-tasks. Recommended practices Compared to the other native batch InputSources, SQL InputSource behaves differently in terms of reading the input data and so it would be helpful to consider the following points before using this InputSource in a production environment: During indexing, each sub-task would execute one of the SQL queries and the results are stored locally on disk. The sub-tasks then proceed to read the data from these local input files and generate segments. Presently, there isn’t any restriction on the size of the generated files and this would require the MiddleManagers or Indexers to have sufficient disk capacity based on the volume of data being indexed. Filtering the SQL queries based on the intervals specified in the granularitySpec can avoid unwanted data being retrieved and stored locally by the indexing sub-tasks. For example, if the intervals specified in the granularitySpec is [\"2013-01-01/2013-01-02\"] and the SQL query is SELECT * FROM table1, SqlInputSource will read all the data for table1 based on the query, even though only data between the intervals specified will be indexed into Druid. Pagination may be used on the SQL queries to ensure that each query pulls a similar amount of data, thereby improving the efficiency of the sub-tasks. Similar to file-based input formats, any updates to existing data will replace the data in segments specific to the intervals specified in the granularitySpec. Combining Input Source The Combining input source is used to read data from multiple InputSources. This input source should be only used if all the delegate input sources are splittable and can be used by the Parallel task. This input source will identify the splits from its delegates and each split will be processed by a worker task. Similar to other input sources, this input source supports a single inputFormat. Therefore, please note that delegate input sources requiring an inputFormat must have the same format for input data. property description required? type This should be \"combining\". Yes delegates List of splittable InputSources to read data from. Yes Sample spec: ... \"ioConfig\": { \"type\": \"index_parallel\", \"inputSource\": { \"type\": \"combining\", \"delegates\" : [ { \"type\": \"local\", \"filter\" : \"*.csv\", \"baseDir\": \"/data/directory\", \"files\": [\"/bar/foo\", \"/foo/bar\"] }, { \"type\": \"druid\", \"dataSource\": \"wikipedia\", \"interval\": \"2013-01-01/2013-01-02\" } ] }, \"inputFormat\": { \"type\": \"csv\" }, ... }, ... # Firehoses (Deprecated) Firehoses are deprecated in 0.17.0. It's highly recommended to use the Input source instead. There are several firehoses readily available in Druid, some are meant for examples, others can be used directly in a production environment. StaticS3Firehose You need to include the druid-s3-extensions as an extension to use the StaticS3Firehose. This firehose ingests events from a predefined list of S3 objects. This firehose is splittable and can be used by the Parallel task. Since each split represents an object in this firehose, each worker task of index_parallel will read an object. Sample spec: \"firehose\" : { \"type\" : \"static-s3\", \"uris\": [\"s3://foo/bar/file.gz\", \"s3://bar/foo/file2.gz\"] } This firehose provides caching and prefetching features. In the Simple task, a firehose can be read twice if intervals or shardSpecs are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scan of objects is slow. Note that prefetching or caching isn't that useful in the Parallel task. property description default required? type This should be static-s3. None yes uris JSON array of URIs where s3 files to be ingested are located. None uris or prefixes must be set prefixes JSON array of URI prefixes for the locations of s3 files to be ingested. None uris or prefixes must be set maxCacheCapacityBytes Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes. 1073741824 no maxFetchCapacityBytes Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read. 1073741824 no prefetchTriggerBytes Threshold to trigger prefetching s3 objects. maxFetchCapacityBytes / 2 no fetchTimeout Timeout for fetching an s3 object. 60000 no maxFetchRetry Maximum retry for fetching an s3 object. 3 no StaticGoogleBlobStoreFirehose You need to include the druid-google-extensions as an extension to use the StaticGoogleBlobStoreFirehose. This firehose ingests events, similar to the StaticS3Firehose, but from an Google Cloud Store. As with the S3 blobstore, it is assumed to be gzipped if the extension ends in .gz This firehose is splittable and can be used by the Parallel task. Since each split represents an object in this firehose, each worker task of index_parallel will read an object. Sample spec: \"firehose\" : { \"type\" : \"static-google-blobstore\", \"blobs\": [ { \"bucket\": \"foo\", \"path\": \"/path/to/your/file.json\" }, { \"bucket\": \"bar\", \"path\": \"/another/path.json\" } ] } This firehose provides caching and prefetching features. In the Simple task, a firehose can be read twice if intervals or shardSpecs are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scan of objects is slow. Note that prefetching or caching isn't that useful in the Parallel task. property description default required? type This should be static-google-blobstore. None yes blobs JSON array of Google Blobs. None yes maxCacheCapacityBytes Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes. 1073741824 no maxFetchCapacityBytes Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read. 1073741824 no prefetchTriggerBytes Threshold to trigger prefetching Google Blobs. maxFetchCapacityBytes / 2 no fetchTimeout Timeout for fetching a Google Blob. 60000 no maxFetchRetry Maximum retry for fetching a Google Blob. 3 no Google Blobs: property description default required? bucket Name of the Google Cloud bucket None yes path The path where data is located. None yes HDFSFirehose You need to include the druid-hdfs-storage as an extension to use the HDFSFirehose. This firehose ingests events from a predefined list of files from the HDFS storage. This firehose is splittable and can be used by the Parallel task. Since each split represents an HDFS file, each worker task of index_parallel will read files. Sample spec: \"firehose\" : { \"type\" : \"hdfs\", \"paths\": \"/foo/bar,/foo/baz\" } This firehose provides caching and prefetching features. During native batch indexing, a firehose can be read twice if intervals are not specified, and, in this case, caching can be useful. Prefetching is preferred when direct scanning of files is slow. Note that prefetching or caching isn't that useful in the Parallel task. Property Description Default type This should be hdfs. none (required) paths HDFS paths. Can be either a JSON array or comma-separated string of paths. Wildcards like * are supported in these paths. none (required) maxCacheCapacityBytes Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes. 1073741824 maxFetchCapacityBytes Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read. 1073741824 prefetchTriggerBytes Threshold to trigger prefetching files. maxFetchCapacityBytes / 2 fetchTimeout Timeout for fetching each file. 60000 maxFetchRetry Maximum number of retries for fetching each file. 3 LocalFirehose This Firehose can be used to read the data from files on local disk, and is mainly intended for proof-of-concept testing, and works with string typed parsers. This Firehose is splittable and can be used by native parallel index tasks. Since each split represents a file in this Firehose, each worker task of index_parallel will read a file. A sample local Firehose spec is shown below: { \"type\": \"local\", \"filter\" : \"*.csv\", \"baseDir\": \"/data/directory\" } property description required? type This should be \"local\". yes filter A wildcard filter for files. See here for more information. yes baseDir directory to search recursively for files to be ingested. yes HttpFirehose This Firehose can be used to read the data from remote sites via HTTP, and works with string typed parsers. This Firehose is splittable and can be used by native parallel index tasks. Since each split represents a file in this Firehose, each worker task of index_parallel will read a file. A sample HTTP Firehose spec is shown below: { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"] } The below configurations can be optionally used if the URIs specified in the spec require a Basic Authentication Header. Omitting these fields from your spec will result in HTTP requests with no Basic Authentication Header. property description default httpAuthenticationUsername Username to use for authentication with specified URIs None httpAuthenticationPassword PasswordProvider to use with specified URIs None Example with authentication fields using the DefaultPassword provider (this requires the password to be in the ingestion spec): { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"], \"httpAuthenticationUsername\": \"username\", \"httpAuthenticationPassword\": \"password123\" } You can also use the other existing Druid PasswordProviders. Here is an example using the EnvironmentVariablePasswordProvider: { \"type\": \"http\", \"uris\": [\"http://example.com/uri1\", \"http://example2.com/uri2\"], \"httpAuthenticationUsername\": \"username\", \"httpAuthenticationPassword\": { \"type\": \"environment\", \"variable\": \"HTTP_FIREHOSE_PW\" } } The below configurations can optionally be used for tuning the Firehose performance. Note that prefetching or caching isn't that useful in the Parallel task. property description default maxCacheCapacityBytes Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes. 1073741824 maxFetchCapacityBytes Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read. 1073741824 prefetchTriggerBytes Threshold to trigger prefetching HTTP objects. maxFetchCapacityBytes / 2 fetchTimeout Timeout for fetching an HTTP object. 60000 maxFetchRetry Maximum retries for fetching an HTTP object. 3 IngestSegmentFirehose This Firehose can be used to read the data from existing druid segments, potentially using a new schema and changing the name, dimensions, metrics, rollup, etc. of the segment. This Firehose is splittable and can be used by native parallel index tasks. This firehose will accept any type of parser, but will only utilize the list of dimensions and the timestamp specification. A sample ingest Firehose spec is shown below: { \"type\": \"ingestSegment\", \"dataSource\": \"wikipedia\", \"interval\": \"2013-01-01/2013-01-02\" } property description required? type This should be \"ingestSegment\". yes dataSource A String defining the data source to fetch rows from, very similar to a table in a relational database yes interval A String representing the ISO-8601 interval. This defines the time range to fetch the data over. yes dimensions The list of dimensions to select. If left empty, no dimensions are returned. If left null or not defined, all dimensions are returned. no metrics The list of metrics to select. If left empty, no metrics are returned. If left null or not defined, all metrics are selected. no filter See Filters no maxInputSegmentBytesPerTask Deprecated. Use Segments Split Hint Spec instead. When used with the native parallel index task, the maximum number of bytes of input segments to process in a single task. If a single segment is larger than this number, it will be processed by itself in a single task (input segments are never split across tasks). Defaults to 150MB. no SqlFirehose This Firehose can be used to ingest events residing in an RDBMS. The database connection information is provided as part of the ingestion spec. For each query, the results are fetched locally and indexed. If there are multiple queries from which data needs to be indexed, queries are prefetched in the background, up to maxFetchCapacityBytes bytes. This Firehose is splittable and can be used by native parallel index tasks. This firehose will accept any type of parser, but will only utilize the list of dimensions and the timestamp specification. See the extension documentation for more detailed ingestion examples. Requires one of the following extensions: MySQL Metadata Store. PostgreSQL Metadata Store. { \"type\": \"sql\", \"database\": { \"type\": \"mysql\", \"connectorConfig\": { \"connectURI\": \"jdbc:mysql://host:port/schema\", \"user\": \"user\", \"password\": \"password\" } }, \"sqls\": [\"SELECT * FROM table1\", \"SELECT * FROM table2\"] } property description default required? type This should be \"sql\". Yes database Specifies the database connection details. The database type corresponds to the extension that supplies the connectorConfig support. The specified extension must be loaded into Druid:[mysql-metadata-storage](../development/extensions-core/mysql.md) for `mysql` [postgresql-metadata-storage](../development/extensions-core/postgresql.md) extension for `postgresql`.You can selectively allow JDBC properties in connectURI. See JDBC connections security config for more details. Yes maxCacheCapacityBytes Maximum size of the cache space in bytes. 0 means disabling cache. Cached files are not removed until the ingestion task completes. 1073741824 No maxFetchCapacityBytes Maximum size of the fetch space in bytes. 0 means disabling prefetch. Prefetched files are removed immediately once they are read. 1073741824 No prefetchTriggerBytes Threshold to trigger prefetching SQL result objects. maxFetchCapacityBytes / 2 No fetchTimeout Timeout for fetching the result set. 60000 No foldCase Toggle case folding of database column names. This may be enabled in cases where the database returns case insensitive column names in query results. false No sqls List of SQL queries where each SQL query would retrieve the data to be indexed. Yes Database property description default required? type The type of database to query. Valid values are mysql and postgresql_ Yes connectorConfig Specify the database connection properties via connectURI, user and password Yes InlineFirehose This Firehose can be used to read the data inlined in its own spec. It can be used for demos or for quickly testing out parsing and schema, and works with string typed parsers. A sample inline Firehose spec is shown below: { \"type\": \"inline\", \"data\": \"0,values,formatted\\n1,as,CSV\" } property description required? type This should be \"inline\". yes data Inlined data to ingest. yes CombiningFirehose This Firehose can be used to combine and merge data from a list of different Firehoses. { \"type\": \"combining\", \"delegates\": [ { firehose1 }, { firehose2 }, ... ] } property description required? type This should be \"combining\" yes delegates List of Firehoses to combine data from yes 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/hadoop.html":{"url":"ingestion/hadoop.html","title":"从Hadoop导入","keywords":"","body":" Apache Hadoop-based batch ingestion in Apache Druid is supported via a Hadoop-ingestion task. These tasks can be posted to a running instance of a Druid Overlord. Please refer to our Hadoop-based vs. native batch comparison table for comparisons between Hadoop-based, native batch (simple), and native batch (parallel) ingestion. To run a Hadoop-based ingestion task, write an ingestion spec as specified below. Then POST it to the /druid/indexer/v1/task endpoint on the Overlord, or use the bin/post-index-task script included with Druid. Tutorial This page contains reference documentation for Hadoop-based ingestion. For a walk-through instead, check out the Loading from Apache Hadoop tutorial. Task syntax A sample task is shown below: { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"timestampSpec\" : { \"column\" : \"timestamp\", \"format\" : \"auto\" }, \"dimensionsSpec\" : { \"dimensions\": [\"page\",\"language\",\"user\",\"unpatrolled\",\"newPage\",\"robot\",\"anonymous\",\"namespace\",\"continent\",\"country\",\"region\",\"city\"], \"dimensionExclusions\" : [], \"spatialDimensions\" : [] } } }, \"metricsSpec\" : [ { \"type\" : \"count\", \"name\" : \"count\" }, { \"type\" : \"doubleSum\", \"name\" : \"added\", \"fieldName\" : \"added\" }, { \"type\" : \"doubleSum\", \"name\" : \"deleted\", \"fieldName\" : \"deleted\" }, { \"type\" : \"doubleSum\", \"name\" : \"delta\", \"fieldName\" : \"delta\" } ], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"DAY\", \"queryGranularity\" : \"NONE\", \"intervals\" : [ \"2013-08-31/2013-09-01\" ] } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/MyDirectory/example/wikipedia_data.json\" } }, \"tuningConfig\" : { \"type\": \"hadoop\" } }, \"hadoopDependencyCoordinates\": } property description required? type The task type, this should always be \"index_hadoop\". yes spec A Hadoop Index Spec. See Ingestion yes hadoopDependencyCoordinates A JSON array of Hadoop dependency coordinates that Druid will use, this property will override the default Hadoop coordinates. Once specified, Druid will look for those Hadoop dependencies from the location specified by druid.extensions.hadoopDependenciesDir no classpathPrefix Classpath that will be prepended for the Peon process. no Also note that Druid automatically computes the classpath for Hadoop job containers that run in the Hadoop cluster. But in case of conflicts between Hadoop and Druid's dependencies, you can manually specify the classpath by setting druid.extensions.hadoopContainerDruidClasspath property. See the extensions config in base druid configuration. dataSchema This field is required. See the dataSchema section of the main ingestion page for details on what it should contain. ioConfig This field is required. Field Type Description Required type String This should always be 'hadoop'. yes inputSpec Object A specification of where to pull the data in from. See below. yes segmentOutputPath String The path to dump segments into. Only used by the Command-line Hadoop indexer. This field must be null otherwise. metadataUpdateSpec Object A specification of how to update the metadata for the druid cluster these segments belong to. Only used by the Command-line Hadoop indexer. This field must be null otherwise. inputSpec There are multiple types of inputSpecs: static A type of inputSpec where a static path to the data files is provided. Field Type Description Required inputFormat String Specifies the Hadoop InputFormat class to use. e.g. org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat no paths Array of String A String of input paths indicating where the raw data is located. yes For example, using the static input paths: \"paths\" : \"hdfs://path/to/data/is/here/data.gz,hdfs://path/to/data/is/here/moredata.gz,hdfs://path/to/data/is/here/evenmoredata.gz\" You can also read from cloud storage such as AWS S3 or Google Cloud Storage. To do so, you need to install the necessary library under Druid's classpath in all MiddleManager or Indexer processes. For S3, you can run the below command to install the Hadoop AWS module. java -classpath \"${DRUID_HOME}lib/*\" org.apache.druid.cli.Main tools pull-deps -h \"org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}\"; cp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/ Once you install the Hadoop AWS module in all MiddleManager and Indexer processes, you can put your S3 paths in the inputSpec with the below job properties. For more configurations, see the Hadoop AWS module. \"paths\" : \"s3a://billy-bucket/the/data/is/here/data.gz,s3a://billy-bucket/the/data/is/here/moredata.gz,s3a://billy-bucket/the/data/is/here/evenmoredata.gz\" \"jobProperties\" : { \"fs.s3a.impl\" : \"org.apache.hadoop.fs.s3a.S3AFileSystem\", \"fs.AbstractFileSystem.s3a.impl\" : \"org.apache.hadoop.fs.s3a.S3A\", \"fs.s3a.access.key\" : \"YOUR_ACCESS_KEY\", \"fs.s3a.secret.key\" : \"YOUR_SECRET_KEY\" } For Google Cloud Storage, you need to install GCS connector jar under ${DRUID_HOME}/hadoop-dependencies in all MiddleManager or Indexer processes. Once you install the GCS Connector jar in all MiddleManager and Indexer processes, you can put your Google Cloud Storage paths in the inputSpec with the below job properties. For more configurations, see the instructions to configure Hadoop, GCS core default and GCS core template. \"paths\" : \"gs://billy-bucket/the/data/is/here/data.gz,gs://billy-bucket/the/data/is/here/moredata.gz,gs://billy-bucket/the/data/is/here/evenmoredata.gz\" \"jobProperties\" : { \"fs.gs.impl\" : \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\", \"fs.AbstractFileSystem.gs.impl\" : \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\" } granularity A type of inputSpec that expects data to be organized in directories according to datetime using the path format: y=XXXX/m=XX/d=XX/H=XX/M=XX/S=XX (where date is represented by lowercase and time is represented by uppercase). Field Type Description Required dataGranularity String Specifies the granularity to expect the data at, e.g. hour means to expect directories y=XXXX/m=XX/d=XX/H=XX. yes inputFormat String Specifies the Hadoop InputFormat class to use. e.g. org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat no inputPath String Base path to append the datetime path to. yes filePattern String Pattern that files should match to be included. yes pathFormat String Joda datetime format for each directory. Default value is \"'y'=yyyy/'m'=MM/'d'=dd/'H'=HH\", or see Joda documentation no For example, if the sample config were run with the interval 2012-06-01/2012-06-02, it would expect data at the paths: s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=00 s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=01 ... s3n://billy-bucket/the/data/is/here/y=2012/m=06/d=01/H=23 dataSource This is a type of inputSpec that reads data already stored inside Druid. This is used to allow \"re-indexing\" data and for \"delta-ingestion\" described later in multi type inputSpec. Field Type Description Required type String. This should always be 'dataSource'. yes ingestionSpec JSON object. Specification of Druid segments to be loaded. See below. yes maxSplitSize Number Enables combining multiple segments into single Hadoop InputSplit according to size of segments. With -1, druid calculates max split size based on user specified number of map task(mapred.map.tasks or mapreduce.job.maps). By default, one split is made for one segment. maxSplitSize is specified in bytes. no useNewAggs Boolean If \"false\", then list of aggregators in \"metricsSpec\" of hadoop indexing task must be same as that used in original indexing task while ingesting raw data. Default value is \"false\". This field can be set to \"true\" when \"inputSpec\" type is \"dataSource\" and not \"multi\" to enable arbitrary aggregators while reindexing. See below for \"multi\" type support for delta-ingestion. no Here is what goes inside ingestionSpec: Field Type Description Required dataSource String Druid dataSource name from which you are loading the data. yes intervals List A list of strings representing ISO-8601 Intervals. yes segments List List of segments from which to read data from, by default it is obtained automatically. You can obtain list of segments to put here by making a POST query to Coordinator at url /druid/coordinator/v1/metadata/datasources/segments?full with list of intervals specified in the request payload, e.g. [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"]. You may want to provide this list manually in order to ensure that segments read are exactly same as they were at the time of task submission, task would fail if the list provided by the user does not match with state of database when the task actually runs. no filter JSON See Filters no dimensions Array of String Name of dimension columns to load. By default, the list will be constructed from parseSpec. If parseSpec does not have an explicit list of dimensions then all the dimension columns present in stored data will be read. no metrics Array of String Name of metric columns to load. By default, the list will be constructed from the \"name\" of all the configured aggregators. no ignoreWhenNoSegments boolean Whether to ignore this ingestionSpec if no segments were found. Default behavior is to throw error when no segments were found. no For example \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"dataSource\", \"ingestionSpec\" : { \"dataSource\": \"wikipedia\", \"intervals\": [\"2014-10-20T00:00:00Z/P2W\"] } }, ... } multi This is a composing inputSpec to combine other inputSpecs. This inputSpec is used for delta ingestion. You can also use a multi inputSpec to combine data from multiple dataSources. However, each particular dataSource can only be specified one time. Note that, \"useNewAggs\" must be set to default value false to support delta-ingestion. Field Type Description Required children Array of JSON objects List of JSON objects containing other inputSpecs. yes For example: \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"multi\", \"children\": [ { \"type\" : \"dataSource\", \"ingestionSpec\" : { \"dataSource\": \"wikipedia\", \"intervals\": [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"], \"segments\": [ { \"dataSource\": \"test1\", \"interval\": \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"version\": \"v2\", \"loadSpec\": { \"type\": \"local\", \"path\": \"/tmp/index1.zip\" }, \"dimensions\": \"host\", \"metrics\": \"visited_sum,unique_hosts\", \"shardSpec\": { \"type\": \"none\" }, \"binaryVersion\": 9, \"size\": 2, \"identifier\": \"test1_2000-01-01T00:00:00.000Z_3000-01-01T00:00:00.000Z_v2\" } ] } }, { \"type\" : \"static\", \"paths\": \"/path/to/more/wikipedia/data/\" } ] }, ... } It is STRONGLY RECOMMENDED to provide list of segments in dataSource inputSpec explicitly so that your delta ingestion task is idempotent. You can obtain that list of segments by making following call to the Coordinator. POST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments?full Request Body: [interval1, interval2,...] for example [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"] tuningConfig The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. Field Type Description Required workingPath String The working path to use for intermediate results (results between Hadoop jobs). Only used by the Command-line Hadoop indexer. The default is '/tmp/druid-indexing'. This field must be null otherwise. version String The version of created segments. Ignored for HadoopIndexTask unless useExplicitVersion is set to true no (default == datetime that indexing starts at) partitionsSpec Object A specification of how to partition each time bucket into segments. Absence of this property means no partitioning will occur. See partitionsSpec below. no (default == 'hashed') maxRowsInMemory Integer The number of rows to aggregate before persisting. Note that this is the number of post-aggregation rows which may not be equal to the number of input events due to roll-up. This is used to manage the required JVM heap size. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set. no (default == 1000000) maxBytesInMemory Long The number of bytes to aggregate in heap memory before persisting. Normally this is computed internally and user does not need to set it. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists). no (default == One-sixth of max JVM memory) leaveIntermediate Boolean Leave behind intermediate files (for debugging) in the workingPath when a job completes, whether it passes or fails. no (default == false) cleanupOnFailure Boolean Clean up intermediate files when a job fails (unless leaveIntermediate is on). no (default == true) overwriteFiles Boolean Override existing files found during indexing. no (default == false) ignoreInvalidRows Boolean DEPRECATED. Ignore rows found to have problems. If false, any exception encountered during parsing will be thrown and will halt ingestion; if true, unparseable rows and fields will be skipped. If maxParseExceptions is defined, this property is ignored. no (default == false) combineText Boolean Use CombineTextInputFormat to combine multiple files into a file split. This can speed up Hadoop jobs when processing a large number of small files. no (default == false) useCombiner Boolean Use Hadoop combiner to merge rows at mapper if possible. no (default == false) jobProperties Object A map of properties to add to the Hadoop job configuration, see below for details. no (default == null) indexSpec Object Tune how data is indexed. See indexSpec on the main ingestion page for more information. no indexSpecForIntermediatePersists Object defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. this can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. however, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see indexSpec for possible values. no (default = same as indexSpec) numBackgroundPersistThreads Integer The number of new background threads to use for incremental persists. Using this feature causes a notable increase in memory pressure and CPU usage but will make the job finish more quickly. If changing from the default of 0 (use current thread for persists), we recommend setting it to 1. no (default == 0) forceExtendableShardSpecs Boolean Forces use of extendable shardSpecs. Hash-based partitioning always uses an extendable shardSpec. For single-dimension partitioning, this option should be set to true to use an extendable shardSpec. For partitioning, please check Partitioning specification. This option can be useful when you need to append more data to existing dataSource. no (default = false) useExplicitVersion Boolean Forces HadoopIndexTask to use version. no (default = false) logParseExceptions Boolean If true, log an error message when a parsing exception occurs, containing information about the row where the error occurred. no(default = false) maxParseExceptions Integer The maximum number of parse exceptions that can occur before the task halts ingestion and fails. Overrides ignoreInvalidRows if maxParseExceptions is defined. no(default = unlimited) useYarnRMJobStatusFallback Boolean If the Hadoop jobs created by the indexing task are unable to retrieve their completion status from the JobHistory server, and this parameter is true, the indexing task will try to fetch the application status from http:///ws/v1/cluster/apps/, where is the value of yarn.resourcemanager.webapp.address in your Hadoop configuration. This flag is intended as a fallback for cases where an indexing task's jobs succeed, but the JobHistory server is unavailable, causing the indexing task to fail because it cannot determine the job statuses. no (default = true) jobProperties \"tuningConfig\" : { \"type\": \"hadoop\", \"jobProperties\": { \"\": \"\", \"\": \"\" } } Hadoop's MapReduce documentation lists the possible configuration parameters. With some Hadoop distributions, it may be necessary to set mapreduce.job.classpath or mapreduce.job.user.classpath.first to avoid class loading issues. See the working with different Hadoop versions documentation for more details. partitionsSpec Segments are always partitioned based on timestamp (according to the granularitySpec) and may be further partitioned in some other way depending on partition type. Druid supports two types of partitioning strategies: hashed (based on the hash of all dimensions in each row), and single_dim (based on ranges of a single dimension). Hashed partitioning is recommended in most cases, as it will improve indexing performance and create more uniformly sized data segments relative to single-dimension partitioning. Hash-based partitioning \"partitionsSpec\": { \"type\": \"hashed\", \"targetRowsPerSegment\": 5000000 } Hashed partitioning works by first selecting a number of segments, and then partitioning rows across those segments according to the hash of all dimensions in each row. The number of segments is determined automatically based on the cardinality of the input set and a target partition size. The configuration options are: Field Description Required type Type of partitionSpec to be used. \"hashed\" targetRowsPerSegment Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. Defaults to 5000000 if numShards is not set. either this or numShards targetPartitionSize Deprecated. Renamed to targetRowsPerSegment. Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. either this or numShards maxRowsPerSegment Deprecated. Renamed to targetRowsPerSegment. Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. either this or numShards numShards Specify the number of partitions directly, instead of a target partition size. Ingestion will run faster, since it can skip the step necessary to select a number of partitions automatically. either this or maxRowsPerSegment partitionDimensions The dimensions to partition on. Leave blank to select all dimensions. Only used with numShards, will be ignored when targetRowsPerSegment is set. no partitionFunction A function to compute hash of partition dimensions. See Hash partition function murmur3_32_abs no Hash partition function In hash partitioning, the partition function is used to compute hash of partition dimensions. The partition dimension values are first serialized into a byte array as a whole, and then the partition function is applied to compute hash of the byte array. Druid currently supports only one partition function. name description murmur3_32_abs Applies an absolute value function to the result of murmur3_32). Single-dimension range partitioning \"partitionsSpec\": { \"type\": \"single_dim\", \"targetRowsPerSegment\": 5000000 } Single-dimension range partitioning works by first selecting a dimension to partition on, and then separating that dimension into contiguous ranges. Each segment will contain all rows with values of that dimension in that range. For example, your segments may be partitioned on the dimension \"host\" using the ranges \"a.example.com\" to \"f.example.com\" and \"f.example.com\" to \"z.example.com\". By default, the dimension to use is determined automatically, although you can override it with a specific dimension. The configuration options are: Field Description Required type Type of partitionSpec to be used. \"single_dim\" targetRowsPerSegment Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. yes targetPartitionSize Deprecated. Renamed to targetRowsPerSegment. Target number of rows to include in a partition, should be a number that targets segments of 500MB~1GB. no maxRowsPerSegment Maximum number of rows to include in a partition. Defaults to 50% larger than the targetRowsPerSegment. no maxPartitionSize Deprecated. Use maxRowsPerSegment instead. Maximum number of rows to include in a partition. Defaults to 50% larger than the targetPartitionSize. no partitionDimension The dimension to partition on. Leave blank to select a dimension automatically. no assumeGrouped Assume that input data has already been grouped on time and dimensions. Ingestion will run faster, but may choose sub-optimal partitions if this assumption is violated. no Remote Hadoop clusters If you have a remote Hadoop cluster, make sure to include the folder holding your configuration *.xml files in your Druid _common configuration folder. If you are having dependency problems with your version of Hadoop and the version compiled with Druid, please see these docs. Elastic MapReduce If your cluster is running on Amazon Web Services, you can use Elastic MapReduce (EMR) to index data from S3. To do this: Create a persistent, long-running cluster. When creating your cluster, enter the following configuration. If you're using the wizard, this should be in advanced mode under \"Edit software settings\": classification=yarn-site,properties=[mapreduce.reduce.memory.mb=6144,mapreduce.reduce.java.opts=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.map.java.opts=758,mapreduce.map.java.opts=-server -Xms512m -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps,mapreduce.task.timeout=1800000] Follow the instructions under Configure for connecting to Hadoop using the XML files from /etc/hadoop/conf on your EMR master. Kerberized Hadoop clusters By default druid can use the existing TGT kerberos ticket available in local kerberos key cache. Although TGT ticket has a limited life cycle, therefore you need to call kinit command periodically to ensure validity of TGT ticket. To avoid this extra external cron job script calling kinit periodically, you can provide the principal name and keytab location and druid will do the authentication transparently at startup and job launching time. Property Possible Values Description Default druid.hadoop.security.kerberos.principal druid@EXAMPLE.COM Principal user name empty druid.hadoop.security.kerberos.keytab /etc/security/keytabs/druid.headlessUser.keytab Path to keytab file empty Loading from S3 with EMR In the jobProperties field in the tuningConfig section of your Hadoop indexing task, add: \"jobProperties\" : { \"fs.s3.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\", \"fs.s3.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\", \"fs.s3.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\", \"fs.s3n.awsAccessKeyId\" : \"YOUR_ACCESS_KEY\", \"fs.s3n.awsSecretAccessKey\" : \"YOUR_SECRET_KEY\", \"fs.s3n.impl\" : \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\", \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec\" } Note that this method uses Hadoop's built-in S3 filesystem rather than Amazon's EMRFS, and is not compatible with Amazon-specific features such as S3 encryption and consistent views. If you need to use these features, you will need to make the Amazon EMR Hadoop JARs available to Druid through one of the mechanisms described in the Using other Hadoop distributions section. Using other Hadoop distributions Druid works out of the box with many Hadoop distributions. If you are having dependency conflicts between Druid and your version of Hadoop, you can try searching for a solution in the Druid user groups, or reading the Druid Different Hadoop Versions documentation. Command line (non-task) version To run: java -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -classpath lib/*: org.apache.druid.cli.Main index hadoop Options \"--coordinate\" - provide a version of Apache Hadoop to use. This property will override the default Hadoop coordinates. Once specified, Apache Druid will look for those Hadoop dependencies from the location specified by druid.extensions.hadoopDependenciesDir. \"--no-default-hadoop\" - don't pull down the default hadoop version Spec file The spec file needs to contain a JSON object where the contents are the same as the \"spec\" field in the Hadoop index task. See Hadoop Batch Ingestion for details on the spec format. In addition, a metadataUpdateSpec and segmentOutputPath field needs to be added to the ioConfig: \"ioConfig\" : { ... \"metadataUpdateSpec\" : { \"type\":\"mysql\", \"connectURI\" : \"jdbc:mysql://localhost:3306/druid\", \"password\" : \"diurd\", \"segmentTable\" : \"druid_segments\", \"user\" : \"druid\" }, \"segmentOutputPath\" : \"/MyDirectory/data/index/output\" }, and a workingPath field needs to be added to the tuningConfig: \"tuningConfig\" : { ... \"workingPath\": \"/tmp\", ... } Metadata Update Job Spec This is a specification of the properties that tell the job how to update metadata such that the Druid cluster will see the output segments and load them. Field Type Description Required type String \"metadata\" is the only value available. yes connectURI String A valid JDBC url to metadata storage. yes user String Username for db. yes password String password for db. yes segmentTable String Table to use in DB. yes These properties should parrot what you have configured for your Coordinator. segmentOutputPath Config Field Type Description Required segmentOutputPath String the path to dump segments into. yes workingPath Config Field Type Description Required workingPath String the working path to use for intermediate results (results between Hadoop jobs). no (default == '/tmp/druid-indexing') Please note that the command line Hadoop indexer doesn't have the locking capabilities of the indexing service, so if you choose to use it, you have to take caution to not override segments created by real-time processing (if you that a real-time pipeline set up). 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/tasks.html":{"url":"ingestion/tasks.html","title":"Druid中的任务","keywords":"","body":" Tasks do all ingestion-related work in Druid. For batch ingestion, you will generally submit tasks directly to Druid using the Task APIs. For streaming ingestion, tasks are generally submitted for you by a supervisor. Task API Task APIs are available in two main places: The Overlord process offers HTTP APIs to submit tasks, cancel tasks, check their status, review logs and reports, and more. Refer to the Tasks API reference page for a full list. Druid SQL includes a sys.tasks table that provides information about currently running tasks. This table is read-only, and has a limited (but useful!) subset of the full information available through the Overlord APIs. Task reports A report containing information about the number of rows ingested, and any parse exceptions that occurred is available for both completed tasks and running tasks. The reporting feature is supported by the simple native batch task, the Hadoop batch task, and Kafka and Kinesis ingestion tasks. Completion report After a task completes, a completion report can be retrieved at: http://:/druid/indexer/v1/task//reports An example output is shown below: { \"ingestionStatsAndErrors\": { \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\", \"payload\": { \"ingestionState\": \"COMPLETED\", \"unparseableEvents\": {}, \"rowStats\": { \"determinePartitions\": { \"processed\": 0, \"processedWithError\": 0, \"thrownAway\": 0, \"unparseable\": 0 }, \"buildSegments\": { \"processed\": 5390324, \"processedWithError\": 0, \"thrownAway\": 0, \"unparseable\": 0 } }, \"errorMsg\": null }, \"type\": \"ingestionStatsAndErrors\" } } Live report When a task is running, a live report containing ingestion state, unparseable events and moving average for number of events processed for 1 min, 5 min, 15 min time window can be retrieved at: http://:/druid/indexer/v1/task//reports and http://:/druid/worker/v1/chat//liveReports An example output is shown below: { \"ingestionStatsAndErrors\": { \"taskId\": \"compact_twitter_2018-09-24T18:24:23.920Z\", \"payload\": { \"ingestionState\": \"RUNNING\", \"unparseableEvents\": {}, \"rowStats\": { \"movingAverages\": { \"buildSegments\": { \"5m\": { \"processed\": 3.392158326408501, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 }, \"15m\": { \"processed\": 1.736165476881023, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 }, \"1m\": { \"processed\": 4.206417693750045, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 } } }, \"totals\": { \"buildSegments\": { \"processed\": 1994, \"processedWithError\": 0, \"thrownAway\": 0, \"unparseable\": 0 } } }, \"errorMsg\": null }, \"type\": \"ingestionStatsAndErrors\" } } A description of the fields: The ingestionStatsAndErrors report provides information about row counts and errors. The ingestionState shows what step of ingestion the task reached. Possible states include: NOT_STARTED: The task has not begun reading any rows DETERMINE_PARTITIONS: The task is processing rows to determine partitioning BUILD_SEGMENTS: The task is processing rows to construct segments COMPLETED: The task has finished its work. Only batch tasks have the DETERMINE_PARTITIONS phase. Realtime tasks such as those created by the Kafka Indexing Service do not have a DETERMINE_PARTITIONS phase. unparseableEvents contains lists of exception messages that were caused by unparseable inputs. This can help with identifying problematic input rows. There will be one list each for the DETERMINE_PARTITIONS and BUILD_SEGMENTS phases. Note that the Hadoop batch task does not support saving of unparseable events. the rowStats map contains information about row counts. There is one entry for each ingestion phase. The definitions of the different row counts are shown below: processed: Number of rows successfully ingested without parsing errors processedWithError: Number of rows that were ingested, but contained a parsing error within one or more columns. This typically occurs where input rows have a parseable structure but invalid types for columns, such as passing in a non-numeric String value for a numeric column. thrownAway: Number of rows skipped. This includes rows with timestamps that were outside of the ingestion task's defined time interval and rows that were filtered out with a transformSpec, but doesn't include the rows skipped by explicit user configurations. For example, the rows skipped by skipHeaderRows or hasHeaderRow in the CSV format are not counted. unparseable: Number of rows that could not be parsed at all and were discarded. This tracks input rows without a parseable structure, such as passing in non-JSON data when using a JSON parser. The errorMsg field shows a message describing the error that caused a task to fail. It will be null if the task was successful. Live reports Row stats The non-parallel simple native batch task, the Hadoop batch task, and Kafka and Kinesis ingestion tasks support retrieval of row stats while the task is running. The live report can be accessed with a GET to the following URL on a Peon running a task: http://:/druid/worker/v1/chat//rowStats An example report is shown below. The movingAverages section contains 1 minute, 5 minute, and 15 minute moving averages of increases to the four row counters, which have the same definitions as those in the completion report. The totals section shows the current totals. { \"movingAverages\": { \"buildSegments\": { \"5m\": { \"processed\": 3.392158326408501, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 }, \"15m\": { \"processed\": 1.736165476881023, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 }, \"1m\": { \"processed\": 4.206417693750045, \"unparseable\": 0, \"thrownAway\": 0, \"processedWithError\": 0 } } }, \"totals\": { \"buildSegments\": { \"processed\": 1994, \"processedWithError\": 0, \"thrownAway\": 0, \"unparseable\": 0 } } } For the Kafka Indexing Service, a GET to the following Overlord API will retrieve live row stat reports from each task being managed by the supervisor and provide a combined report. http://:/druid/indexer/v1/supervisor//stats Unparseable events Lists of recently-encountered unparseable events can be retrieved from a running task with a GET to the following Peon API: http://:/druid/worker/v1/chat//unparseableEvents Note that this functionality is not supported by all task types. Currently, it is only supported by the non-parallel native batch task (type index) and the tasks created by the Kafka and Kinesis indexing services. Task lock system This section explains the task locking system in Druid. Druid's locking system and versioning system are tightly coupled with each other to guarantee the correctness of ingested data. \"Overshadowing\" between segments You can run a task to overwrite existing data. The segments created by an overwriting task overshadows existing segments. Note that the overshadow relation holds only for the same time chunk and the same data source. These overshadowed segments are not considered in query processing to filter out stale data. Each segment has a major version and a minor version. The major version is represented as a timestamp in the format of \"yyyy-MM-dd'T'hh:mm:ss\" while the minor version is an integer number. These major and minor versions are used to determine the overshadow relation between segments as seen below. A segment s1 overshadows another s2 if s1 has a higher major version than s2, or s1 has the same major version and a higher minor version than s2. Here are some examples. A segment of the major version of 2019-01-01T00:00:00.000Z and the minor version of 0 overshadows another of the major version of 2018-01-01T00:00:00.000Z and the minor version of 1. A segment of the major version of 2019-01-01T00:00:00.000Z and the minor version of 1 overshadows another of the major version of 2019-01-01T00:00:00.000Z and the minor version of 0. Locking If you are running two or more druid tasks which generate segments for the same data source and the same time chunk, the generated segments could potentially overshadow each other, which could lead to incorrect query results. To avoid this problem, tasks will attempt to get locks prior to creating any segment in Druid. There are two types of locks, i.e., time chunk lock and segment lock. When the time chunk lock is used, a task locks the entire time chunk of a data source where generated segments will be written. For example, suppose we have a task ingesting data into the time chunk of 2019-01-01T00:00:00.000Z/2019-01-02T00:00:00.000Z of the wikipedia data source. With the time chunk locking, this task will lock the entire time chunk of 2019-01-01T00:00:00.000Z/2019-01-02T00:00:00.000Z of the wikipedia data source before it creates any segments. As long as it holds the lock, any other tasks will be unable to create segments for the same time chunk of the same data source. The segments created with the time chunk locking have a higher major version than existing segments. Their minor version is always 0. When the segment lock is used, a task locks individual segments instead of the entire time chunk. As a result, two or more tasks can create segments for the same time chunk of the same data source simultaneously if they are reading different segments. For example, a Kafka indexing task and a compaction task can always write segments into the same time chunk of the same data source simultaneously. The reason for this is because a Kafka indexing task always appends new segments, while a compaction task always overwrites existing segments. The segments created with the segment locking have the same major version and a higher minor version. The segment locking is still experimental. It could have unknown bugs which potentially lead to incorrect query results. To enable segment locking, you may need to set forceTimeChunkLock to false in the task context. Once forceTimeChunkLock is unset, the task will choose a proper lock type to use automatically. Please note that segment lock is not always available. The most common use case where time chunk lock is enforced is when an overwriting task changes the segment granularity. Also, the segment locking is supported by only native indexing tasks and Kafka/Kinesis indexing tasks. Hadoop indexing tasks and index_realtime tasks (used by Tranquility) don't support it yet. forceTimeChunkLock in the task context is only applied to individual tasks. If you want to unset it for all tasks, you would want to set druid.indexer.tasklock.forceTimeChunkLock to false in the overlord configuration. Lock requests can conflict with each other if two or more tasks try to get locks for the overlapped time chunks of the same data source. Note that the lock conflict can happen between different locks types. The behavior on lock conflicts depends on the task priority. If all tasks of conflicting lock requests have the same priority, then the task who requested first will get the lock. Other tasks will wait for the task to release the lock. If a task of a lower priority asks a lock later than another of a higher priority, this task will also wait for the task of a higher priority to release the lock. If a task of a higher priority asks a lock later than another of a lower priority, then this task will preempt the other task of a lower priority. The lock of the lower-prioritized task will be revoked and the higher-prioritized task will acquire a new lock. This lock preemption can happen at any time while a task is running except when it is publishing segments in a critical section. Its locks become preemptible again once publishing segments is finished. Note that locks are shared by the tasks of the same groupId. For example, Kafka indexing tasks of the same supervisor have the same groupId and share all locks with each other. Lock priority Each task type has a different default lock priority. The below table shows the default priorities of different task types. Higher the number, higher the priority. task type default priority Realtime index task 75 Batch index task 50 Merge/Append/Compaction task 25 Other tasks 0 You can override the task priority by setting your priority in the task context as below. \"context\" : { \"priority\" : 100 } Context parameters The task context is used for various individual task configuration. The following parameters apply to all task types. property default description taskLockTimeout 300000 task lock timeout in millisecond. For more details, see Locking. forceTimeChunkLock true Setting this to false is still experimental Force to always use time chunk lock. If not set, each task automatically chooses a lock type to use. If this set, it will overwrite the druid.indexer.tasklock.forceTimeChunkLock configuration for the overlord. See Locking for more details. priority Different based on task types. See Priority. Task priority When a task acquires a lock, it sends a request via HTTP and awaits until it receives a response containing the lock acquisition result. As a result, an HTTP timeout error can occur if taskLockTimeout is greater than druid.server.http.maxIdleTime of Overlords. All task types index See Native batch ingestion (simple task). index_parallel See Native batch ingestion (parallel task). index_sub Submitted automatically, on your behalf, by an index_parallel task. index_hadoop See Hadoop-based ingestion. index_kafka Submitted automatically, on your behalf, by a Kafka-based ingestion supervisor. index_kinesis Submitted automatically, on your behalf, by a Kinesis-based ingestion supervisor. index_realtime Submitted automatically, on your behalf, by Tranquility. compact Compaction tasks merge all segments of the given interval. See the documentation on compaction for details. kill Kill tasks delete all metadata about certain segments and removes them from deep storage. See the documentation on deleting data for details. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"ingestion/faq.html":{"url":"ingestion/faq.html","title":"FAQ","keywords":"","body":" Realtime ingestion The most common cause of this is because events being ingested are out of band of Druid's windowPeriod. Druid realtime ingestion only accepts events within a configurable windowPeriod of the current time. You can verify this is what is happening by looking at the logs of your real-time process for log lines containing ingest/events/*. These metrics will indicate the events ingested, rejected, etc. We recommend using batch ingestion methods for historical data in production. Batch Ingestion If you are trying to batch load historical data but no events are being loaded, make sure the interval of your ingestion spec actually encapsulates the interval of your data. Events outside this interval are dropped. What types of data does Druid support? Druid can ingest JSON, CSV, TSV and other delimited data out of the box. Druid supports single dimension values, or multiple dimension values (an array of strings). Druid supports long, float, and double numeric columns. Not all of my events were ingested Druid will reject events outside of a window period. The best way to see if events are being rejected is to check the Druid ingest metrics. If the number of ingested events seem correct, make sure your query is correctly formed. If you included a count aggregator in your ingestion spec, you will need to query for the results of this aggregate with a longSum aggregator. Issuing a query with a count aggregator will count the number of Druid rows, which includes roll-up. Where do my Druid segments end up after ingestion? Depending on what druid.storage.type is set to, Druid will upload segments to some Deep Storage. Local disk is used as the default deep storage. My stream ingest is not handing segments off First, make sure there are no exceptions in the logs of the ingestion process. Also make sure that druid.storage.type is set to a deep storage that isn't local if you are running a distributed cluster. Other common reasons that hand-off fails are as follows: 1) Druid is unable to write to the metadata storage. Make sure your configurations are correct. 2) Historical processes are out of capacity and cannot download any more segments. You'll see exceptions in the Coordinator logs if this occurs and the Coordinator console will show the Historicals are near capacity. 3) Segments are corrupt and cannot be downloaded. You'll see exceptions in your Historical processes if this occurs. 4) Deep storage is improperly configured. Make sure that your segment actually exists in deep storage and that the Coordinator logs have no errors. How do I get HDFS to work? Make sure to include the druid-hdfs-storage and all the hadoop configuration, dependencies (that can be obtained by running command hadoop classpath on a machine where hadoop has been setup) in the classpath. And, provide necessary HDFS settings as described in deep storage . How do I know when I can make query to Druid after submitting batch ingestion task? You can verify if segments created by a recent ingestion task are loaded onto historicals and available for querying using the following workflow. Submit your ingestion task. Repeatedly poll the Overlord's tasks API ( /druid/indexer/v1/task/{taskId}/status) until your task is shown to be successfully completed. Poll the Segment Loading by Datasource API (/druid/coordinator/v1/datasources/{dataSourceName}/loadstatus) with forceMetadataRefresh=true and interval= once. (Note: forceMetadataRefresh=true refreshes Coordinator's metadata cache of all datasources. This can be a heavy operation in terms of the load on the metadata store but is necessary to make sure that we verify all the latest segments' load status) If there are segments not yet loaded, continue to step 4, otherwise you can now query the data. Repeatedly poll the Segment Loading by Datasource API (/druid/coordinator/v1/datasources/{dataSourceName}/loadstatus) with forceMetadataRefresh=false and interval=. Continue polling until all segments are loaded. Once all segments are loaded you can now query the data. Note that this workflow only guarantees that the segments are available at the time of the Segment Loading by Datasource API call. Segments can still become missing because of historical process failures or any other reasons afterward. I don't see my Druid segments on my Historical processes You can check the Coordinator console located at :. Make sure that your segments have actually loaded on Historical processes. If your segments are not present, check the Coordinator logs for messages about capacity of replication errors. One reason that segments are not downloaded is because Historical processes have maxSizes that are too small, making them incapable of downloading more data. You can change that with (for example): -Ddruid.segmentCache.locations=[{\"path\":\"/tmp/druid/storageLocation\",\"maxSize\":\"500000000000\"}] My queries are returning empty results You can use a segment metadata query for the dimensions and metrics that have been created for your datasource. Make sure that the name of the aggregators you use in your query match one of these metrics. Also make sure that the query interval you specify match a valid time range where data exists. How can I Reindex existing data in Druid with schema changes? You can use DruidInputSource with the Parallel task to ingest existing druid segments using a new schema and change the name, dimensions, metrics, rollup, etc. of the segment. See DruidInputSource for more details. Or, if you use hadoop based ingestion, then you can use \"dataSource\" input spec to do reindexing. See the Update existing data section of the data management page for more details. How can I change the granularity of existing data in Druid? In a lot of situations you may want to lower the granularity of older data. Example, any data older than 1 month has only hour level granularity but newer data has minute level granularity. This use case is same as re-indexing. To do this use the DruidInputSource and run a Parallel task. The DruidInputSource will allow you to take in existing segments from Druid and aggregate them and feed them back into Druid. It will also allow you to filter the data in those segments while feeding it back in. This means if there are rows you want to delete, you can just filter them away during re-ingestion. Typically the above will be run as a batch job to say everyday feed in a chunk of data and aggregate it. Or, if you use hadoop based ingestion, then you can use \"dataSource\" input spec to do reindexing. See the Update existing data section of the data management page for more details. Real-time ingestion seems to be stuck There are a few ways this can occur. Druid will throttle ingestion to prevent out of memory problems if the intermediate persists are taking too long or if hand-off is taking too long. If your process logs indicate certain columns are taking a very long time to build (for example, if your segment granularity is hourly, but creating a single column takes 30 minutes), you should re-evaluate your configuration or scale up your real-time ingestion. More information Getting data into Druid can definitely be difficult for first time users. Please don't hesitate to ask questions in our IRC channel or on our google groups page. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/sql.html":{"url":"querying/sql.html","title":"通过SQL查询","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the SQL language. Druid SQL is a built-in SQL layer and an alternative to Druid's native JSON-based query language, and is powered by a parser and planner based on Apache Calcite. Druid SQL translates SQL into native Druid queries on the query Broker (the first process you query), which are then passed down to data processes as native Druid queries. Other than the (slight) overhead of translating SQL on the Broker, there isn't an additional performance penalty versus native queries. Query syntax Druid SQL supports SELECT queries with the following structure: [ EXPLAIN PLAN FOR ] [ WITH tableName [ ( column1, column2, ... ) ] AS ( query ) ] SELECT [ ALL | DISTINCT ] { * | exprs } FROM { | () | [ INNER | LEFT ] JOIN ON condition } [ WHERE expr ] [ GROUP BY [ exprs | GROUPING SETS ( (exprs), ... ) | ROLLUP (exprs) | CUBE (exprs) ] ] [ HAVING expr ] [ ORDER BY expr [ ASC | DESC ], expr [ ASC | DESC ], ... ] [ LIMIT limit ] [ OFFSET offset ] [ UNION ALL ] FROM The FROM clause can refer to any of the following: Table datasources from the druid schema. This is the default schema, so Druid table datasources can be referenced as either druid.dataSourceName or simply dataSourceName. Lookups from the lookup schema, for example lookup.countries. Note that lookups can also be queried using the LOOKUP function. Subqueries. Joins between anything in this list, except between native datasources (table, lookup, query) and system tables. The join condition must be an equality between expressions from the left- and right-hand side of the join. Metadata tables from the INFORMATION_SCHEMA or sys schemas. Unlike the other options for the FROM clause, metadata tables are not considered datasources. They exist only in the SQL layer. For more information about table, lookup, query, and join datasources, refer to the Datasources documentation. WHERE The WHERE clause refers to columns in the FROM table, and will be translated to native filters. The WHERE clause can also reference a subquery, like WHERE col1 IN (SELECT foo FROM ...). Queries like this are executed as a join on the subquery, described below in the Query translation section. GROUP BY The GROUP BY clause refers to columns in the FROM table. Using GROUP BY, DISTINCT, or any aggregation functions will trigger an aggregation query using one of Druid's three native aggregation query types. GROUP BY can refer to an expression or a select clause ordinal position (like GROUP BY 2 to group by the second selected column). The GROUP BY clause can also refer to multiple grouping sets in three ways. The most flexible is GROUP BY GROUPING SETS, for example GROUP BY GROUPING SETS ( (country, city), () ). This example is equivalent to a GROUP BY country, city followed by GROUP BY () (a grand total). With GROUPING SETS, the underlying data is only scanned one time, leading to better efficiency. Second, GROUP BY ROLLUP computes a grouping set for each level of the grouping expressions. For example GROUP BY ROLLUP (country, city) is equivalent to GROUP BY GROUPING SETS ( (country, city), (country), () ) and will produce grouped rows for each country / city pair, along with subtotals for each country, along with a grand total. Finally, GROUP BY CUBE computes a grouping set for each combination of grouping expressions. For example, GROUP BY CUBE (country, city) is equivalent to GROUP BY GROUPING SETS ( (country, city), (country), (city), () ). Grouping columns that do not apply to a particular row will contain NULL. For example, when computing GROUP BY GROUPING SETS ( (country, city), () ), the grand total row corresponding to () will have NULL for the \"country\" and \"city\" columns. Column may also be NULL if it was NULL in the data itself. To differentiate such rows, you can use GROUPING aggregation. When using GROUP BY GROUPING SETS, GROUP BY ROLLUP, or GROUP BY CUBE, be aware that results may not be generated in the order that you specify your grouping sets in the query. If you need results to be generated in a particular order, use the ORDER BY clause. HAVING The HAVING clause refers to columns that are present after execution of GROUP BY. It can be used to filter on either grouping expressions or aggregated values. It can only be used together with GROUP BY. ORDER BY The ORDER BY clause refers to columns that are present after execution of GROUP BY. It can be used to order the results based on either grouping expressions or aggregated values. ORDER BY can refer to an expression or a select clause ordinal position (like ORDER BY 2 to order by the second selected column). For non-aggregation queries, ORDER BY can only order by the __time column. For aggregation queries, ORDER BY can order by any column. LIMIT The LIMIT clause limits the number of rows returned. In some situations Druid will push down this limit to data servers, which boosts performance. Limits are always pushed down for queries that run with the native Scan or TopN query types. With the native GroupBy query type, it is pushed down when ordering on a column that you are grouping by. If you notice that adding a limit doesn't change performance very much, then it's possible that Druid wasn't able to push down the limit for your query. OFFSET The OFFSET clause skips a certain number of rows when returning results. If both LIMIT and OFFSET are provided, then OFFSET will be applied first, followed by LIMIT. For example, using LIMIT 100 OFFSET 10 will return 100 rows, starting from row number 10. Together, LIMIT and OFFSET can be used to implement pagination. However, note that if the underlying datasource is modified between page fetches, then the different pages will not necessarily align with each other. There are two important factors that can affect the performance of queries that use OFFSET: Skipped rows still need to be generated internally and then discarded, meaning that raising offsets to high values can cause queries to use additional resources. OFFSET is only supported by the Scan and GroupBy native query types. Therefore, a query with OFFSET will use one of those two types, even if it might otherwise have run as a Timeseries or TopN. Switching query engines in this way can affect performance. UNION ALL The \"UNION ALL\" operator fuses multiple queries together. Druid SQL supports the UNION ALL operator in two situations: top-level and table-level. Queries that use UNION ALL in any other way will not be able to execute. Top-level UNION ALL can be used at the very top outer layer of a SQL query (not in a subquery, and not in the FROM clause). In this case, the underlying queries will be run separately, back to back, and their results will all be returned in one result set. For example: SELECT COUNT(*) FROM tbl WHERE my_column = 'value1' UNION ALL SELECT COUNT(*) FROM tbl WHERE my_column = 'value2' When UNION ALL occurs at the top level of a query like this, the results from the unioned queries are concatenated together and appear one after the other. Table-level UNION ALL can be used to query multiple tables at the same time. In this case, it must appear in a subquery in the FROM clause, and the lower-level subqueries that are inputs to the UNION ALL operator must be simple table SELECTs (no expressions, column aliasing, etc). The query will run natively using a union datasource. The same columns must be selected from each table in the same order, and those columns must either have the same types, or types that can be implicitly cast to each other (such as different numeric types). For this reason, it is generally more robust to write your queries to select specific columns. If you use SELECT *, you will need to modify your queries if a new column is added to one of the tables but not to the others. For example: SELECT col1, COUNT(*) FROM ( SELECT col1, col2, col3 FROM tbl1 UNION ALL SELECT col1, col2, col3 FROM tbl2 ) GROUP BY col1 When UNION ALL occurs at the table level, the rows from the unioned tables are not guaranteed to be processed in any particular order. They may be processed in an interleaved fashion. If you need a particular result ordering, use ORDER BY on the outer query. EXPLAIN PLAN Add \"EXPLAIN PLAN FOR\" to the beginning of any query to get information about how it will be translated. In this case, the query will not actually be executed. Refer to the Query translation documentation for help interpreting EXPLAIN PLAN output. Identifiers and literals Identifiers like datasource and column names can optionally be quoted using double quotes. To escape a double quote inside an identifier, use another double quote, like \"My \"\"very own\"\" identifier\". All identifiers are case-sensitive and no implicit case conversions are performed. Literal strings should be quoted with single quotes, like 'foo'. Literal strings with Unicode escapes can be written like U&'fo\\00F6', where character codes in hex are prefixed by a backslash. Literal numbers can be written in forms like 100 (denoting an integer), 100.0 (denoting a floating point value), or 1.0e5 (scientific notation). Literal timestamps can be written like TIMESTAMP '2000-01-01 00:00:00'. Literal intervals, used for time arithmetic, can be written like INTERVAL '1' HOUR, INTERVAL '1 02:03' DAY TO MINUTE, INTERVAL '1-2' YEAR TO MONTH, and so on. Dynamic parameters Druid SQL supports dynamic parameters using question mark (?) syntax, where parameters are bound to ? placeholders at execution time. To use dynamic parameters, replace any literal in the query with a ? character and provide a corresponding parameter value when you execute the query. Parameters are bound to the placeholders in the order in which they are passed. Parameters are supported in both the HTTP POST and JDBC APIs. Data types Standard types Druid natively supports five basic column types: \"long\" (64 bit signed int), \"float\" (32 bit float), \"double\" (64 bit float) \"string\" (UTF-8 encoded strings and string arrays), and \"complex\" (catch-all for more exotic data types like hyperUnique and approxHistogram columns). Timestamps (including the __time column) are treated by Druid as longs, with the value being the number of milliseconds since 1970-01-01 00:00:00 UTC, not counting leap seconds. Therefore, timestamps in Druid do not carry any timezone information, but only carry information about the exact moment in time they represent. See the Time functions section for more information about timestamp handling. The following table describes how Druid maps SQL types onto native types at query runtime. Casts between two SQL types that have the same Druid runtime type will have no effect, other than exceptions noted in the table. Casts between two SQL types that have different Druid runtime types will generate a runtime cast in Druid. If a value cannot be properly cast to another value, as in CAST('foo' AS BIGINT), the runtime will substitute a default value. NULL values cast to non-nullable types will also be substituted with a default value (for example, nulls cast to numbers will be converted to zeroes). SQL type Druid runtime type Default value Notes CHAR STRING '' VARCHAR STRING '' Druid STRING columns are reported as VARCHAR. Can include multi-value strings as well. DECIMAL DOUBLE 0.0 DECIMAL uses floating point, not fixed point math FLOAT FLOAT 0.0 Druid FLOAT columns are reported as FLOAT REAL DOUBLE 0.0 DOUBLE DOUBLE 0.0 Druid DOUBLE columns are reported as DOUBLE BOOLEAN LONG false TINYINT LONG 0 SMALLINT LONG 0 INTEGER LONG 0 BIGINT LONG 0 Druid LONG columns (except __time) are reported as BIGINT TIMESTAMP LONG 0, meaning 1970-01-01 00:00:00 UTC Druid's __time column is reported as TIMESTAMP. Casts between string and timestamp types assume standard SQL formatting, e.g. 2000-01-02 03:04:05, not ISO8601 formatting. For handling other formats, use one of the time functions DATE LONG 0, meaning 1970-01-01 Casting TIMESTAMP to DATE rounds down the timestamp to the nearest day. Casts between string and date types assume standard SQL formatting, e.g. 2000-01-02. For handling other formats, use one of the time functions OTHER COMPLEX none May represent various Druid column types such as hyperUnique, approxHistogram, etc Multi-value strings Druid's native type system allows strings to potentially have multiple values. These multi-value string dimensions will be reported in SQL as VARCHAR typed, and can be syntactically used like any other VARCHAR. Regular string functions that refer to multi-value string dimensions will be applied to all values for each row individually. Multi-value string dimensions can also be treated as arrays via special multi-value string functions, which can perform powerful array-aware operations. Grouping by a multi-value expression will observe the native Druid multi-value aggregation behavior, which is similar to the UNNEST functionality available in some other SQL dialects. Refer to the documentation on multi-value string dimensions for additional details. Because multi-value dimensions are treated by the SQL planner as VARCHAR, there are some inconsistencies between how they are handled in Druid SQL and in native queries. For example, expressions involving multi-value dimensions may be incorrectly optimized by the Druid SQL planner: multi_val_dim = 'a' AND multi_val_dim = 'b' will be optimized to false, even though it is possible for a single row to have both \"a\" and \"b\" as values for multi_val_dim. The SQL behavior of multi-value dimensions will change in a future release to more closely align with their behavior in native queries. NULL values The druid.generic.useDefaultValueForNull runtime property controls Druid's NULL handling mode. In the default mode (true), Druid treats NULLs and empty strings interchangeably, rather than according to the SQL standard. In this mode Druid SQL only has partial support for NULLs. For example, the expressions col IS NULL and col = '' are equivalent, and both will evaluate to true if col contains an empty string. Similarly, the expression COALESCE(col1, col2) will return col2 if col1 is an empty string. While the COUNT(*) aggregator counts all rows, the COUNT(expr) aggregator will count the number of rows where expr is neither null nor the empty string. Numeric columns in this mode are not nullable; any null or missing values will be treated as zeroes. In SQL compatible mode (false), NULLs are treated more closely to the SQL standard. The property affects both storage and querying, so for best behavior, it should be set at both ingestion time and query time. There is some overhead associated with the ability to handle NULLs; see the segment internals documentation for more details. Aggregation functions Aggregation functions can appear in the SELECT clause of any query. Any aggregator can be filtered using syntax like AGG(expr) FILTER(WHERE whereExpr). Filtered aggregators will only aggregate rows that match their filter. It's possible for two aggregators in the same SQL query to have different filters. Only the COUNT aggregation can accept DISTINCT. The order of aggregation operations across segments is not deterministic. This means that non-commutative aggregation functions can produce inconsistent results across the same query. Functions that operate on an input type of \"float\" or \"double\" may also see these differences in aggregation results across multiple query runs because of this. If precisely the same value is desired across multiple query runs, consider using the ROUND function to smooth out the inconsistencies between queries. Function Notes COUNT(*) Counts the number of rows. COUNT(DISTINCT expr) Counts distinct values of expr, which can be string, numeric, or hyperUnique. By default this is approximate, using a variant of HyperLogLog. To get exact counts set \"useApproximateCountDistinct\" to \"false\". If you do this, expr must be string or numeric, since exact counts are not possible using hyperUnique columns. See also APPROX_COUNT_DISTINCT(expr). In exact mode, only one distinct count per query is permitted. SUM(expr) Sums numbers. MIN(expr) Takes the minimum of numbers. MAX(expr) Takes the maximum of numbers. AVG(expr) Averages numbers. APPROX_COUNT_DISTINCT(expr) Counts distinct values of expr, which can be a regular column or a hyperUnique column. This is always approximate, regardless of the value of \"useApproximateCountDistinct\". This uses Druid's built-in \"cardinality\" or \"hyperUnique\" aggregators. See also COUNT(DISTINCT expr). APPROX_COUNT_DISTINCT_DS_HLL(expr, [lgK, tgtHllType]) Counts distinct values of expr, which can be a regular column or an HLL sketch column. The lgK and tgtHllType parameters are described in the HLL sketch documentation. This is always approximate, regardless of the value of \"useApproximateCountDistinct\". See also COUNT(DISTINCT expr). The DataSketches extension must be loaded to use this function. APPROX_COUNT_DISTINCT_DS_THETA(expr, [size]) Counts distinct values of expr, which can be a regular column or a Theta sketch column. The size parameter is described in the Theta sketch documentation. This is always approximate, regardless of the value of \"useApproximateCountDistinct\". See also COUNT(DISTINCT expr). The DataSketches extension must be loaded to use this function. DS_HLL(expr, [lgK, tgtHllType]) Creates an HLL sketch on the values of expr, which can be a regular column or a column containing HLL sketches. The lgK and tgtHllType parameters are described in the HLL sketch documentation. The DataSketches extension must be loaded to use this function. DS_THETA(expr, [size]) Creates a Theta sketch on the values of expr, which can be a regular column or a column containing Theta sketches. The size parameter is described in the Theta sketch documentation. The DataSketches extension must be loaded to use this function. APPROX_QUANTILE(expr, probability, [resolution]) Computes approximate quantiles on numeric or approxHistogram exprs. The \"probability\" should be between 0 and 1 (exclusive). The \"resolution\" is the number of centroids to use for the computation. Higher resolutions will give more precise results but also have higher overhead. If not provided, the default resolution is 50. The approximate histogram extension must be loaded to use this function. APPROX_QUANTILE_DS(expr, probability, [k]) Computes approximate quantiles on numeric or Quantiles sketch exprs. The \"probability\" should be between 0 and 1 (exclusive). The k parameter is described in the Quantiles sketch documentation. The DataSketches extension must be loaded to use this function. APPROX_QUANTILE_FIXED_BUCKETS(expr, probability, numBuckets, lowerLimit, upperLimit, [outlierHandlingMode]) Computes approximate quantiles on numeric or fixed buckets histogram exprs. The \"probability\" should be between 0 and 1 (exclusive). The numBuckets, lowerLimit, upperLimit, and outlierHandlingMode parameters are described in the fixed buckets histogram documentation. The approximate histogram extension must be loaded to use this function. DS_QUANTILES_SKETCH(expr, [k]) Creates a Quantiles sketch on the values of expr, which can be a regular column or a column containing quantiles sketches. The k parameter is described in the Quantiles sketch documentation. The DataSketches extension must be loaded to use this function. BLOOM_FILTER(expr, numEntries) Computes a bloom filter from values produced by expr, with numEntries maximum number of distinct values before false positive rate increases. See bloom filter extension documentation for additional details. TDIGEST_QUANTILE(expr, quantileFraction, [compression]) Builds a T-Digest sketch on values produced by expr and returns the value for the quantile. Compression parameter (default value 100) determines the accuracy and size of the sketch. Higher compression means higher accuracy but more space to store sketches. See t-digest extension documentation for additional details. TDIGEST_GENERATE_SKETCH(expr, [compression]) Builds a T-Digest sketch on values produced by expr. Compression parameter (default value 100) determines the accuracy and size of the sketch Higher compression means higher accuracy but more space to store sketches. See t-digest extension documentation for additional details. VAR_POP(expr) Computes variance population of expr. See stats extension documentation for additional details. VAR_SAMP(expr) Computes variance sample of expr. See stats extension documentation for additional details. VARIANCE(expr) Computes variance sample of expr. See stats extension documentation for additional details. STDDEV_POP(expr) Computes standard deviation population of expr. See stats extension documentation for additional details. STDDEV_SAMP(expr) Computes standard deviation sample of expr. See stats extension documentation for additional details. STDDEV(expr) Computes standard deviation sample of expr. See stats extension documentation for additional details. EARLIEST(expr) Returns the earliest value of expr, which must be numeric. If expr comes from a relation with a timestamp column (like a Druid datasource) then \"earliest\" is the value first encountered with the minimum overall timestamp of all values being aggregated. If expr does not come from a relation with a timestamp, then it is simply the first value encountered. EARLIEST(expr, maxBytesPerString) Like EARLIEST(expr), but for strings. The maxBytesPerString parameter determines how much aggregation space to allocate per string. Strings longer than this limit will be truncated. This parameter should be set as low as possible, since high values will lead to wasted memory. LATEST(expr) Returns the latest value of expr, which must be numeric. If expr comes from a relation with a timestamp column (like a Druid datasource) then \"latest\" is the value last encountered with the maximum overall timestamp of all values being aggregated. If expr does not come from a relation with a timestamp, then it is simply the last value encountered. LATEST(expr, maxBytesPerString) Like LATEST(expr), but for strings. The maxBytesPerString parameter determines how much aggregation space to allocate per string. Strings longer than this limit will be truncated. This parameter should be set as low as possible, since high values will lead to wasted memory. ANY_VALUE(expr) Returns any value of expr including null. expr must be numeric. This aggregator can simplify and optimize the performance by returning the first encountered value (including null) ANY_VALUE(expr, maxBytesPerString) Like ANY_VALUE(expr), but for strings. The maxBytesPerString parameter determines how much aggregation space to allocate per string. Strings longer than this limit will be truncated. This parameter should be set as low as possible, since high values will lead to wasted memory. GROUPING(expr, expr...) Returns a number to indicate which groupBy dimension is included in a row, when using GROUPING SETS. Refer to additional documentation on how to infer this number. For advice on choosing approximate aggregation functions, check out our approximate aggregations documentation. Scalar functions Numeric functions For mathematical operations, Druid SQL will use integer math if all operands involved in an expression are integers. Otherwise, Druid will switch to floating point math. You can force this to happen by casting one of your operands to FLOAT. At runtime, Druid will widen 32-bit floats to 64-bit for most expressions. Function Notes ABS(expr) Absolute value. CEIL(expr) Ceiling. EXP(expr) e to the power of expr. FLOOR(expr) Floor. LN(expr) Logarithm (base e). LOG10(expr) Logarithm (base 10). POWER(expr, power) expr to a power. SQRT(expr) Square root. TRUNCATE(expr[, digits]) Truncate expr to a specific number of decimal digits. If digits is negative, then this truncates that many places to the left of the decimal point. Digits defaults to zero if not specified. TRUNC(expr[, digits]) Synonym for TRUNCATE. ROUND(expr[, digits]) ROUND(x, y) would return the value of the x rounded to the y decimal places. While x can be an integer or floating-point number, y must be an integer. The type of the return value is specified by that of x. y defaults to 0 if omitted. When y is negative, x is rounded on the left side of the y decimal points. If expr evaluates to either NaN, expr will be converted to 0. If expr is infinity, expr will be converted to the nearest finite double. x + y Addition. x - y Subtraction. x * y Multiplication. x / y Division. MOD(x, y) Modulo (remainder of x divided by y). SIN(expr) Trigonometric sine of an angle expr. COS(expr) Trigonometric cosine of an angle expr. TAN(expr) Trigonometric tangent of an angle expr. COT(expr) Trigonometric cotangent of an angle expr. ASIN(expr) Arc sine of expr. ACOS(expr) Arc cosine of expr. ATAN(expr) Arc tangent of expr. ATAN2(y, x) Angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta). DEGREES(expr) Converts an angle measured in radians to an approximately equivalent angle measured in degrees RADIANS(expr) Converts an angle measured in degrees to an approximately equivalent angle measured in radians String functions String functions accept strings, and return a type appropriate to the function. Function Notes x || y Concat strings x and y. CONCAT(expr, expr...) Concats a list of expressions. TEXTCAT(expr, expr) Two argument version of CONCAT. STRING_FORMAT(pattern[, args...]) Returns a string formatted in the manner of Java's String.format. LENGTH(expr) Length of expr in UTF-16 code units. CHAR_LENGTH(expr) Synonym for LENGTH. CHARACTER_LENGTH(expr) Synonym for LENGTH. STRLEN(expr) Synonym for LENGTH. LOOKUP(expr, lookupName) Look up expr in a registered query-time lookup table. Note that lookups can also be queried directly using the lookup schema. LOWER(expr) Returns expr in all lowercase. PARSE_LONG(string[, radix]) Parses a string into a long (BIGINT) with the given radix, or 10 (decimal) if a radix is not provided. POSITION(needle IN haystack [FROM fromIndex]) Returns the index of needle within haystack, with indexes starting from 1. The search will begin at fromIndex, or 1 if fromIndex is not specified. If the needle is not found, returns 0. REGEXP_EXTRACT(expr, pattern, [index]) Apply regular expression pattern to expr and extract a capture group, or NULL if there is no match. If index is unspecified or zero, returns the first substring that matched the pattern. The pattern may match anywhere inside expr; if you want to match the entire string instead, use the ^ and $ markers at the start and end of your pattern. Note: when druid.generic.useDefaultValueForNull = true, it is not possible to differentiate an empty-string match from a non-match (both will return NULL). REGEXP_LIKE(expr, pattern) Returns whether expr matches regular expression pattern. The pattern may match anywhere inside expr; if you want to match the entire string instead, use the ^ and $ markers at the start and end of your pattern. Similar to LIKE, but uses regexps instead of LIKE patterns. Especially useful in WHERE clauses. CONTAINS_STRING(, str) Returns true if the str is a substring of expr. ICONTAINS_STRING(, str) Returns true if the str is a substring of expr. The match is case-insensitive. REPLACE(expr, pattern, replacement) Replaces pattern with replacement in expr, and returns the result. STRPOS(haystack, needle) Returns the index of needle within haystack, with indexes starting from 1. If the needle is not found, returns 0. SUBSTRING(expr, index, [length]) Returns a substring of expr starting at index, with a max length, both measured in UTF-16 code units. RIGHT(expr, [length]) Returns the rightmost length characters from expr. LEFT(expr, [length]) Returns the leftmost length characters from expr. SUBSTR(expr, index, [length]) Synonym for SUBSTRING. TRIM([BOTH | LEADING | TRAILING] [ FROM] expr) Returns expr with characters removed from the leading, trailing, or both ends of \"expr\" if they are in \"chars\". If \"chars\" is not provided, it defaults to \" \" (a space). If the directional argument is not provided, it defaults to \"BOTH\". BTRIM(expr[, chars]) Alternate form of TRIM(BOTH FROM ). LTRIM(expr[, chars]) Alternate form of TRIM(LEADING FROM ). RTRIM(expr[, chars]) Alternate form of TRIM(TRAILING FROM ). UPPER(expr) Returns expr in all uppercase. REVERSE(expr) Reverses expr. REPEAT(expr, [N]) Repeats expr N times LPAD(expr, length[, chars]) Returns a string of length from expr left-padded with chars. If length is shorter than the length of expr, the result is expr which is truncated to length. The result will be null if either expr or chars is null. If chars is an empty string, no padding is added, however expr may be trimmed if necessary. RPAD(expr, length[, chars]) Returns a string of length from expr right-padded with chars. If length is shorter than the length of expr, the result is expr which is truncated to length. The result will be null if either expr or chars is null. If chars is an empty string, no padding is added, however expr may be trimmed if necessary. Time functions Time functions can be used with Druid's __time column, with any column storing millisecond timestamps through use of the MILLIS_TO_TIMESTAMP function, or with any column storing string timestamps through use of the TIME_PARSE function. By default, time operations use the UTC time zone. You can change the time zone by setting the connection context parameter \"sqlTimeZone\" to the name of another time zone, like \"America/Los_Angeles\", or to an offset like \"-08:00\". If you need to mix multiple time zones in the same query, or if you need to use a time zone other than the connection time zone, some functions also accept time zones as parameters. These parameters always take precedence over the connection time zone. Literal timestamps in the connection time zone can be written using TIMESTAMP '2000-01-01 00:00:00' syntax. The simplest way to write literal timestamps in other time zones is to use TIME_PARSE, like TIME_PARSE('2000-02-01 00:00:00', NULL, 'America/Los_Angeles'). Function Notes CURRENT_TIMESTAMP Current timestamp in the connection's time zone. CURRENT_DATE Current date in the connection's time zone. DATE_TRUNC(, ) Rounds down a timestamp, returning it as a new timestamp. Unit can be 'milliseconds', 'second', 'minute', 'hour', 'day', 'week', 'month', 'quarter', 'year', 'decade', 'century', or 'millennium'. TIME_CEIL(, , [, []]) Rounds up a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". This function is similar to CEIL but is more flexible. TIME_FLOOR(, , [, []]) Rounds down a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". This function is similar to FLOOR but is more flexible. TIME_SHIFT(, , , []) Shifts a timestamp by a period (step times), returning it as a new timestamp. Period can be any ISO8601 period. Step may be negative. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". TIME_EXTRACT(, [, []]) Extracts a time part from expr, returning it as a number. Unit can be EPOCH, SECOND, MINUTE, HOUR, DAY (day of month), DOW (day of week), DOY (day of year), WEEK (week of week year), MONTH (1 through 12), QUARTER (1 through 4), or YEAR. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". This function is similar to EXTRACT but is more flexible. Unit and time zone must be literals, and must be provided quoted, like TIME_EXTRACT(__time, 'HOUR') or TIME_EXTRACT(__time, 'HOUR', 'America/Los_Angeles'). TIME_PARSE(, [, []]) Parses a string into a timestamp using a given Joda DateTimeFormat pattern, or ISO8601 (e.g. 2000-01-02T03:04:05Z) if the pattern is not provided. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\", and will be used as the time zone for strings that do not include a time zone offset. Pattern and time zone must be literals. Strings that cannot be parsed as timestamps will be returned as NULL. TIME_FORMAT(, [, []]) Formats a timestamp as a string with a given Joda DateTimeFormat pattern, or ISO8601 (e.g. 2000-01-02T03:04:05Z) if the pattern is not provided. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". Pattern and time zone must be literals. MILLIS_TO_TIMESTAMP(millis_expr) Converts a number of milliseconds since the epoch into a timestamp. TIMESTAMP_TO_MILLIS(timestamp_expr) Converts a timestamp into a number of milliseconds since the epoch. EXTRACT( FROM timestamp_expr) Extracts a time part from expr, returning it as a number. Unit can be EPOCH, MICROSECOND, MILLISECOND, SECOND, MINUTE, HOUR, DAY (day of month), DOW (day of week), ISODOW (ISO day of week), DOY (day of year), WEEK (week of year), MONTH, QUARTER, YEAR, ISOYEAR, DECADE, CENTURY or MILLENNIUM. Units must be provided unquoted, like EXTRACT(HOUR FROM __time). FLOOR(timestamp_expr TO ) Rounds down a timestamp, returning it as a new timestamp. Unit can be SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR. CEIL(timestamp_expr TO ) Rounds up a timestamp, returning it as a new timestamp. Unit can be SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR. TIMESTAMPADD(, , ) Equivalent to timestamp + count * INTERVAL '1' UNIT. TIMESTAMPDIFF(, , ) Returns the (signed) number of unit between timestamp1 and timestamp2. Unit can be SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR. timestamp_expr { + | - } Add or subtract an amount of time from a timestamp. interval_expr can include interval literals like INTERVAL '2' HOUR, and may include interval arithmetic as well. This operator treats days as uniformly 86400 seconds long, and does not take into account daylight savings time. To account for daylight savings time, use TIME_SHIFT instead. Reduction functions Reduction functions operate on zero or more expressions and return a single expression. If no expressions are passed as arguments, then the result is NULL. The expressions must all be convertible to a common data type, which will be the type of the result: If all argument are NULL, the result is NULL. Otherwise, NULL arguments are ignored. If the arguments comprise a mix of numbers and strings, the arguments are interpreted as strings. If all arguments are integer numbers, the arguments are interpreted as longs. If all arguments are numbers and at least one argument is a double, the arguments are interpreted as doubles. Function Notes GREATEST([expr1, ...]) Evaluates zero or more expressions and returns the maximum value based on comparisons as described above. LEAST([expr1, ...]) Evaluates zero or more expressions and returns the minimum value based on comparisons as described above. IP address functions For the IPv4 address functions, the address argument can either be an IPv4 dotted-decimal string (e.g., '192.168.0.1') or an IP address represented as an integer (e.g., 3232235521). The subnet argument should be a string formatted as an IPv4 address subnet in CIDR notation (e.g., '192.168.0.0/16'). Function Notes IPV4_MATCH(address, subnet) Returns true if the address belongs to the subnet literal, else false. If address is not a valid IPv4 address, then false is returned. This function is more efficient if address is an integer instead of a string. IPV4_PARSE(address) Parses address into an IPv4 address stored as an integer . If address is an integer that is a valid IPv4 address, then it is passed through. Returns null if address cannot be represented as an IPv4 address. IPV4_STRINGIFY(address) Converts address into an IPv4 address dotted-decimal string. If address is a string that is a valid IPv4 address, then it is passed through. Returns null if address cannot be represented as an IPv4 address. Comparison operators Function Notes x = y Equals. x <> y Not-equals. x > y Greater than. x >= y Greater than or equal to. x Less than. x Less than or equal to. x BETWEEN y AND z Equivalent to x >= y AND x . x NOT BETWEEN y AND z Equivalent to x z. x LIKE pattern [ESCAPE esc] True if x matches a SQL LIKE pattern (with an optional escape). x NOT LIKE pattern [ESCAPE esc] True if x does not match a SQL LIKE pattern (with an optional escape). x IS NULL True if x is NULL or empty string. x IS NOT NULL True if x is neither NULL nor empty string. x IS TRUE True if x is true. x IS NOT TRUE True if x is not true. x IS FALSE True if x is false. x IS NOT FALSE True if x is not false. x IN (values) True if x is one of the listed values. x NOT IN (values) True if x is not one of the listed values. x IN (subquery) True if x is returned by the subquery. This will be translated into a join; see Query translation for details. x NOT IN (subquery) True if x is not returned by the subquery. This will be translated into a join; see Query translation for details. x AND y Boolean AND. x OR y Boolean OR. NOT x Boolean NOT. Sketch functions These functions operate on expressions or columns that return sketch objects. HLL sketch functions The following functions operate on DataSketches HLL sketches. The DataSketches extension must be loaded to use the following functions. Function Notes HLL_SKETCH_ESTIMATE(expr, [round]) Returns the distinct count estimate from an HLL sketch. expr must return an HLL sketch. The optional round boolean parameter will round the estimate if set to true, with a default of false. HLL_SKETCH_ESTIMATE_WITH_ERROR_BOUNDS(expr, [numStdDev]) Returns the distinct count estimate and error bounds from an HLL sketch. expr must return an HLL sketch. An optional numStdDev argument can be provided. HLL_SKETCH_UNION([lgK, tgtHllType], expr0, expr1, ...) Returns a union of HLL sketches, where each input expression must return an HLL sketch. The lgK and tgtHllType can be optionally specified as the first parameter; if provided, both optional parameters must be specified. HLL_SKETCH_TO_STRING(expr) Returns a human-readable string representation of an HLL sketch for debugging. expr must return an HLL sketch. Theta sketch functions The following functions operate on theta sketches. The DataSketches extension must be loaded to use the following functions. Function Notes THETA_SKETCH_ESTIMATE(expr) Returns the distinct count estimate from a theta sketch. expr must return a theta sketch. THETA_SKETCH_ESTIMATE_WITH_ERROR_BOUNDS(expr, errorBoundsStdDev) Returns the distinct count estimate and error bounds from a theta sketch. expr must return a theta sketch. THETA_SKETCH_UNION([size], expr0, expr1, ...) Returns a union of theta sketches, where each input expression must return a theta sketch. The size can be optionally specified as the first parameter. THETA_SKETCH_INTERSECT([size], expr0, expr1, ...) Returns an intersection of theta sketches, where each input expression must return a theta sketch. The size can be optionally specified as the first parameter. THETA_SKETCH_NOT([size], expr0, expr1, ...) Returns a set difference of theta sketches, where each input expression must return a theta sketch. The size can be optionally specified as the first parameter. Quantiles sketch functions The following functions operate on quantiles sketches. The DataSketches extension must be loaded to use the following functions. Function Notes DS_GET_QUANTILE(expr, fraction) Returns the quantile estimate corresponding to fraction from a quantiles sketch. expr must return a quantiles sketch. DS_GET_QUANTILES(expr, fraction0, fraction1, ...) Returns a string representing an array of quantile estimates corresponding to a list of fractions from a quantiles sketch. expr must return a quantiles sketch. DS_HISTOGRAM(expr, splitPoint0, splitPoint1, ...) Returns a string representing an approximation to the histogram given a list of split points that define the histogram bins from a quantiles sketch. expr must return a quantiles sketch. DS_CDF(expr, splitPoint0, splitPoint1, ...) Returns a string representing approximation to the Cumulative Distribution Function given a list of split points that define the edges of the bins from a quantiles sketch. expr must return a quantiles sketch. DS_RANK(expr, value) Returns an approximation to the rank of a given value that is the fraction of the distribution less than that value from a quantiles sketch. expr must return a quantiles sketch. DS_QUANTILE_SUMMARY(expr) Returns a string summary of a quantiles sketch, useful for debugging. expr must return a quantiles sketch. Other scalar functions Function Notes CAST(value AS TYPE) Cast value to another type. See Data types for details about how Druid SQL handles CAST. CASE expr WHEN value1 THEN result1 \\[ WHEN value2 THEN result2 ... \\] \\[ ELSE resultN \\] END Simple CASE. CASE WHEN boolean_expr1 THEN result1 \\[ WHEN boolean_expr2 THEN result2 ... \\] \\[ ELSE resultN \\] END Searched CASE. NULLIF(value1, value2) Returns NULL if value1 and value2 match, else returns value1. COALESCE(value1, value2, ...) Returns the first value that is neither NULL nor empty string. NVL(expr,expr-for-null) Returns 'expr-for-null' if 'expr' is null (or empty string for string type). BLOOM_FILTER_TEST(, ) Returns true if the value is contained in a Base64-serialized bloom filter. See the Bloom filter extension documentation for additional details. Multi-value string functions All 'array' references in the multi-value string function documentation can refer to multi-value string columns or ARRAY literals. Function Notes ARRAY(expr1,expr ...) constructs a SQL ARRAY literal from the expression arguments, using the type of the first argument as the output array type MV_LENGTH(arr) returns length of array expression MV_OFFSET(arr,long) returns the array element at the 0 based index supplied, or null for an out of range index MV_ORDINAL(arr,long) returns the array element at the 1 based index supplied, or null for an out of range index MV_CONTAINS(arr,expr) returns 1 if the array contains the element specified by expr, or contains all elements specified by expr if expr is an array, else 0 MV_OVERLAP(arr1,arr2) returns 1 if arr1 and arr2 have any elements in common, else 0 MV_OFFSET_OF(arr,expr) returns the 0 based index of the first occurrence of expr in the array, or -1 or null if druid.generic.useDefaultValueForNull=false if no matching elements exist in the array. MV_ORDINAL_OF(arr,expr) returns the 1 based index of the first occurrence of expr in the array, or -1 or null if druid.generic.useDefaultValueForNull=false if no matching elements exist in the array. MV_PREPEND(expr,arr) adds expr to arr at the beginning, the resulting array type determined by the type of the array MV_APPEND(arr1,expr) appends expr to arr, the resulting array type determined by the type of the first array MV_CONCAT(arr1,arr2) concatenates 2 arrays, the resulting array type determined by the type of the first array MV_SLICE(arr,start,end) return the subarray of arr from the 0 based index start(inclusive) to end(exclusive), or null, if start is less than 0, greater than length of arr or less than end MV_TO_STRING(arr,str) joins all elements of arr by the delimiter specified by str STRING_TO_MV(str1,str2) splits str1 into an array on the delimiter specified by str2 Query translation Druid SQL translates SQL queries to native queries before running them, and understanding how this translation works is key to getting good performance. Best practices Consider this (non-exhaustive) list of things to look out for when looking into the performance implications of how your SQL queries are translated to native queries. If you wrote a filter on the primary time column __time, make sure it is being correctly translated to an \"intervals\" filter, as described in the Time filters section below. If not, you may need to change the way you write the filter. Try to avoid subqueries underneath joins: they affect both performance and scalability. This includes implicit subqueries generated by conditions on mismatched types, and implicit subqueries generated by conditions that use expressions to refer to the right-hand side. Currently, Druid does not support pushing down predicates (condition and filter) past a Join (i.e. into Join's children). Druid only supports pushing predicates into the join if they originated from above the join. Hence, the location of predicates and filters in your Druid SQL is very important. Also, as a result of this, comma joins should be avoided. Read through the Query execution page to understand how various types of native queries will be executed. Be careful when interpreting EXPLAIN PLAN output, and use request logging if in doubt. Request logs will show the exact native query that was run. See the next section for more details. If you encounter a query that could be planned better, feel free to raise an issue on GitHub. A reproducible test case is always appreciated. Interpreting EXPLAIN PLAN output The EXPLAIN PLAN functionality can help you understand how a given SQL query will be translated to native. For simple queries that do not involve subqueries or joins, the output of EXPLAIN PLAN is easy to interpret. The native query that will run is embedded as JSON inside a \"DruidQueryRel\" line: > EXPLAIN PLAN FOR SELECT COUNT(*) FROM wikipedia DruidQueryRel(query=[{\"queryType\":\"timeseries\",\"dataSource\":\"wikipedia\",\"intervals\":\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\",\"granularity\":\"all\",\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}]}], signature=[{a0:LONG}]) For more complex queries that do involve subqueries or joins, EXPLAIN PLAN is somewhat more difficult to interpret. For example, consider this query: > EXPLAIN PLAN FOR > SELECT > channel, > COUNT(*) > FROM wikipedia > WHERE channel IN (SELECT page FROM wikipedia GROUP BY page ORDER BY COUNT(*) DESC LIMIT 10) > GROUP BY channel DruidJoinQueryRel(condition=[=($1, $3)], joinType=[inner], query=[{\"queryType\":\"groupBy\",\"dataSource\":{\"type\":\"table\",\"name\":\"__join__\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"granularity\":\"all\",\"dimensions\":[\"channel\"],\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}]}], signature=[{d0:STRING, a0:LONG}]) DruidQueryRel(query=[{\"queryType\":\"scan\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"resultFormat\":\"compactedList\",\"columns\":[\"__time\",\"channel\",\"page\"],\"granularity\":\"all\"}], signature=[{__time:LONG, channel:STRING, page:STRING}]) DruidQueryRel(query=[{\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikipedia\"},\"dimension\":\"page\",\"metric\":{\"type\":\"numeric\",\"metric\":\"a0\"},\"threshold\":10,\"intervals\":{\"type\":\"intervals\",\"intervals\":[\"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\"]},\"granularity\":\"all\",\"aggregations\":[{\"type\":\"count\",\"name\":\"a0\"}]}], signature=[{d0:STRING}]) Here, there is a join with two inputs. The way to read this is to consider each line of the EXPLAIN PLAN output as something that might become a query, or might just become a simple datasource. The query field they all have is called a \"partial query\" and represents what query would be run on the datasource represented by that line, if that line ran by itself. In some cases — like the \"scan\" query in the second line of this example — the query does not actually run, and it ends up being translated to a simple table datasource. See the Join translation section for more details about how this works. We can see this for ourselves using Druid's request logging feature. After enabling logging and running this query, we can see that it actually runs as the following native query. { \"queryType\": \"groupBy\", \"dataSource\": { \"type\": \"join\", \"left\": \"wikipedia\", \"right\": { \"type\": \"query\", \"query\": { \"queryType\": \"topN\", \"dataSource\": \"wikipedia\", \"dimension\": {\"type\": \"default\", \"dimension\": \"page\", \"outputName\": \"d0\"}, \"metric\": {\"type\": \"numeric\", \"metric\": \"a0\"}, \"threshold\": 10, \"intervals\": \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\", \"granularity\": \"all\", \"aggregations\": [ { \"type\": \"count\", \"name\": \"a0\"} ] } }, \"rightPrefix\": \"j0.\", \"condition\": \"(\\\"page\\\" == \\\"j0.d0\\\")\", \"joinType\": \"INNER\" }, \"intervals\": \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\", \"granularity\": \"all\", \"dimensions\": [ {\"type\": \"default\", \"dimension\": \"channel\", \"outputName\": \"d0\"} ], \"aggregations\": [ { \"type\": \"count\", \"name\": \"a0\"} ] } Query types Druid SQL uses four different native query types. Scan is used for queries that do not aggregate (no GROUP BY, no DISTINCT). Timeseries is used for queries that GROUP BY FLOOR(__time TO ) or TIME_FLOOR(__time, period), have no other grouping expressions, no HAVING or LIMIT clauses, no nesting, and either no ORDER BY, or an ORDER BY that orders by same expression as present in GROUP BY. It also uses Timeseries for \"grand total\" queries that have aggregation functions but no GROUP BY. This query type takes advantage of the fact that Druid segments are sorted by time. TopN is used by default for queries that group by a single expression, do have ORDER BY and LIMIT clauses, do not have HAVING clauses, and are not nested. However, the TopN query type will deliver approximate ranking and results in some cases; if you want to avoid this, set \"useApproximateTopN\" to \"false\". TopN results are always computed in memory. See the TopN documentation for more details. GroupBy is used for all other aggregations, including any nested aggregation queries. Druid's GroupBy is a traditional aggregation engine: it delivers exact results and rankings and supports a wide variety of features. GroupBy aggregates in memory if it can, but it may spill to disk if it doesn't have enough memory to complete your query. Results are streamed back from data processes through the Broker if you ORDER BY the same expressions in your GROUP BY clause, or if you don't have an ORDER BY at all. If your query has an ORDER BY referencing expressions that don't appear in the GROUP BY clause (like aggregation functions) then the Broker will materialize a list of results in memory, up to a max of your LIMIT, if any. See the GroupBy documentation for details about tuning performance and memory use. Time filters For all native query types, filters on the __time column will be translated into top-level query \"intervals\" whenever possible, which allows Druid to use its global time index to quickly prune the set of data that must be scanned. Consider this (non-exhaustive) list of time filters that will be recognized and translated to \"intervals\": __time >= TIMESTAMP '2000-01-01 00:00:00' (comparison to absolute time) __time >= CURRENT_TIMESTAMP - INTERVAL '8' HOUR (comparison to relative time) FLOOR(__time TO DAY) = TIMESTAMP '2000-01-01 00:00:00' (specific day) Refer to the Interpreting EXPLAIN PLAN output section for details on confirming that time filters are being translated as you expect. Joins SQL join operators are translated to native join datasources as follows: Joins that the native layer can handle directly are translated literally, to a join datasource whose left, right, and condition are faithful translations of the original SQL. This includes any SQL join where the right-hand side is a lookup or subquery, and where the condition is an equality where one side is an expression based on the left-hand table, the other side is a simple column reference to the right-hand table, and both sides of the equality are the same data type. If a join cannot be handled directly by a native join datasource as written, Druid SQL will insert subqueries to make it runnable. For example, foo INNER JOIN bar ON foo.abc = LOWER(bar.def) cannot be directly translated, because there is an expression on the right-hand side instead of a simple column access. A subquery will be inserted that effectively transforms this clause to foo INNER JOIN (SELECT LOWER(def) AS def FROM bar) t ON foo.abc = t.def. Druid SQL does not currently reorder joins to optimize queries. Refer to the Interpreting EXPLAIN PLAN output section for details on confirming that joins are being translated as you expect. Refer to the Query execution page for information about how joins are executed. Subqueries Subqueries in SQL are generally translated to native query datasources. Refer to the Query execution page for information about how subqueries are executed. Note: Subqueries in the WHERE clause, like WHERE col1 IN (SELECT foo FROM ...) are translated to inner joins. Approximations Druid SQL will use approximate algorithms in some situations: The COUNT(DISTINCT col) aggregation functions by default uses a variant of HyperLogLog, a fast approximate distinct counting algorithm. Druid SQL will switch to exact distinct counts if you set \"useApproximateCountDistinct\" to \"false\", either through query context or through Broker configuration. GROUP BY queries over a single column with ORDER BY and LIMIT may be executed using the TopN engine, which uses an approximate algorithm. Druid SQL will switch to an exact grouping algorithm if you set \"useApproximateTopN\" to \"false\", either through query context or through Broker configuration. Aggregation functions that are labeled as using sketches or approximations, such as APPROX_COUNT_DISTINCT, are always approximate, regardless of configuration. Unsupported features Druid does not support all SQL features. In particular, the following features are not supported. JOIN between native datasources (table, lookup, subquery) and system tables. JOIN conditions that are not an equality between expressions from the left- and right-hand sides. JOIN conditions containing a constant value inside the condition. JOIN conditions on a column which contains a multi-value dimension. OVER clauses, and analytic functions such as LAG and LEAD. ORDER BY for a non-aggregating query, except for ORDER BY __time or ORDER BY __time DESC, which are supported. This restriction only applies to non-aggregating queries; you can ORDER BY any column in an aggregating query. DDL and DML. Using Druid-specific functions like TIME_PARSE and APPROX_QUANTILE_DS on system tables. Additionally, some Druid native query features are not supported by the SQL language. Some unsupported Druid features include: Inline datasources. Spatial filters. Query cancellation. Multi-value dimensions are only partially implemented in Druid SQL. There are known inconsistencies between their behavior in SQL queries and in native queries due to how they are currently treated by the SQL planner. Client APIs HTTP POST You can make Druid SQL queries using HTTP via POST to the endpoint /druid/v2/sql/. The request should be a JSON object with a \"query\" field, like {\"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar'\"}. Request Property Description Default query SQL query string. none (required) resultFormat Format of query results. See Responses for details. \"object\" header Whether or not to include a header. See [Responses] for details. false context JSON object containing connection context parameters. {} (empty) parameters List of query parameters for parameterized queries. Each parameter in the list should be a JSON object like {\"type\": \"VARCHAR\", \"value\": \"foo\"}. The type should be a SQL type; see Data types for a list of supported SQL types. [] (empty) You can use curl to send SQL queries from the command-line: $ cat query.json {\"query\":\"SELECT COUNT(*) AS TheCount FROM data_source\"} $ curl -XPOST -H'Content-Type: application/json' http://BROKER:8082/druid/v2/sql/ -d @query.json [{\"TheCount\":24433}] There are a variety of connection context parameters you can provide by adding a \"context\" map, like: { \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\", \"context\" : { \"sqlTimeZone\" : \"America/Los_Angeles\" } } Parameterized SQL queries are also supported: { \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = ? AND __time > ?\", \"parameters\": [ { \"type\": \"VARCHAR\", \"value\": \"bar\"}, { \"type\": \"TIMESTAMP\", \"value\": \"2000-01-01 00:00:00\" } ] } Metadata is available over HTTP POST by querying metadata tables. Responses Druid SQL's HTTP POST API supports a variety of result formats. You can specify these by adding a \"resultFormat\" parameter, like: { \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\", \"resultFormat\" : \"object\" } The supported result formats are: Format Description Content-Type object The default, a JSON array of JSON objects. Each object's field names match the columns returned by the SQL query, and are provided in the same order as the SQL query. application/json array JSON array of JSON arrays. Each inner array has elements matching the columns returned by the SQL query, in order. application/json objectLines Like \"object\", but the JSON objects are separated by newlines instead of being wrapped in a JSON array. This can make it easier to parse the entire response set as a stream, if you do not have ready access to a streaming JSON parser. To make it possible to detect a truncated response, this format includes a trailer of one blank line. text/plain arrayLines Like \"array\", but the JSON arrays are separated by newlines instead of being wrapped in a JSON array. This can make it easier to parse the entire response set as a stream, if you do not have ready access to a streaming JSON parser. To make it possible to detect a truncated response, this format includes a trailer of one blank line. text/plain csv Comma-separated values, with one row per line. Individual field values may be escaped by being surrounded in double quotes. If double quotes appear in a field value, they will be escaped by replacing them with double-double-quotes like \"\"this\"\". To make it possible to detect a truncated response, this format includes a trailer of one blank line. text/csv You can additionally request a header by setting \"header\" to true in your request, like: { \"query\" : \"SELECT COUNT(*) FROM data_source WHERE foo = 'bar' AND __time > TIMESTAMP '2000-01-01 00:00:00'\", \"resultFormat\" : \"arrayLines\", \"header\" : true } In this case, the first result returned will be a header. For the csv, array, and arrayLines formats, the header will be a list of column names. For the object and objectLines formats, the header will be an object where the keys are column names, and the values are null. Errors that occur before the response body is sent will be reported in JSON, with an HTTP 500 status code, in the same format as native Druid query errors. If an error occurs while the response body is being sent, at that point it is too late to change the HTTP status code or report a JSON error, so the response will simply end midstream and an error will be logged by the Druid server that was handling your request. As a caller, it is important that you properly handle response truncation. This is easy for the \"object\" and \"array\" formats, since truncated responses will be invalid JSON. For the line-oriented formats, you should check the trailer they all include: one blank line at the end of the result set. If you detect a truncated response, either through a JSON parsing error or through a missing trailing newline, you should assume the response was not fully delivered due to an error. JDBC You can make Druid SQL queries using the Avatica JDBC driver. We recommend using Avatica JDBC driver version 1.17.0 or later. Note that as of the time of this writing, Avatica 1.17.0, the latest version, does not support passing connection string parameters from the URL to Druid, so you must pass them using a Properties object. Once you've downloaded the Avatica client jar, add it to your classpath and use the connect string jdbc:avatica:remote:url=http://BROKER:8082/druid/v2/sql/avatica/. Example code: // Connect to /druid/v2/sql/avatica/ on your Broker. String url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\"; // Set any connection context parameters you need here (see \"Connection context\" below). // Or leave empty for default behavior. Properties connectionProperties = new Properties(); try (Connection connection = DriverManager.getConnection(url, connectionProperties)) { try ( final Statement statement = connection.createStatement(); final ResultSet resultSet = statement.executeQuery(query) ) { while (resultSet.next()) { // process result set } } } Table metadata is available over JDBC using connection.getMetaData() or by querying the \"INFORMATION_SCHEMA\" tables. Connection stickiness Druid's JDBC server does not share connection state between Brokers. This means that if you're using JDBC and have multiple Druid Brokers, you should either connect to a specific Broker, or use a load balancer with sticky sessions enabled. The Druid Router process provides connection stickiness when balancing JDBC requests, and can be used to achieve the necessary stickiness even with a normal non-sticky load balancer. Please see the Router documentation for more details. Note that the non-JDBC JSON over HTTP API is stateless and does not require stickiness. Dynamic Parameters You can also use parameterized queries in JDBC code, as in this example; PreparedStatement statement = connection.prepareStatement(\"SELECT COUNT(*) AS cnt FROM druid.foo WHERE dim1 = ? OR dim1 = ?\"); statement.setString(1, \"abc\"); statement.setString(2, \"def\"); final ResultSet resultSet = statement.executeQuery(); Connection context Druid SQL supports setting connection parameters on the client. The parameters in the table below affect SQL planning. All other context parameters you provide will be attached to Druid queries and can affect how they run. See Query context for details on the possible options. String url = \"jdbc:avatica:remote:url=http://localhost:8082/druid/v2/sql/avatica/\"; // Set any query context parameters you need here. Properties connectionProperties = new Properties(); connectionProperties.setProperty(\"sqlTimeZone\", \"America/Los_Angeles\"); connectionProperties.setProperty(\"useCache\", \"false\"); try (Connection connection = DriverManager.getConnection(url, connectionProperties)) { // create and execute statements, process result sets, etc } Note that to specify an unique identifier for SQL query, use sqlQueryId instead of queryId. Setting queryId for a SQL request has no effect, all native queries underlying SQL will use auto-generated queryId. Connection context can be specified as JDBC connection properties or as a \"context\" object in the JSON API. Parameter Description Default value sqlQueryId Unique identifier given to this SQL query. For HTTP client, it will be returned in X-Druid-SQL-Query-Id header. auto-generated sqlTimeZone Sets the time zone for this connection, which will affect how time functions and timestamp literals behave. Should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". druid.sql.planner.sqlTimeZone on the Broker (default: UTC) useApproximateCountDistinct Whether to use an approximate cardinality algorithm for COUNT(DISTINCT foo). druid.sql.planner.useApproximateCountDistinct on the Broker (default: true) useApproximateTopN Whether to use approximate TopN queries when a SQL query could be expressed as such. If false, exact GroupBy queries will be used instead. druid.sql.planner.useApproximateTopN on the Broker (default: true) Metadata tables Druid Brokers infer table and column metadata for each datasource from segments loaded in the cluster, and use this to plan SQL queries. This metadata is cached on Broker startup and also updated periodically in the background through SegmentMetadata queries. Background metadata refreshing is triggered by segments entering and exiting the cluster, and can also be throttled through configuration. Druid exposes system information through special system tables. There are two such schemas available: Information Schema and Sys Schema. Information schema provides details about table and column types. The \"sys\" schema provides information about Druid internals like segments/tasks/servers. INFORMATION SCHEMA You can access table and column metadata through JDBC using connection.getMetaData(), or through the INFORMATION_SCHEMA tables described below. For example, to retrieve metadata for the Druid datasource \"foo\", use the query: SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'druid' AND TABLE_NAME = 'foo' Note: INFORMATION_SCHEMA tables do not currently support Druid-specific functions like TIME_PARSE and APPROX_QUANTILE_DS. Only standard SQL functions can be used. SCHEMATA table INFORMATION_SCHEMA.SCHEMATA provides a list of all known schemas, which include druid for standard Druid Table datasources, lookup for Lookups, sys for the virtual System metadata tables, and INFORMATION_SCHEMA for these virtual tables. Tables are allowed to have the same name across different schemas, so the schema may be included in an SQL statement to distinguish them, e.g. lookup.table vs druid.table. Column Notes CATALOG_NAME Always set as druid SCHEMA_NAME druid, lookup, sys, or INFORMATION_SCHEMA SCHEMA_OWNER Unused DEFAULT_CHARACTER_SET_CATALOG Unused DEFAULT_CHARACTER_SET_SCHEMA Unused DEFAULT_CHARACTER_SET_NAME Unused SQL_PATH Unused TABLES table INFORMATION_SCHEMA.TABLES provides a list of all known tables and schemas. Column Notes TABLE_CATALOG Always set as druid TABLE_SCHEMA The 'schema' which the table falls under, see SCHEMATA table for details TABLE_NAME Table name. For the druid schema, this is the dataSource. TABLE_TYPE \"TABLE\" or \"SYSTEM_TABLE\" IS_JOINABLE If a table is directly joinable if on the right hand side of a JOIN statement, without performing a subquery, this value will be set to YES, otherwise NO. Lookups are always joinable because they are globally distributed among Druid query processing nodes, but Druid datasources are not, and will use a less efficient subquery join. IS_BROADCAST If a table is 'broadcast' and distributed among all Druid query processing nodes, this value will be set to YES, such as lookups and Druid datasources which have a 'broadcast' load rule, else NO. COLUMNS table INFORMATION_SCHEMA.COLUMNS provides a list of all known columns across all tables and schema. Column Notes TABLE_CATALOG Always set as druid TABLE_SCHEMA The 'schema' which the table column falls under, see SCHEMATA table for details TABLE_NAME The 'table' which the column belongs to, see TABLES table for details COLUMN_NAME The column name ORDINAL_POSITION The order in which the column is stored in a table COLUMN_DEFAULT Unused IS_NULLABLE DATA_TYPE CHARACTER_MAXIMUM_LENGTH Unused CHARACTER_OCTET_LENGTH Unused NUMERIC_PRECISION NUMERIC_PRECISION_RADIX NUMERIC_SCALE DATETIME_PRECISION CHARACTER_SET_NAME COLLATION_NAME JDBC_TYPE Type code from java.sql.Types (Druid extension) SYSTEM SCHEMA The \"sys\" schema provides visibility into Druid segments, servers and tasks. Note: \"sys\" tables do not currently support Druid-specific functions like TIME_PARSE and APPROX_QUANTILE_DS. Only standard SQL functions can be used. SEGMENTS table Segments table provides details on all Druid segments, whether they are published yet or not. Column Type Notes segment_id STRING Unique segment identifier datasource STRING Name of datasource start STRING Interval start time (in ISO 8601 format) end STRING Interval end time (in ISO 8601 format) size LONG Size of segment in bytes version STRING Version string (generally an ISO8601 timestamp corresponding to when the segment set was first started). Higher version means the more recently created segment. Version comparing is based on string comparison. partition_num LONG Partition number (an integer, unique within a datasource+interval+version; may not necessarily be contiguous) num_replicas LONG Number of replicas of this segment currently being served num_rows LONG Number of rows in current segment, this value could be null if unknown to Broker at query time is_published LONG Boolean is represented as long type where 1 = true, 0 = false. 1 represents this segment has been published to the metadata store with used=1. See the Architecture page for more details. is_available LONG Boolean is represented as long type where 1 = true, 0 = false. 1 if this segment is currently being served by any process(Historical or realtime). See the Architecture page for more details. is_realtime LONG Boolean is represented as long type where 1 = true, 0 = false. 1 if this segment is only served by realtime tasks, and 0 if any historical process is serving this segment. is_overshadowed LONG Boolean is represented as long type where 1 = true, 0 = false. 1 if this segment is published and is fully overshadowed by some other published segments. Currently, is_overshadowed is always false for unpublished segments, although this may change in the future. You can filter for segments that \"should be published\" by filtering for is_published = 1 AND is_overshadowed = 0. Segments can briefly be both published and overshadowed if they were recently replaced, but have not been unpublished yet. See the Architecture page for more details. shard_spec STRING JSON-serialized form of the segment ShardSpec dimensions STRING JSON-serialized form of the segment dimensions metrics STRING JSON-serialized form of the segment metrics last_compaction_state STRING JSON-serialized form of the compaction task's config (compaction task which created this segment). May be null if segment was not created by compaction task. For example to retrieve all segments for datasource \"wikipedia\", use the query: SELECT * FROM sys.segments WHERE datasource = 'wikipedia' Another example to retrieve segments total_size, avg_size, avg_num_rows and num_segments per datasource: SELECT datasource, SUM(\"size\") AS total_size, CASE WHEN SUM(\"size\") = 0 THEN 0 ELSE SUM(\"size\") / (COUNT(*) FILTER(WHERE \"size\" > 0)) END AS avg_size, CASE WHEN SUM(num_rows) = 0 THEN 0 ELSE SUM(\"num_rows\") / (COUNT(*) FILTER(WHERE num_rows > 0)) END AS avg_num_rows, COUNT(*) AS num_segments FROM sys.segments GROUP BY 1 ORDER BY 2 DESC If you want to retrieve segment that was compacted (ANY compaction): SELECT * FROM sys.segments WHERE last_compaction_state is not null or if you want to retrieve segment that was compacted only by a particular compaction spec (such as that of the auto compaction): SELECT * FROM sys.segments WHERE last_compaction_state == 'SELECT * FROM sys.segments where last_compaction_state = 'CompactionState{partitionsSpec=DynamicPartitionsSpec{maxRowsPerSegment=5000000, maxTotalRows=9223372036854775807}, indexSpec={bitmap={type=roaring, compressRunOnSerialization=true}, dimensionCompression=lz4, metricCompression=lz4, longEncoding=longs, segmentLoader=null}}' Caveat: Note that a segment can be served by more than one stream ingestion tasks or Historical processes, in that case it would have multiple replicas. These replicas are weakly consistent with each other when served by multiple ingestion tasks, until a segment is eventually served by a Historical, at that point the segment is immutable. Broker prefers to query a segment from Historical over an ingestion task. But if a segment has multiple realtime replicas, for e.g.. Kafka index tasks, and one task is slower than other, then the sys.segments query results can vary for the duration of the tasks because only one of the ingestion tasks is queried by the Broker and it is not guaranteed that the same task gets picked every time. The num_rows column of segments table can have inconsistent values during this period. There is an open issue about this inconsistency with stream ingestion tasks. SERVERS table Servers table lists all discovered servers in the cluster. Column Type Notes server STRING Server name in the form host:port host STRING Hostname of the server plaintext_port LONG Unsecured port of the server, or -1 if plaintext traffic is disabled tls_port LONG TLS port of the server, or -1 if TLS is disabled server_type STRING Type of Druid service. Possible values include: COORDINATOR, OVERLORD, BROKER, ROUTER, HISTORICAL, MIDDLE_MANAGER or PEON. tier STRING Distribution tier see druid.server.tier. Only valid for HISTORICAL type, for other types it's null current_size LONG Current size of segments in bytes on this server. Only valid for HISTORICAL type, for other types it's 0 max_size LONG Max size in bytes this server recommends to assign to segments see druid.server.maxSize. Only valid for HISTORICAL type, for other types it's 0 is_leader LONG 1 if the server is currently the 'leader' (for services which have the concept of leadership), otherwise 0 if the server is not the leader, or the default long value (0 or null depending on druid.generic.useDefaultValueForNull) if the server type does not have the concept of leadership To retrieve information about all servers, use the query: SELECT * FROM sys.servers; SERVER_SEGMENTS table SERVER_SEGMENTS is used to join servers with segments table Column Type Notes server STRING Server name in format host:port (Primary key of servers table) segment_id STRING Segment identifier (Primary key of segments table) JOIN between \"servers\" and \"segments\" can be used to query the number of segments for a specific datasource, grouped by server, example query: SELECT count(segments.segment_id) as num_segments from sys.segments as segments INNER JOIN sys.server_segments as server_segments ON segments.segment_id = server_segments.segment_id INNER JOIN sys.servers as servers ON servers.server = server_segments.server WHERE segments.datasource = 'wikipedia' GROUP BY servers.server; TASKS table The tasks table provides information about active and recently-completed indexing tasks. For more information check out the documentation for ingestion tasks. Column Type Notes task_id STRING Unique task identifier group_id STRING Task group ID for this task, the value depends on the task type. For example, for native index tasks, it's same as task_id, for sub tasks, this value is the parent task's ID type STRING Task type, for example this value is \"index\" for indexing tasks. See tasks-overview datasource STRING Datasource name being indexed created_time STRING Timestamp in ISO8601 format corresponding to when the ingestion task was created. Note that this value is populated for completed and waiting tasks. For running and pending tasks this value is set to 1970-01-01T00:00:00Z queue_insertion_time STRING Timestamp in ISO8601 format corresponding to when this task was added to the queue on the Overlord status STRING Status of a task can be RUNNING, FAILED, SUCCESS runner_status STRING Runner status of a completed task would be NONE, for in-progress tasks this can be RUNNING, WAITING, PENDING duration LONG Time it took to finish the task in milliseconds, this value is present only for completed tasks location STRING Server name where this task is running in the format host:port, this information is present only for RUNNING tasks host STRING Hostname of the server where task is running plaintext_port LONG Unsecured port of the server, or -1 if plaintext traffic is disabled tls_port LONG TLS port of the server, or -1 if TLS is disabled error_msg STRING Detailed error message in case of FAILED tasks For example, to retrieve tasks information filtered by status, use the query SELECT * FROM sys.tasks WHERE status='FAILED'; SUPERVISORS table The supervisors table provides information about supervisors. Column Type Notes supervisor_id STRING Supervisor task identifier state STRING Basic state of the supervisor. Available states: UNHEALTHY_SUPERVISOR, UNHEALTHY_TASKS, PENDING, RUNNING, SUSPENDED, STOPPING. Check Kafka Docs for details. detailed_state STRING Supervisor specific state. (See documentation of the specific supervisor for details, e.g. Kafka or Kinesis) healthy LONG Boolean represented as long type where 1 = true, 0 = false. 1 indicates a healthy supervisor type STRING Type of supervisor, e.g. kafka, kinesis or materialized_view source STRING Source of the supervisor, e.g. Kafka topic or Kinesis stream suspended LONG Boolean represented as long type where 1 = true, 0 = false. 1 indicates supervisor is in suspended state spec STRING JSON-serialized supervisor spec For example, to retrieve supervisor tasks information filtered by health status, use the query SELECT * FROM sys.supervisors WHERE healthy=0; Server configuration Druid SQL planning occurs on the Broker and is configured by Broker runtime properties. Security Please see Defining SQL permissions in the basic security documentation for information on permissions needed for making SQL queries. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/querying.html":{"url":"querying/querying.html","title":"通过JSON查询","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native query language. For information about how Druid SQL chooses which native query types to use when it runs a SQL query, refer to the SQL documentation. Native queries in Druid are JSON objects and are typically issued to the Broker or Router processes. Queries can be posted like this: curl -X POST ':/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/json' -d @ Replace : with the appropriate address and port for your system. For example, if running the quickstart configuration, replace : with localhost:8888. You can also enter them directly in the Druid console's Query view. Simply pasting a native query into the console switches the editor into JSON mode. Druid's native query language is JSON over HTTP, although many members of the community have contributed different client libraries in other languages to query Druid. The Content-Type/Accept Headers can also take 'application/x-jackson-smile'. curl -X POST ':/druid/v2/?pretty' -H 'Content-Type:application/json' -H 'Accept:application/x-jackson-smile' -d @ If the Accept header is not provided, it defaults to the value of 'Content-Type' header. Druid's native query is relatively low level, mapping closely to how computations are performed internally. Druid queries are designed to be lightweight and complete very quickly. This means that for more complex analysis, or to build more complex visualizations, multiple Druid queries may be required. Even though queries are typically made to Brokers or Routers, they can also be accepted by Historical processes and by Peons (task JVMs)) that are running stream ingestion tasks. This may be valuable if you want to query results for specific segments that are served by specific processes. Available queries Druid has numerous query types for various use cases. Queries are composed of various JSON properties and Druid has different types of queries for different use cases. The documentation for the various query types describe all the JSON properties that can be set. Aggregation queries Timeseries TopN GroupBy Metadata queries TimeBoundary SegmentMetadata DatasourceMetadata Other queries Scan Search Which query type should I use? For aggregation queries, if more than one would satisfy your needs, we generally recommend using Timeseries or TopN whenever possible, as they are specifically optimized for their use cases. If neither is a good fit, you should use the GroupBy query, which is the most flexible. Query cancellation Queries can be cancelled explicitly using their unique identifier. If the query identifier is set at the time of query, or is otherwise known, the following endpoint can be used on the Broker or Router to cancel the query. DELETE /druid/v2/{queryId} For example, if the query ID is abc123, the query can be cancelled as follows: curl -X DELETE \"http://host:port/druid/v2/abc123\" Query errors Authentication and authorization failures For secured Druid clusters, query requests respond with an HTTP 401 response code in case of an authentication failure. For authorization failures, an HTTP 403 response code is returned. Query execution failures If a query fails, Druid returns a response with an HTTP response code and a JSON object with the following structure: { \"error\" : \"Query timeout\", \"errorMessage\" : \"Timeout waiting for task.\", \"errorClass\" : \"java.util.concurrent.TimeoutException\", \"host\" : \"druid1.example.com:8083\" } The fields in the response are: field description error A well-defined error code (see below). errorMessage A free-form message with more information about the error. May be null. errorClass The class of the exception that caused this error. May be null. host The host on which this error occurred. May be null. Possible Druid error codes for the error field include: Error code HTTP response code description SQL parse failed 400 Only for SQL queries. The SQL query failed to parse. Plan validation failed 400 Only for SQL queries. The SQL query failed to validate. Resource limit exceeded 400 The query exceeded a configured resource limit (e.g. groupBy maxResults). Query capacity exceeded 429 The query failed to execute because of the lack of resources available at the time when the query was submitted. The resources could be any runtime resources such as query scheduler lane capacity, merge buffers, and so on. The error message should have more details about the failure. Unsupported operation 501 The query attempted to perform an unsupported operation. This may occur when using undocumented features or when using an incompletely implemented extension. Query timeout 504 The query timed out. Query interrupted 500 The query was interrupted, possibly due to JVM shutdown. Query cancelled 500 The query was cancelled through the query cancellation API. Truncated response context 500 An intermediate response context for the query exceeded the built-in limit of 7KB.The response context is an internal data structure that Druid servers use to share out-of-band information when sending query results to each other. It is serialized in an HTTP header with a maximum length of 7KB. This error occurs when an intermediate response context sent from a data server (like a Historical) to the Broker exceeds this limit.The response context is used for a variety of purposes, but the one most likely to generate a large context is sharing details about segments that move during a query. That means this error can potentially indicate that a very large number of segments moved in between the time a Broker issued a query and the time it was processed on Historicals. This should rarely, if ever, occur during normal operation. Unknown exception 500 Some other exception occurred. Check errorMessage and errorClass for details, although keep in mind that the contents of those fields are free-form and may change from release to release. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/query-execution.html":{"url":"querying/query-execution.html","title":"查询执行流程","keywords":"","body":" This document describes how Druid executes native queries, but since Druid SQL queries are translated to native queries, this document applies to the SQL runtime as well. Refer to the SQL Query translation page for information about how SQL queries are translated to native queries. Druid's approach to query execution varies depending on the kind of datasource you are querying. Datasource type table Queries that operate directly on table datasources are executed using a scatter-gather approach led by the Broker process. The process looks like this: The Broker identifies which segments are relevant to the query based on the \"intervals\" parameter. Segments are always partitioned by time, so any segment whose interval overlaps the query interval is potentially relevant. The Broker may additionally further prune the segment list based on the \"filter\", if the input data was partitioned by range using the single_dim partitionsSpec, and if the filter matches the dimension used for partitioning. The Broker, having pruned the list of segments for the query, forwards the query to data servers (like Historicals and tasks running on MiddleManagers) that are currently serving those segments. For all query types except Scan, data servers process each segment in parallel and generate partial results for each segment. The specific processing that is done depends on the query type. These partial results may be cached if query caching is enabled. For Scan queries, segments are processed in order by a single thread, and results are not cached. The Broker receives partial results from each data server, merges them into the final result set, and returns them to the caller. For Timeseries and Scan queries, and for GroupBy queries where there is no sorting, the Broker is able to do this in a streaming fashion. Otherwise, the Broker fully computes the result set before returning anything. lookup Queries that operate directly on lookup datasources (without a join) are executed on the Broker that received the query, using its local copy of the lookup. All registered lookup tables are preloaded in-memory on the Broker. The query runs single-threaded. Execution of queries that use lookups as right-hand inputs to a join are executed in a way that depends on their \"base\" (bottom-leftmost) datasource, as described in the join section below. union Queries that operate directly on union datasources are split up on the Broker into a separate query for each table that is part of the union. Each of these queries runs separately, and the Broker merges their results together. inline Queries that operate directly on inline datasources are executed on the Broker that received the query. The query runs single-threaded. Execution of queries that use inline datasources as right-hand inputs to a join are executed in a way that depends on their \"base\" (bottom-leftmost) datasource, as described in the join section below. query Query datasources are subqueries. Each subquery is executed as if it was its own query and the results are brought back to the Broker. Then, the Broker continues on with the rest of the query as if the subquery was replaced with an inline datasource. In most cases, subquery results are fully buffered in memory on the Broker before the rest of the query proceeds, meaning subqueries execute sequentially. The total number of rows buffered across all subqueries of a given query in this way cannot exceed the druid.server.http.maxSubqueryRows property. There is one exception: if the outer query and all subqueries are the groupBy type, then subquery results can be processed in a streaming fashion and the druid.server.http.maxSubqueryRows limit does not apply. join Join datasources are handled using a broadcast hash-join approach. The Broker executes any subqueries that are inputs the join, as described in the query section, and replaces them with inline datasources. The Broker flattens a join tree, if present, into a \"base\" datasource (the bottom-leftmost one) and other leaf datasources (the rest). Query execution proceeds using the same structure that the base datasource would use on its own. If the base datasource is a table, segments are pruned based on \"intervals\" as usual, and the query is executed on the cluster by forwarding it to all relevant data servers in parallel. If the base datasource is a lookup or inline datasource (including an inline datasource that was the result of inlining a subquery), the query is executed on the Broker itself. The base query cannot be a union, because unions are not currently supported as inputs to a join. Before beginning to process the base datasource, the server(s) that will execute the query first inspect all the non-base leaf datasources to determine if a new hash table needs to be built for the upcoming hash join. Currently, lookups do not require new hash tables to be built (because they are preloaded), but inline datasources do. Query execution proceeds again using the same structure that the base datasource would use on its own, with one addition: while processing the base datasource, Druid servers will use the hash tables built from the other join inputs to produce the join result row-by-row, and query engines will operate on the joined rows rather than the base rows. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/datasource.html":{"url":"querying/datasource.html","title":"数据源(datasource)","keywords":"","body":" Datasources in Apache Druid are things that you can query. The most common kind of datasource is a table datasource, and in many contexts the word \"datasource\" implicitly refers to table datasources. This is especially true during data ingestion, where ingestion is always creating or writing into a table datasource. But at query time, there are many other types of datasources available. The word \"datasource\" is generally spelled dataSource (with a capital S) when it appears in API requests and responses. Datasource type table SELECT column1, column2 FROM \"druid\".\"dataSourceName\" { \"queryType\": \"scan\", \"dataSource\": \"dataSourceName\", \"columns\": [\"column1\", \"column2\"], \"intervals\": [\"0000/3000\"] } The table datasource is the most common type. This is the kind of datasource you get when you perform data ingestion. They are split up into segments, distributed around the cluster, and queried in parallel. In Druid SQL, table datasources reside in the the druid schema. This is the default schema, so table datasources can be referenced as either druid.dataSourceName or simply dataSourceName. In native queries, table datasources can be referenced using their names as strings (as in the example above), or by using JSON objects of the form: \"dataSource\": { \"type\": \"table\", \"name\": \"dataSourceName\" } To see a list of all table datasources, use the SQL query SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'druid'. lookup SELECT k, v FROM lookup.countries { \"queryType\": \"scan\", \"dataSource\": { \"type\": \"lookup\", \"lookup\": \"countries\" }, \"columns\": [\"k\", \"v\"], \"intervals\": [\"0000/3000\"] } Lookup datasources correspond to Druid's key-value lookup objects. In Druid SQL, they reside in the the lookup schema. They are preloaded in memory on all servers, so they can be accessed rapidly. They can be joined onto regular tables using the join operator. Lookup datasources are key-value oriented and always have exactly two columns: k (the key) and v (the value), and both are always strings. To see a list of all lookup datasources, use the SQL query SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'lookup'. Performance tip: Lookups can be joined with a base table either using an explicit join, or by using the SQL LOOKUP function. However, the join operator must evaluate the condition on each row, whereas the LOOKUP function can defer evaluation until after an aggregation phase. This means that the LOOKUP function is usually faster than joining to a lookup datasource. Refer to the Query execution page for more details on how queries are executed when you use table datasources. union { \"queryType\": \"scan\", \"dataSource\": { \"type\": \"union\", \"dataSources\": [\"\", \"\", \"\"] }, \"columns\": [\"column1\", \"column2\"], \"intervals\": [\"0000/3000\"] } Union datasources allow you to treat two or more table datasources as a single datasource. The datasources being unioned do not need to have identical schemas. If they do not fully match up, then columns that exist in one table but not another will be treated as if they contained all null values in the tables where they do not exist. The list of \"dataSources\" must be nonempty. If you want to query an empty dataset, use an inline datasource instead. Union datasources are not available in Druid SQL. Refer to the Query execution page for more details on how queries are executed when you use union datasources. inline { \"queryType\": \"scan\", \"dataSource\": { \"type\": \"inline\", \"columnNames\": [\"country\", \"city\"], \"rows\": [ [\"United States\", \"San Francisco\"], [\"Canada\", \"Calgary\"] ] }, \"columns\": [\"country\", \"city\"], \"intervals\": [\"0000/3000\"] } Inline datasources allow you to query a small amount of data that is embedded in the query itself. They are useful when you want to write a query on a small amount of data without loading it first. They are also useful as inputs into a join. Druid also uses them internally to handle subqueries that need to be inlined on the Broker. See the query datasource documentation for more details. There are two fields in an inline datasource: an array of columnNames and an array of rows. Each row is an array that must be exactly as long as the list of columnNames. The first element in each row corresponds to the first column in columnNames, and so on. Inline datasources are not available in Druid SQL. Refer to the Query execution page for more details on how queries are executed when you use inline datasources. query -- Uses a subquery to count hits per page, then takes the average. SELECT AVG(cnt) AS average_hits_per_page FROM (SELECT page, COUNT(*) AS hits FROM site_traffic GROUP BY page) { \"queryType\": \"timeseries\", \"dataSource\": { \"type\": \"query\", \"query\": { \"queryType\": \"groupBy\", \"dataSource\": \"site_traffic\", \"intervals\": [\"0000/3000\"], \"granularity\": \"all\", \"dimensions\": [\"page\"], \"aggregations\": [ { \"type\": \"count\", \"name\": \"hits\" } ] } }, \"intervals\": [\"0000/3000\"], \"granularity\": \"all\", \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"hits\", \"fieldName\": \"hits\" }, { \"type\": \"count\", \"name\": \"pages\" } ], \"postAggregations\": [ { \"type\": \"expression\", \"name\": \"average_hits_per_page\", \"expression\": \"hits / pages\" } ] } Query datasources allow you to issue subqueries. In native queries, they can appear anywhere that accepts a dataSource. In SQL, they can appear in the following places, always surrounded by parentheses: The FROM clause: FROM (). As inputs to a JOIN: t1 INNER JOIN t2 ON t1. = t2.. In the WHERE clause: WHERE { IN | NOT IN } (). These are translated to joins by the SQL planner. Performance tip: In most cases, subquery results are fully buffered in memory on the Broker and then further processing occurs on the Broker itself. This means that subqueries with large result sets can cause performance bottlenecks or run into memory usage limits on the Broker. See the Query execution page for more details on how subqueries are executed and what limits will apply. join -- Joins \"sales\" with \"countries\" (using \"store\" as the join key) to get sales by country. SELECT store_to_country.v AS country, SUM(sales.revenue) AS country_revenue FROM sales INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k GROUP BY countries.v { \"queryType\": \"groupBy\", \"dataSource\": { \"type\": \"join\", \"left\": \"sales\", \"right\": { \"type\": \"lookup\", \"lookup\": \"store_to_country\" }, \"rightPrefix\": \"r.\", \"condition\": \"store == \\\"r.k\\\"\", \"joinType\": \"INNER\" }, \"intervals\": [\"0000/3000\"], \"granularity\": \"all\", \"dimensions\": [ { \"type\": \"default\", \"outputName\": \"country\", \"dimension\": \"r.v\" } ], \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"country_revenue\", \"fieldName\": \"revenue\" } ] } Join datasources allow you to do a SQL-style join of two datasources. Stacking joins on top of each other allows you to join arbitrarily many datasources. In Druid , joins are implemented with a broadcast hash-join algorithm. This means that all tables other than the leftmost \"base\" table must fit in memory. It also means that the join condition must be an equality. This feature is intended mainly to allow joining regular Druid tables with lookup, inline, and query datasources. Refer to the Query execution page for more details on how queries are executed when you use join datasources. Joins in SQL SQL joins take the form: [ INNER | LEFT [OUTER] ] JOIN ON The condition must involve only equalities, but functions are okay, and there can be multiple equalities ANDed together. Conditions like t1.x = t2.x, or LOWER(t1.x) = t2.x, or t1.x = t2.x AND t1.y = t2.y can all be handled. Conditions like t1.x <> t2.x cannot currently be handled. Note that Druid SQL is less rigid than what native join datasources can handle. In cases where a SQL query does something that is not allowed as-is with a native join datasource, Druid SQL will generate a subquery. This can have a substantial effect on performance and scalability, so it is something to watch out for. Some examples of when the SQL layer will generate subqueries include: Joining a regular Druid table to itself, or to another regular Druid table. The native join datasource can accept a table on the left-hand side, but not the right, so a subquery is needed. Join conditions where the expressions on either side are of different types. Join conditions where the right-hand expression is not a direct column access. For more information about how Druid translates SQL to native queries, refer to the Druid SQL documentation. Joins in native queries Native join datasources have the following properties. All are required. Field Description left Left-hand datasource. Must be of type table, join, lookup, query, or inline. Placing another join as the left datasource allows you to join arbitrarily many datasources. right Right-hand datasource. Must be of type lookup, query, or inline. Note that this is more rigid than what Druid SQL requires. rightPrefix String prefix that will be applied to all columns from the right-hand datasource, to prevent them from colliding with columns from the left-hand datasource. Can be any string, so long as it is nonempty and is not be a prefix of the string __time. Any columns from the left-hand side that start with your rightPrefix will be shadowed. It is up to you to provide a prefix that will not shadow any important columns from the left side. condition Expression that must be an equality where one side is an expression of the left-hand side, and the other side is a simple column reference to the right-hand side. Note that this is more rigid than what Druid SQL requires: here, the right-hand reference must be a simple column reference; in SQL it can be an expression. joinType INNER or LEFT. Join performance Joins are a feature that can significantly affect performance of your queries. Some performance tips and notes: Joins are especially useful with lookup datasources, but in most cases, the LOOKUP function performs better than a join. Consider using the LOOKUP function if it is appropriate for your use case. When using joins in Druid SQL, keep in mind that it can generate subqueries that you did not explicitly include in your queries. Refer to the Druid SQL documentation for more details about when this happens and how to detect it. One common reason for implicit subquery generation is if the types of the two halves of an equality do not match. For example, since lookup keys are always strings, the condition druid.d JOIN lookup.l ON d.field = l.field will perform best if d.field is a string. As of Druid , the join operator must evaluate the condition for each row. In the future, we expect to implement both early and deferred condition evaluation, which we expect to improve performance considerably for common use cases. Currently, Druid does not support pushing down predicates (condition and filter) past a Join (i.e. into Join's children). Druid only supports pushing predicates into the join if they originated from above the join. Hence, the location of predicates and filters in your Druid SQL is very important. Also, as a result of this, comma joins should be avoided. Future work for joins Joins are an area of active development in Druid. The following features are missing today but may appear in future versions: Reordering of predicates and filters (pushing up and/or pushing down) to get the most performant plan. Preloaded dimension tables that are wider than lookups (i.e. supporting more than a single key and single value). RIGHT OUTER and FULL OUTER joins. Currently, they are partially implemented. Queries will run but results will not always be correct. Performance-related optimizations as mentioned in the previous section. Join algorithms other than broadcast hash-joins. Join condition on a column compared to a constant value. Join conditions on a column containing a multi-value dimension. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/joins.html":{"url":"querying/joins.html","title":"联合查询(join)","keywords":"","body":" Druid has two features related to joining of data: Join operators. These are available using a join datasource in native queries, or using the JOIN operator in Druid SQL. Refer to the join datasource documentation for information about how joins work in Druid. Query-time lookups, simple key-to-value mappings. These are preloaded on all servers that are involved in queries and can be accessed with or without an explicit join operator. Refer to the lookups documentation for more details. Whenever possible, for best performance it is good to avoid joins at query time. Often this can be accomplished by joining data before it is loaded into Druid. However, there are situations where joins or lookups are the best solution available despite the performance overhead, including: The fact-to-dimension (star and snowflake schema) case: you need to change dimension values after initial ingestion, and aren't able to reingest to do this. In this case, you can use lookups for your dimension tables. Your workload requires joins or filters on subqueries. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/lookups.html":{"url":"querying/lookups.html","title":"字典查询(lookups)","keywords":"","body":" Lookups are an experimental feature. Lookups are a concept in Apache Druid where dimension values are (optionally) replaced with new values, allowing join-like functionality. Applying lookups in Druid is similar to joining a dimension table in a data warehouse. See dimension specs for more information. For the purpose of these documents, a \"key\" refers to a dimension value to match, and a \"value\" refers to its replacement. So if you wanted to map appid-12345 to Super Mega Awesome App then the key would be appid-12345 and the value would be Super Mega Awesome App. It is worth noting that lookups support not just use cases where keys map one-to-one to unique values, such as country code and country name, but also support use cases where multiple IDs map to the same value, e.g. multiple app-ids mapping to a single account manager. When lookups are one-to-one, Druid is able to apply additional optimizations at query time; see Query execution below for more details. Lookups do not have history. They always use the current data. This means that if the chief account manager for a particular app-id changes, and you issue a query with a lookup to store the app-id to account manager relationship, it will return the current account manager for that app-id REGARDLESS of the time range over which you query. If you require data time range sensitive lookups, such a use case is not currently supported dynamically at query time, and such data belongs in the raw denormalized data for use in Druid. Lookups are generally preloaded in-memory on all servers. But very small lookups (on the order of a few dozen to a few hundred entries) can also be passed inline in native queries time using the \"map\" lookup type. Refer to the dimension specs documentation for details. Other lookup types are available as extensions, including: Globally cached lookups from local files, remote URIs, or JDBC through lookups-cached-global. Globally cached lookups from a Kafka topic through kafka-extraction-namespace. Query Syntax In Druid SQL, lookups can be queried using the LOOKUP function, for example: SELECT LOOKUP(store, 'store_to_country') AS country, SUM(revenue) FROM sales GROUP BY 1 They can also be queried using the JOIN operator: SELECT store_to_country.v AS country, SUM(sales.revenue) AS country_revenue FROM sales INNER JOIN lookup.store_to_country ON sales.store = store_to_country.k GROUP BY 1 In native queries, lookups can be queried with dimension specs or extraction functions. Query Execution When executing an aggregation query involving lookup functions (like the SQL LOOKUP function, Druid can decide to apply them while scanning and aggregating rows, or to apply them after aggregation is complete. It is more efficient to apply lookups after aggregation is complete, so Druid will do this if it can. Druid decides this by checking if the lookup is marked as \"injective\" or not. In general, you should set this property for any lookup that is naturally one-to-one, to allow Druid to run your queries as fast as possible. Injective lookups should include all possible keys that may show up in your dataset, and should also map all keys to unique values. This matters because non-injective lookups may map different keys to the same value, which must be accounted for during aggregation, lest query results contain two result values that should have been aggregated into one. This lookup is injective (assuming it contains all possible keys from your data): 1 -> Foo 2 -> Bar 3 -> Billy But this one is not, since both \"2\" and \"3\" map to the same key: 1 -> Foo 2 -> Bar 3 -> Bar To tell Druid that your lookup is injective, you must specify \"injective\" : true in the lookup configuration. Druid will not detect this automatically. Currently, the injective lookup optimization is not triggered when lookups are inputs to a join datasource. It is only used when lookup functions are used directly, without the join operator. Dynamic Configuration Dynamic lookup configuration is an experimental feature. Static configuration is no longer supported. The following documents the behavior of the cluster-wide config which is accessible through the Coordinator. The configuration is propagated through the concept of \"tier\" of servers. A \"tier\" is defined as a group of services which should receive a set of lookups. For example, you might have all Historicals be part of __default, and Peons be part of individual tiers for the datasources they are tasked with. The tiers for lookups are completely independent of Historical tiers. These configs are accessed using JSON through the following URI template http://:/druid/coordinator/v1/lookups/config/{tier}/{id} All URIs below are assumed to have http://: prepended. If you have NEVER configured lookups before, you MUST post an empty json object {} to /druid/coordinator/v1/lookups/config to initialize the configuration. These endpoints will return one of the following results: 404 if the resource is not found 400 if there is a problem in the formatting of the request 202 if the request was accepted asynchronously (POST and DELETE) 200 if the request succeeded (GET only) Configuration propagation behavior The configuration is propagated to the query serving processes (Broker / Router / Peon / Historical) by the Coordinator. The query serving processes have an internal API for managing lookups on the process and those are used by the Coordinator. The Coordinator periodically checks if any of the processes need to load/drop lookups and updates them appropriately. Please note that only 2 simultaneous lookup configuration propagation requests can be concurrently handled by a single query serving process. This limit is applied to prevent lookup handling from consuming too many server HTTP connections. API for configuring lookups Bulk update Lookups can be updated in bulk by posting a JSON object to /druid/coordinator/v1/lookups/config. The format of the json object is as follows: { \"\": { \"\": { \"version\": \"\", \"lookupExtractorFactory\": { \"type\": \"\", \"\": \"\" } } } } Note that \"version\" is an arbitrary string assigned by the user, when making updates to existing lookup then user would need to specify a lexicographically higher version. For example, a config might look something like: { \"__default\": { \"country_code\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"77483\": \"United States\" } } }, \"site_id\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"cachedNamespace\", \"extractionNamespace\": { \"type\": \"jdbc\", \"connectorConfig\": { \"createTables\": true, \"connectURI\": \"jdbc:mysql:\\/\\/localhost:3306\\/druid\", \"user\": \"druid\", \"password\": \"diurd\" }, \"table\": \"lookupTable\", \"keyColumn\": \"country_id\", \"valueColumn\": \"country_name\", \"tsColumn\": \"timeColumn\" }, \"firstCacheTimeout\": 120000, \"injective\": true } }, \"site_id_customer1\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"847632\": \"Internal Use Only\" } } }, \"site_id_customer2\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"AHF77\": \"Home\" } } } }, \"realtime_customer1\": { \"country_code\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"77483\": \"United States\" } } }, \"site_id_customer1\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"847632\": \"Internal Use Only\" } } } }, \"realtime_customer2\": { \"country_code\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"77483\": \"United States\" } } }, \"site_id_customer2\": { \"version\": \"v0\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"AHF77\": \"Home\" } } } } } All entries in the map will UPDATE existing entries. No entries will be deleted. Update lookup A POST to a particular lookup extractor factory via /druid/coordinator/v1/lookups/config/{tier}/{id} will update that specific extractor factory. For example, a post to /druid/coordinator/v1/lookups/config/realtime_customer1/site_id_customer1 might contain the following: { \"version\": \"v1\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"847632\": \"Internal Use Only\" } } } This will replace the site_id_customer1 lookup in the realtime_customer1 with the definition above. Get all lookups A GET to /druid/coordinator/v1/lookups/config/all will return all known lookup specs for all tiers. Get lookup A GET to a particular lookup extractor factory is accomplished via /druid/coordinator/v1/lookups/config/{tier}/{id} Using the prior example, a GET to /druid/coordinator/v1/lookups/config/realtime_customer2/site_id_customer2 should return { \"version\": \"v1\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"AHF77\": \"Home\" } } } Delete lookup A DELETE to /druid/coordinator/v1/lookups/config/{tier}/{id} will remove that lookup from the cluster. If it was last lookup in the tier, then tier is deleted as well. Delete tier A DELETE to /druid/coordinator/v1/lookups/config/{tier} will remove that tier from the cluster. List tier names A GET to /druid/coordinator/v1/lookups/config will return a list of known tier names in the dynamic configuration. To discover a list of tiers currently active in the cluster in addition to ones known in the dynamic configuration, the parameter discover=true can be added as per /druid/coordinator/v1/lookups/config?discover=true. List lookup names A GET to /druid/coordinator/v1/lookups/config/{tier} will return a list of known lookup names for that tier. These end points can be used to get the propagation status of configured lookups to processes using lookups such as Historicals. API for lookup status List load status of all lookups GET /druid/coordinator/v1/lookups/status with optional query parameter detailed. List load status of lookups in a tier GET /druid/coordinator/v1/lookups/status/{tier} with optional query parameter detailed. List load status of single lookup GET /druid/coordinator/v1/lookups/status/{tier}/{lookup} with optional query parameter detailed. List lookup state of all processes GET /druid/coordinator/v1/lookups/nodeStatus with optional query parameter discover to discover tiers advertised by other Druid nodes, or by default, returning all configured lookup tiers. The default response will also include the lookups which are loaded, being loaded, or being dropped on each node, for each tier, including the complete lookup spec. Add the optional query parameter detailed=false to only include the 'version' of the lookup instead of the complete spec. List lookup state of processes in a tier GET /druid/coordinator/v1/lookups/nodeStatus/{tier} List lookup state of single process GET /druid/coordinator/v1/lookups/nodeStatus/{tier}/{host:port} Internal API The Peon, Router, Broker, and Historical processes all have the ability to consume lookup configuration. There is an internal API these processes use to list/load/drop their lookups starting at /druid/listen/v1/lookups. These follow the same convention for return values as the cluster wide dynamic configuration. Following endpoints can be used for debugging purposes but not otherwise. Get lookups A GET to the process at /druid/listen/v1/lookups will return a json map of all the lookups currently active on the process. The return value will be a json map of the lookups to their extractor factories. { \"site_id_customer2\": { \"version\": \"v1\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"AHF77\": \"Home\" } } } } Get lookup A GET to the process at /druid/listen/v1/lookups/some_lookup_name will return the LookupExtractorFactory for the lookup identified by some_lookup_name. The return value will be the json representation of the factory. { \"version\": \"v1\", \"lookupExtractorFactory\": { \"type\": \"map\", \"map\": { \"AHF77\": \"Home\" } } } Configuration See Lookups Dynamic Configuration for Coordinator configuration. To configure a Broker / Router / Historical / Peon to announce itself as part of a lookup tier, use following properties. Property Description Default druid.lookup.lookupTier The tier for lookups for this process. This is independent of other tiers. __default druid.lookup.lookupTierIsDatasource For some things like indexing service tasks, the datasource is passed in the runtime properties of a task. This option fetches the tierName from the same value as the datasource for the task. It is suggested to only use this as Peon options for the indexing service, if at all. If true, druid.lookup.lookupTier MUST NOT be specified \"false\" To configure the behavior of the dynamic configuration manager, use the following properties on the Coordinator: Property Description Default druid.manager.lookups.hostTimeout Timeout (in ms) PER HOST for processing request 2000(2 seconds) druid.manager.lookups.allHostTimeout Timeout (in ms) to finish lookup management on all the processes. 900000(15 mins) druid.manager.lookups.period How long to pause between management cycles 120000(2 mins) druid.manager.lookups.threadPoolSize Number of service processes that can be managed concurrently 10 Saving configuration across restarts It is possible to save the configuration across restarts such that a process will not have to wait for Coordinator action to re-populate its lookups. To do this the following property is set: Property Description Default druid.lookup.snapshotWorkingDir Working path used to store snapshot of current lookup configuration, leaving this property null will disable snapshot/bootstrap utility null druid.lookup.enableLookupSyncOnStartup Enable the lookup synchronization process with Coordinator on startup. The queryable processes will fetch and load the lookups from the Coordinator instead of waiting for the Coordinator to load the lookups for them. Users may opt to disable this option if there are no lookups configured in the cluster. true druid.lookup.numLookupLoadingThreads Number of threads for loading the lookups in parallel on startup. This thread pool is destroyed once startup is done. It is not kept during the lifetime of the JVM Available Processors / 2 druid.lookup.coordinatorFetchRetries How many times to retry to fetch the lookup bean list from Coordinator, during the sync on startup. 3 druid.lookup.lookupStartRetries How many times to retry to start each lookup, either during the sync on startup, or during the runtime. 3 druid.lookup.coordinatorRetryDelay How long to delay (in millis) between retries to fetch lookup list from the Coordinator during the sync on startup. 60_000 Introspect a Lookup The Broker provides an API for lookup introspection if the lookup type implements a LookupIntrospectHandler. A GET request to /druid/v1/lookups/introspect/{lookupId} will return the map of complete values. ex: GET /druid/v1/lookups/introspect/nato-phonetic { \"A\": \"Alfa\", \"B\": \"Bravo\", \"C\": \"Charlie\", ... \"Y\": \"Yankee\", \"Z\": \"Zulu\", \"-\": \"Dash\" } The list of keys can be retrieved via GET to /druid/v1/lookups/introspect/{lookupId}/keys\" ex: GET /druid/v1/lookups/introspect/nato-phonetic/keys [ \"A\", \"B\", \"C\", ... \"Y\", \"Z\", \"-\" ] A GET request to /druid/v1/lookups/introspect/{lookupId}/values\" will return the list of values. ex: GET /druid/v1/lookups/introspect/nato-phonetic/values [ \"Alfa\", \"Bravo\", \"Charlie\", ... \"Yankee\", \"Zulu\", \"Dash\" ] Druid version 0.10.0 to 0.10.1 upgrade/downgrade Overall druid cluster lookups configuration is persisted in metadata store and also individual lookup processes optionally persist a snapshot of loaded lookups on disk. If upgrading from druid version 0.10.0 to 0.10.1, then migration for all persisted metadata is handled automatically. If downgrading from 0.10.1 to 0.9.0 then lookups updates done via Coordinator while 0.10.1 was running, would be lost. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/multi-value-dimensions.html":{"url":"querying/multi-value-dimensions.html","title":"结构化列值(multi-value-dimensions)","keywords":"","body":" Apache Druid supports \"multi-value\" string dimensions. These are generated when an input field contains an array of values instead of a single value (e.g. JSON arrays, or a TSV field containing one or more listDelimiter characters). This document describes the behavior of groupBy (topN has similar behavior) queries on multi-value dimensions when they are used as a dimension being grouped by. See the section on multi-value columns in segments for internal representation details. Examples in this document are in the form of native Druid queries. Refer to the Druid SQL documentation for details about using multi-value string dimensions in SQL. Querying multi-value dimensions Suppose, you have a dataSource with a segment that contains the following rows, with a multi-value dimension called tags. {\"timestamp\": \"2011-01-12T00:00:00.000Z\", \"tags\": [\"t1\",\"t2\",\"t3\"]} #row1 {\"timestamp\": \"2011-01-13T00:00:00.000Z\", \"tags\": [\"t3\",\"t4\",\"t5\"]} #row2 {\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": [\"t5\",\"t6\",\"t7\"]} #row3 {\"timestamp\": \"2011-01-14T00:00:00.000Z\", \"tags\": []} #row4 Filtering All query types, as well as filtered aggregators, can filter on multi-value dimensions. Filters follow these rules on multi-value dimensions: Value filters (like \"selector\", \"bound\", and \"in\") match a row if any of the values of a multi-value dimension match the filter. The Column Comparison filter will match a row if the dimensions have any overlap. Value filters that match null or \"\" (empty string) will match empty cells in a multi-value dimension. Logical expression filters behave the same way they do on single-value dimensions: \"and\" matches a row if all underlying filters match that row; \"or\" matches a row if any underlying filters match that row; \"not\" matches a row if the underlying filter does not match the row. For example, this \"or\" filter would match row1 and row2 of the dataset above, but not row3: { \"type\": \"or\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t1\" }, { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t3\" } ] } This \"and\" filter would match only row1 of the dataset above: { \"type\": \"and\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t1\" }, { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t3\" } ] } This \"selector\" filter would match row4 of the dataset above: { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": null } Grouping topN and groupBy queries can group on multi-value dimensions. When grouping on a multi-value dimension, all values from matching rows will be used to generate one group per value. This can be thought of as the equivalent to the UNNEST operator used on an ARRAY type that many SQL dialects support. This means it's possible for a query to return more groups than there are rows. For example, a topN on the dimension tags with filter \"t1\" AND \"t3\" would match only row1, and generate a result with three groups: t1, t2, and t3. If you only need to include values that match your filter, you can use a filtered dimensionSpec. This can also improve performance. Example: GroupBy query with no filtering See GroupBy querying for details. { \"queryType\": \"groupBy\", \"dataSource\": \"test\", \"intervals\": [ \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\" ], \"granularity\": { \"type\": \"all\" }, \"dimensions\": [ { \"type\": \"default\", \"dimension\": \"tags\", \"outputName\": \"tags\" } ], \"aggregations\": [ { \"type\": \"count\", \"name\": \"count\" } ] } returns following result. [ { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t1\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t2\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 2, \"tags\": \"t3\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t4\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 2, \"tags\": \"t5\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t6\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t7\" } } ] notice how original rows are \"exploded\" into multiple rows and merged. Example: GroupBy query with a selector query filter See query filters for details of selector query filter. { \"queryType\": \"groupBy\", \"dataSource\": \"test\", \"intervals\": [ \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\" ], \"filter\": { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t3\" }, \"granularity\": { \"type\": \"all\" }, \"dimensions\": [ { \"type\": \"default\", \"dimension\": \"tags\", \"outputName\": \"tags\" } ], \"aggregations\": [ { \"type\": \"count\", \"name\": \"count\" } ] } returns following result. [ { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t1\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t2\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 2, \"tags\": \"t3\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t4\" } }, { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 1, \"tags\": \"t5\" } } ] You might be surprised to see inclusion of \"t1\", \"t2\", \"t4\" and \"t5\" in the results. It happens because query filter is applied on the row before explosion. For multi-value dimensions, selector filter for \"t3\" would match row1 and row2, after which exploding is done. For multi-value dimensions, query filter matches a row if any individual value inside the multiple values matches the query filter. Example: GroupBy query with a selector query filter and additional filter in \"dimensions\" attributes To solve the problem above and to get only rows for \"t3\" returned, you would have to use a \"filtered dimension spec\" as in the query below. See section on filtered dimensionSpecs in dimensionSpecs for details. { \"queryType\": \"groupBy\", \"dataSource\": \"test\", \"intervals\": [ \"1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z\" ], \"filter\": { \"type\": \"selector\", \"dimension\": \"tags\", \"value\": \"t3\" }, \"granularity\": { \"type\": \"all\" }, \"dimensions\": [ { \"type\": \"listFiltered\", \"delegate\": { \"type\": \"default\", \"dimension\": \"tags\", \"outputName\": \"tags\" }, \"values\": [\"t3\"] } ], \"aggregations\": [ { \"type\": \"count\", \"name\": \"count\" } ] } returns the following result. [ { \"timestamp\": \"1970-01-01T00:00:00.000Z\", \"event\": { \"count\": 2, \"tags\": \"t3\" } } ] Note that, for groupBy queries, you could get similar result with a having spec but using a filtered dimensionSpec is much more efficient because that gets applied at the lowest level in the query processing pipeline. Having specs are applied at the outermost level of groupBy query processing. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/multitenancy.html":{"url":"querying/multitenancy.html","title":"多租户","keywords":"","body":" Apache Druid is often used to power user-facing data applications, where multitenancy is an important requirement. This document outlines Druid's multitenant storage and querying features. Shared datasources or datasource-per-tenant? A datasource is the Druid equivalent of a database table. Multitenant workloads can either use a separate datasource for each tenant, or can share one or more datasources between tenants using a \"tenant_id\" dimension. When deciding which path to go down, consider that each path has pros and cons. Pros of datasources per tenant: Each datasource can have its own schema, its own backfills, its own partitioning rules, and its own data loading and expiration rules. Queries can be faster since there will be fewer segments to examine for a typical tenant's query. You get the most flexibility. Pros of shared datasources: Each datasource requires its own JVMs for realtime indexing. Each datasource requires its own YARN resources for Hadoop batch jobs. Each datasource requires its own segment files on disk. For these reasons it can be wasteful to have a very large number of small datasources. One compromise is to use more than one datasource, but a smaller number than tenants. For example, you could have some tenants with partitioning rules A and some with partitioning rules B; you could use two datasources and split your tenants between them. Partitioning shared datasources If your multitenant cluster uses shared datasources, most of your queries will likely filter on a \"tenant_id\" dimension. These sorts of queries perform best when data is well-partitioned by tenant. There are a few ways to accomplish this. With batch indexing, you can use single-dimension partitioning to partition your data by tenant_id. Druid always partitions by time first, but the secondary partition within each time bucket will be on tenant_id. With realtime indexing, you'd do this by tweaking the stream you send to Druid. For example, if you're using Kafka then you can have your Kafka producer partition your topic by a hash of tenant_id. Customizing data distribution Druid additionally supports multitenancy by providing configurable means of distributing data. Druid's Historical processes can be configured into tiers, and rules can be set that determines which segments go into which tiers. One use case of this is that recent data tends to be accessed more frequently than older data. Tiering enables more recent segments to be hosted on more powerful hardware for better performance. A second copy of recent segments can be replicated on cheaper hardware (a different tier), and older segments can also be stored on this tier. Supporting high query concurrency Druid's fundamental unit of computation is a segment. Processes scan segments in parallel and a given process can scan druid.processing.numThreads concurrently. To process more data in parallel and increase performance, more cores can be added to a cluster. Druid segments should be sized such that any computation over any given segment should complete in at most 500ms. Druid internally stores requests to scan segments in a priority queue. If a given query requires scanning more segments than the total number of available processors in a cluster, and many similarly expensive queries are concurrently running, we don't want any query to be starved out. Druid's internal processing logic will scan a set of segments from one query and release resources as soon as the scans complete. This allows for a second set of segments from another query to be scanned. By keeping segment computation time very small, we ensure that resources are constantly being yielded, and segments pertaining to different queries are all being processed. Druid queries can optionally set a priority flag in the query context. Queries known to be slow (download or reporting style queries) can be de-prioritized and more interactive queries can have higher priority. Broker processes can also be dedicated to a given tier. For example, one set of Broker processes can be dedicated to fast interactive queries, and a second set of Broker processes can be dedicated to slower reporting queries. Druid also provides a Router process that can route queries to different Brokers based on various query parameters (datasource, interval, etc.). 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/caching.html":{"url":"querying/caching.html","title":"查询缓存","keywords":"","body":" Apache Druid supports query result caching at both the segment and whole-query result level. Cache data can be stored in the local JVM heap or in an external distributed key/value store. In all cases, the Druid cache is a query result cache. The only difference is whether the result is a partial result for a particular segment, or the result for an entire query. In both cases, the cache is invalidated as soon as any underlying data changes; it will never return a stale result. Segment-level caching allows the cache to be leveraged even when some of the underling segments are mutable and undergoing real-time ingestion. In this case, Druid will potentially cache query results for immutable historical segments, while re-computing results for the real-time segments on each query. Whole-query result level caching is not useful in this scenario, since it would be continuously invalidated. Segment-level caching does require Druid to merge the per-segment results on each query, even when they are served from the cache. For this reason, whole-query result level caching can be more efficient if invalidation due to real-time ingestion is not an issue. Using and populating cache All caches have a pair of parameters that control the behavior of how individual queries interact with the cache, a 'use' cache parameter, and a 'populate' cache parameter. These settings must be enabled at the service level via runtime properties to utilize cache, but can be controlled on a per query basis by setting them on the query context. The 'use' parameter obviously controls if a query will utilize cached results. The 'populate' parameter controls if a query will update cached results. These are separate parameters to allow queries on uncommon data to utilize cached results without polluting the cache with results that are unlikely to be re-used by other queries, for example large reports or very old data. Query caching on Brokers Brokers support both segment-level and whole-query result level caching. Segment-level caching is controlled by the parameters useCache and populateCache. Whole-query result level caching is controlled by the parameters useResultLevelCache and populateResultLevelCache and runtime properties druid.broker.cache.*. Enabling segment-level caching on the Broker can yield faster results than if query caches were enabled on Historicals for small clusters. This is the recommended setup for smaller production clusters (not recommended for large production clusters, since when the property druid.broker.cache.populateCache is set to true (and query context parameter populateCache is not set to false), results from Historicals are returned on a per segment basis, and Historicals will not be able to do any local result merging. This impairs the ability of the Druid cluster to scale well. Query caching on Historicals Historicals only support segment-level caching. Segment-level caching is controlled by the query context parameters useCache and populateCache and runtime properties druid.historical.cache.*. Larger production clusters should enable segment-level cache population on Historicals only (not on Brokers) to avoid having to use Brokers to merge all query results. Enabling cache population on the Historicals instead of the Brokers enables the Historicals to do their own local result merging and puts less strain on the Brokers. Query caching on Ingestion Tasks Task executor processes such as the Peon or the experimental Indexer only support segment-level caching. Segment-level caching is controlled by the query context parameters useCache and populateCache and runtime properties druid.realtime.cache.*. Larger production clusters should enable segment-level cache population on task execution processes only (not on Brokers) to avoid having to use Brokers to merge all query results. Enabling cache population on the task execution processes instead of the Brokers enables the task execution processes to do their own local result merging and puts less strain on the Brokers. Note that the task executor processes only support caches that keep their data locally, such as the caffeine cache. This restriction exists because the cache stores results at the level of intermediate partial segments generated by the ingestion tasks. These intermediate partial segments will not necessarily be identical across task replicas, so remote cache types such as memcached will be ignored by task executor processes. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/query-context.html":{"url":"querying/query-context.html","title":"查询上下文","keywords":"","body":" General parameters The query context is used for various query configuration parameters. Query context parameters can be specified in the following ways: For Druid SQL, context parameters are provided either as a JSON object named context to the HTTP POST API, or as properties to the JDBC connection. For native queries, context parameters are provided as a JSON object named context. Note that setting query context will override both the default value and the runtime properties value in the format of druid.query.default.context.{property_key} (if set). These parameters apply to all query types. property default description timeout druid.server.http.defaultQueryTimeout Query timeout in millis, beyond which unfinished queries will be cancelled. 0 timeout means no timeout. To set the default timeout, see Broker configuration priority 0 Query Priority. Queries with higher priority get precedence for computational resources. lane null Query lane, used to control usage limits on classes of queries. See Broker configuration for more details. queryId auto-generated Unique identifier given to this query. If a query ID is set or known, this can be used to cancel the query useCache true Flag indicating whether to leverage the query cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Apache Druid uses druid.broker.cache.useCache or druid.historical.cache.useCache to determine whether or not to read from the query cache populateCache true Flag indicating whether to save the results of the query to the query cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateCache or druid.historical.cache.populateCache to determine whether or not to save the results of this query to the query cache useResultLevelCache true Flag indicating whether to leverage the result level cache for this query. When set to false, it disables reading from the query cache for this query. When set to true, Druid uses druid.broker.cache.useResultLevelCache to determine whether or not to read from the result-level query cache populateResultLevelCache true Flag indicating whether to save the results of the query to the result level cache. Primarily used for debugging. When set to false, it disables saving the results of this query to the query cache. When set to true, Druid uses druid.broker.cache.populateResultLevelCache to determine whether or not to save the results of this query to the result-level query cache bySegment false Return \"by segment\" results. Primarily used for debugging, setting it to true returns results associated with the data segment they came from finalize true Flag indicating whether to \"finalize\" aggregation results. Primarily used for debugging. For instance, the hyperUnique aggregator will return the full HyperLogLog sketch instead of the estimated cardinality when this flag is set to false maxScatterGatherBytes druid.server.http.maxScatterGatherBytes Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. This parameter can be used to further reduce maxScatterGatherBytes limit at query time. See Broker configuration for more details. maxQueuedBytes druid.broker.http.maxQueuedBytes Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to maxScatterGatherBytes, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled. serializeDateTimeAsLong false If true, DateTime is serialized as long in the result returned by Broker and the data transportation between Broker and compute process serializeDateTimeAsLongInner false If true, DateTime is serialized as long in the data transportation between Broker and compute process enableParallelMerge true Enable parallel result merging on the Broker. Note that druid.processing.merge.useParallelMergePool must be enabled for this setting to be set to true. See Broker configuration for more details. parallelMergeParallelism druid.processing.merge.pool.parallelism Maximum number of parallel threads to use for parallel result merging on the Broker. See Broker configuration for more details. parallelMergeInitialYieldRows druid.processing.merge.task.initialYieldNumRows Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See Broker configuration for more details. parallelMergeSmallBatchRows druid.processing.merge.task.smallBatchNumRows Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See Broker configuration for more details. useFilterCNF false If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects. secondaryPartitionPruning true Enable secondary partition pruning on the Broker. The Broker will always prune unnecessary segments from the input scan based on a filter on time intervals, but if the data is further partitioned with hash or range partitioning, this option will enable additional pruning based on a filter on secondary partition dimensions. Query-type-specific parameters In addition, some query types offer context parameters specific to that query type. TopN property default description minTopNThreshold 1000 The top minTopNThreshold local results from each segment are returned for merging to determine the global topN. Timeseries property default description skipEmptyBuckets false Disable timeseries zero-filling behavior, so only buckets with results will be returned. GroupBy See the list of GroupBy query context parameters available on the groupBy query page. Vectorization parameters The GroupBy and Timeseries query types can run in vectorized mode, which speeds up query execution by processing batches of rows at a time. Not all queries can be vectorized. In particular, vectorization currently has the following requirements: All query-level filters must either be able to run on bitmap indexes or must offer vectorized row-matchers. These include \"selector\", \"bound\", \"in\", \"like\", \"regex\", \"search\", \"and\", \"or\", and \"not\". All filters in filtered aggregators must offer vectorized row-matchers. All aggregators must offer vectorized implementations. These include \"count\", \"doubleSum\", \"floatSum\", \"longSum\", \"longMin\", \"longMax\", \"doubleMin\", \"doubleMax\", \"floatMin\", \"floatMax\", \"longAny\", \"doubleAny\", \"floatAny\", \"stringAny\", \"hyperUnique\", \"filtered\", \"approxHistogram\", \"approxHistogramFold\", and \"fixedBucketsHistogram\" (with numerical input). All virtual columns must offer vectorized implementations. Currently for expression virtual columns, support for vectorization is decided on a per expression basis, depending on the type of input and the functions used by the expression. See the currently supported list in the expression documentation. For GroupBy: All dimension specs must be \"default\" (no extraction functions or filtered dimension specs). For GroupBy: No multi-value dimensions. For Timeseries: No \"descending\" order. Only immutable segments (not real-time). Only table datasources (not joins, subqueries, lookups, or inline datasources). Other query types (like TopN, Scan, Select, and Search) ignore the \"vectorize\" parameter, and will execute without vectorization. These query types will ignore the \"vectorize\" parameter even if it is set to \"force\". property default description vectorize true Enables or disables vectorized query execution. Possible values are false (disabled), true (enabled if possible, disabled otherwise, on a per-segment basis), and force (enabled, and groupBy or timeseries queries that cannot be vectorized will fail). The \"force\" setting is meant to aid in testing, and is not generally useful in production (since real-time segments can never be processed with vectorized execution, any queries on real-time data will fail). This will override druid.query.default.context.vectorize if it's set. vectorSize 512 Sets the row batching size for a particular query. This will override druid.query.default.context.vectorSize if it's set. vectorizeVirtualColumns false Enables or disables vectorized query processing of queries with virtual columns, layered on top of vectorize (vectorize must also be set to true for a query to utilize vectorization). Possible values are false (disabled), true (enabled if possible, disabled otherwise, on a per-segment basis), and force (enabled, and groupBy or timeseries queries with virtual columns that cannot be vectorized will fail). The \"force\" setting is meant to aid in testing, and is not generally useful in production. This will override druid.query.default.context.vectorizeVirtualColumns if it's set. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/timeseriesquery.html":{"url":"querying/timeseriesquery.html","title":"Timeseries","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type in the native language. For information about when Druid SQL will use this query type, refer to the SQL documentation. These types of queries take a timeseries query object and return an array of JSON objects where each object represents a value asked for by the timeseries query. An example timeseries query object is shown below: { \"queryType\": \"timeseries\", \"dataSource\": \"sample_datasource\", \"granularity\": \"day\", \"descending\": \"true\", \"filter\": { \"type\": \"and\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"sample_dimension1\", \"value\": \"sample_value1\" }, { \"type\": \"or\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"sample_dimension2\", \"value\": \"sample_value2\" }, { \"type\": \"selector\", \"dimension\": \"sample_dimension3\", \"value\": \"sample_value3\" } ] } ] }, \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }, { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" } ], \"postAggregations\": [ { \"type\": \"arithmetic\", \"name\": \"sample_divide\", \"fn\": \"/\", \"fields\": [ { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name1\", \"fieldName\": \"sample_name1\" }, { \"type\": \"fieldAccess\", \"name\": \"postAgg__sample_name2\", \"fieldName\": \"sample_name2\" } ] } ], \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ] } There are 7 main parts to a timeseries query: property description required? queryType This String should always be \"timeseries\"; this is the first thing Apache Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes descending Whether to make descending ordered result. Default is false(ascending). no intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. yes granularity Defines the granularity to bucket query results. See Granularities yes filter See Filters no aggregations See Aggregations no postAggregations See Post Aggregations no limit An integer that limits the number of results. The default is unlimited. no context Can be used to modify query behavior, including grand totals and zero-filling. See also Context for parameters that apply to all query types. no To pull it all together, the above query would return 2 data points, one for each day between 2012-01-01 and 2012-01-03, from the \"sample_datasource\" table. Each data point would be the (long) sum of sample_fieldName1, the (double) sum of sample_fieldName2 and the (double) result of sample_fieldName1 divided by sample_fieldName2 for the filter set. The output looks like this: [ { \"timestamp\": \"2012-01-01T00:00:00.000Z\", \"result\": { \"sample_name1\": , \"sample_name2\": , \"sample_divide\": } }, { \"timestamp\": \"2012-01-02T00:00:00.000Z\", \"result\": { \"sample_name1\": , \"sample_name2\": , \"sample_divide\": } } ] Grand totals Druid can include an extra \"grand totals\" row as the last row of a timeseries result set. To enable this, add \"grandTotal\" : true to your query context. For example: { \"queryType\": \"timeseries\", \"dataSource\": \"sample_datasource\", \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ], \"granularity\": \"day\", \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" }, { \"type\": \"doubleSum\", \"name\": \"sample_name2\", \"fieldName\": \"sample_fieldName2\" } ], \"context\": { \"grandTotal\": true } } The grand totals row will appear as the last row in the result array, and will have no timestamp. It will be the last row even if the query is run in \"descending\" mode. Post-aggregations in the grand totals row will be computed based upon the grand total aggregations. Zero-filling Timeseries queries normally fill empty interior time buckets with zeroes. For example, if you issue a \"day\" granularity timeseries query for the interval 2012-01-01/2012-01-04, and no data exists for 2012-01-02, you will receive: [ { \"timestamp\": \"2012-01-01T00:00:00.000Z\", \"result\": { \"sample_name1\": } }, { \"timestamp\": \"2012-01-02T00:00:00.000Z\", \"result\": { \"sample_name1\": 0 } }, { \"timestamp\": \"2012-01-03T00:00:00.000Z\", \"result\": { \"sample_name1\": } } ] Time buckets that lie completely outside the data interval are not zero-filled. You can disable all zero-filling with the context flag \"skipEmptyBuckets\". In this mode, the data point for 2012-01-02 would be omitted from the results. A query with this context flag set would look like: { \"queryType\": \"timeseries\", \"dataSource\": \"sample_datasource\", \"granularity\": \"day\", \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"sample_name1\", \"fieldName\": \"sample_fieldName1\" } ], \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-04T00:00:00.000\" ], \"context\" : { \"skipEmptyBuckets\": \"true\" } } 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/topnquery.html":{"url":"querying/topnquery.html","title":"TopN","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type in the native language. For information about when Druid SQL will use this query type, refer to the SQL documentation. Apache Druid TopN queries return a sorted set of results for the values in a given dimension according to some criteria. Conceptually, they can be thought of as an approximate GroupByQuery over a single dimension with an Ordering spec. TopNs are much faster and resource efficient than GroupBys for this use case. These types of queries take a topN query object and return an array of JSON objects where each object represents a value asked for by the topN query. TopNs are approximate in that each data process will rank their top K results and only return those top K results to the Broker. K, by default in Druid, is max(1000, threshold). In practice, this means that if you ask for the top 1000 items ordered, the correctness of the first ~900 items will be 100%, and the ordering of the results after that is not guaranteed. TopNs can be made more accurate by increasing the threshold. A topN query object looks like: { \"queryType\": \"topN\", \"dataSource\": \"sample_data\", \"dimension\": \"sample_dim\", \"threshold\": 5, \"metric\": \"count\", \"granularity\": \"all\", \"filter\": { \"type\": \"and\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"dim1\", \"value\": \"some_value\" }, { \"type\": \"selector\", \"dimension\": \"dim2\", \"value\": \"some_other_val\" } ] }, \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"count\", \"fieldName\": \"count\" }, { \"type\": \"doubleSum\", \"name\": \"some_metric\", \"fieldName\": \"some_metric\" } ], \"postAggregations\": [ { \"type\": \"arithmetic\", \"name\": \"average\", \"fn\": \"/\", \"fields\": [ { \"type\": \"fieldAccess\", \"name\": \"some_metric\", \"fieldName\": \"some_metric\" }, { \"type\": \"fieldAccess\", \"name\": \"count\", \"fieldName\": \"count\" } ] } ], \"intervals\": [ \"2013-08-31T00:00:00.000/2013-09-03T00:00:00.000\" ] } There are 11 parts to a topN query. property description required? queryType This String should always be \"topN\"; this is the first thing Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. yes granularity Defines the granularity to bucket query results. See Granularities yes filter See Filters no aggregations See Aggregations for numeric metricSpec, aggregations or postAggregations should be specified. Otherwise no. postAggregations See Post Aggregations for numeric metricSpec, aggregations or postAggregations should be specified. Otherwise no. dimension A String or JSON object defining the dimension that you want the top taken for. For more info, see DimensionSpecs yes threshold An integer defining the N in the topN (i.e. how many results you want in the top list) yes metric A String or JSON object specifying the metric to sort by for the top list. For more info, see TopNMetricSpec. yes context See Context no Please note the context JSON object is also available for topN queries and should be used with the same caution as the timeseries case. The format of the results would look like so: [ { \"timestamp\": \"2013-08-31T00:00:00.000Z\", \"result\": [ { \"dim1\": \"dim1_val\", \"count\": 111, \"some_metrics\": 10669, \"average\": 96.11711711711712 }, { \"dim1\": \"another_dim1_val\", \"count\": 88, \"some_metrics\": 28344, \"average\": 322.09090909090907 }, { \"dim1\": \"dim1_val3\", \"count\": 70, \"some_metrics\": 871, \"average\": 12.442857142857143 }, { \"dim1\": \"dim1_val4\", \"count\": 62, \"some_metrics\": 815, \"average\": 13.14516129032258 }, { \"dim1\": \"dim1_val5\", \"count\": 60, \"some_metrics\": 2787, \"average\": 46.45 } ] } ] Behavior on multi-value dimensions topN queries can group on multi-value dimensions. When grouping on a multi-value dimension, all values from matching rows will be used to generate one group per value. It's possible for a query to return more groups than there are rows. For example, a topN on the dimension tags with filter \"t1\" AND \"t3\" would match only row1, and generate a result with three groups: t1, t2, and t3. If you only need to include values that match your filter, you can use a filtered dimensionSpec. This can also improve performance. See Multi-value dimensions for more details. Aliasing The current TopN algorithm is an approximate algorithm. The top 1000 local results from each segment are returned for merging to determine the global topN. As such, the topN algorithm is approximate in both rank and results. Approximate results ONLY APPLY WHEN THERE ARE MORE THAN 1000 DIM VALUES. A topN over a dimension with fewer than 1000 unique dimension values can be considered accurate in rank and accurate in aggregates. The threshold can be modified from it's default 1000 via the server parameter druid.query.topN.minTopNThreshold which need to restart servers to take effect or set minTopNThreshold in query context which take effect per query. If you are wanting the top 100 of a high cardinality, uniformly distributed dimension ordered by some low-cardinality, uniformly distributed dimension, you are potentially going to get aggregates back that are missing data. To put it another way, the best use cases for topN are when you can have confidence that the overall results are uniformly in the top. For example, if a particular site ID is in the top 10 for some metric for every hour of every day, then it will probably be accurate in the topN over multiple days. But if a site barely in the top 1000 for any given hour, but over the whole query granularity is in the top 500 (example: a site which gets highly uniform traffic co-mingling in the dataset with sites with highly periodic data), then a top500 query may not have that particular site a the exact rank, and may not be accurate for that particular site's aggregates. Before continuing in this section, please consider if you really need exact results. Getting exact results is a very resource intensive process. For the vast majority of \"useful\" data results, an approximate topN algorithm supplies plenty of accuracy. Users wishing to get an exact rank and exact aggregates topN over a dimension with greater than 1000 unique values should issue a groupBy query and sort the results themselves. This is very computationally expensive for high-cardinality dimensions. Users who can tolerate approximate rank topN over a dimension with greater than 1000 unique values, but require exact aggregates can issue two queries. One to get the approximate topN dimension values, and another topN with dimension selection filters which only use the topN results of the first. Example First query { \"aggregations\": [ { \"fieldName\": \"L_QUANTITY_longSum\", \"name\": \"L_QUANTITY_\", \"type\": \"longSum\" } ], \"dataSource\": \"tpch_year\", \"dimension\":\"l_orderkey\", \"granularity\": \"all\", \"intervals\": [ \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\" ], \"metric\": \"L_QUANTITY_\", \"queryType\": \"topN\", \"threshold\": 2 } Example second query { \"aggregations\": [ { \"fieldName\": \"L_TAX_doubleSum\", \"name\": \"L_TAX_\", \"type\": \"doubleSum\" }, { \"fieldName\": \"L_DISCOUNT_doubleSum\", \"name\": \"L_DISCOUNT_\", \"type\": \"doubleSum\" }, { \"fieldName\": \"L_EXTENDEDPRICE_doubleSum\", \"name\": \"L_EXTENDEDPRICE_\", \"type\": \"doubleSum\" }, { \"fieldName\": \"L_QUANTITY_longSum\", \"name\": \"L_QUANTITY_\", \"type\": \"longSum\" }, { \"name\": \"count\", \"type\": \"count\" } ], \"dataSource\": \"tpch_year\", \"dimension\":\"l_orderkey\", \"filter\": { \"fields\": [ { \"dimension\": \"l_orderkey\", \"type\": \"selector\", \"value\": \"103136\" }, { \"dimension\": \"l_orderkey\", \"type\": \"selector\", \"value\": \"1648672\" } ], \"type\": \"or\" }, \"granularity\": \"all\", \"intervals\": [ \"1900-01-09T00:00:00.000Z/2992-01-10T00:00:00.000Z\" ], \"metric\": \"L_QUANTITY_\", \"queryType\": \"topN\", \"threshold\": 2 } 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/groupbyquery.html":{"url":"querying/groupbyquery.html","title":"GroupBy","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type in the native language. For information about when Druid SQL will use this query type, refer to the SQL documentation. These types of Apache Druid queries take a groupBy query object and return an array of JSON objects where each object represents a grouping asked for by the query. Note: If you are doing aggregations with time as your only grouping, or an ordered groupBy over a single dimension, consider Timeseries and TopN queries as well as groupBy. Their performance may be better in some cases. See Alternatives below for more details. An example groupBy query object is shown below: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", \"granularity\": \"day\", \"dimensions\": [\"country\", \"device\"], \"limitSpec\": { \"type\": \"default\", \"limit\": 5000, \"columns\": [\"country\", \"data_transfer\"] }, \"filter\": { \"type\": \"and\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"carrier\", \"value\": \"AT&T\" }, { \"type\": \"or\", \"fields\": [ { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Apple\" }, { \"type\": \"selector\", \"dimension\": \"make\", \"value\": \"Samsung\" } ] } ] }, \"aggregations\": [ { \"type\": \"longSum\", \"name\": \"total_usage\", \"fieldName\": \"user_count\" }, { \"type\": \"doubleSum\", \"name\": \"data_transfer\", \"fieldName\": \"data_transfer\" } ], \"postAggregations\": [ { \"type\": \"arithmetic\", \"name\": \"avg_usage\", \"fn\": \"/\", \"fields\": [ { \"type\": \"fieldAccess\", \"fieldName\": \"data_transfer\" }, { \"type\": \"fieldAccess\", \"fieldName\": \"total_usage\" } ] } ], \"intervals\": [ \"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\" ], \"having\": { \"type\": \"greaterThan\", \"aggregation\": \"total_usage\", \"value\": 100 } } Following are main parts to a groupBy query: property description required? queryType This String should always be \"groupBy\"; this is the first thing Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes dimensions A JSON list of dimensions to do the groupBy over; or see DimensionSpec for ways to extract dimensions. yes limitSpec See LimitSpec. no having See Having. no granularity Defines the granularity of the query. See Granularities yes filter See Filters no aggregations See Aggregations no postAggregations See Post Aggregations no intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. yes subtotalsSpec A JSON array of arrays to return additional result sets for groupings of subsets of top level dimensions. It is described later in more detail. no context An additional JSON Object which can be used to specify certain flags. no To pull it all together, the above query would return n\\m* data points, up to a maximum of 5000 points, where n is the cardinality of the country dimension, m is the cardinality of the device dimension, each day between 2012-01-01 and 2012-01-03, from the sample_datasource table. Each data point contains the (long) sum of total_usage if the value of the data point is greater than 100, the (double) sum of data_transfer and the (double) result of total_usage divided by data_transfer for the filter set for a particular grouping of country and device. The output looks like this: [ { \"version\" : \"v1\", \"timestamp\" : \"2012-01-01T00:00:00.000Z\", \"event\" : { \"country\" : , \"device\" : , \"total_usage\" : , \"data_transfer\" :, \"avg_usage\" : } }, { \"version\" : \"v1\", \"timestamp\" : \"2012-01-01T00:00:12.000Z\", \"event\" : { \"dim1\" : , \"dim2\" : , \"sample_name1\" : , \"sample_name2\" :, \"avg_usage\" : } }, ... ] Behavior on multi-value dimensions groupBy queries can group on multi-value dimensions. When grouping on a multi-value dimension, all values from matching rows will be used to generate one group per value. It's possible for a query to return more groups than there are rows. For example, a groupBy on the dimension tags with filter \"t1\" AND \"t3\" would match only row1, and generate a result with three groups: t1, t2, and t3. If you only need to include values that match your filter, you can use a filtered dimensionSpec. This can also improve performance. See Multi-value dimensions for more details. More on subtotalsSpec The subtotals feature allows computation of multiple sub-groupings in a single query. To use this feature, add a \"subtotalsSpec\" to your query as a list of subgroup dimension sets. It should contain the outputName from dimensions in your dimensions attribute, in the same order as they appear in the dimensions attribute (although, of course, you may skip some). For example, consider a groupBy query like this one: { \"type\": \"groupBy\", ... ... \"dimensions\": [ { \"type\" : \"default\", \"dimension\" : \"d1col\", \"outputName\": \"D1\" }, { \"type\" : \"extraction\", \"dimension\" : \"d2col\", \"outputName\" : \"D2\", \"extractionFn\" : extraction_func }, { \"type\":\"lookup\", \"dimension\":\"d3col\", \"outputName\":\"D3\", \"name\":\"my_lookup\" } ], ... ... \"subtotalsSpec\":[ [\"D1\", \"D2\", D3\"], [\"D1\", \"D3\"], [\"D3\"]], .. } The result of the subtotalsSpec would be equivalent to concatenating the result of three groupBy queries, with the \"dimensions\" field being [\"D1\", \"D2\", D3\"], [\"D1\", \"D3\"] and [\"D3\"], given the DimensionSpec shown above. The response for the query above would look something like: [ { \"version\" : \"v1\", \"timestamp\" : \"t1\", \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" } } }, { \"version\" : \"v1\", \"timestamp\" : \"t2\", \"event\" : { \"D1\": \"..\", \"D2\": \"..\", \"D3\": \"..\" } } }, ... ... { \"version\" : \"v1\", \"timestamp\" : \"t1\", \"event\" : { \"D1\": \"..\", \"D2\": null, \"D3\": \"..\" } } }, { \"version\" : \"v1\", \"timestamp\" : \"t2\", \"event\" : { \"D1\": \"..\", \"D2\": null, \"D3\": \"..\" } } }, ... ... { \"version\" : \"v1\", \"timestamp\" : \"t1\", \"event\" : { \"D1\": null, \"D2\": null, \"D3\": \"..\" } } }, { \"version\" : \"v1\", \"timestamp\" : \"t2\", \"event\" : { \"D1\": null, \"D2\": null, \"D3\": \"..\" } } }, ... ] Notice that dimensions that are not included in an individual subtotalsSpec grouping are returned with a null value. This response format represents a behavior change as of Apache Druid 0.18.0. In release 0.17.0 and earlier, such dimensions were entirely excluded from the result. If you were relying on this old behavior to determine whether a particular dimension was not part of a subtotal grouping, you can now use Grouping aggregator instead. Implementation details Strategies GroupBy queries can be executed using two different strategies. The default strategy for a cluster is determined by the \"druid.query.groupBy.defaultStrategy\" runtime property on the Broker. This can be overridden using \"groupByStrategy\" in the query context. If neither the context field nor the property is set, the \"v2\" strategy will be used. \"v2\", the default, is designed to offer better performance and memory management. This strategy generates per-segment results using a fully off-heap map. Data processes merge the per-segment results using a fully off-heap concurrent facts map combined with an on-heap string dictionary. This may optionally involve spilling to disk. Data processes return sorted results to the Broker, which merges result streams using an N-way merge. The broker materializes the results if necessary (e.g. if the query sorts on columns other than its dimensions). Otherwise, it streams results back as they are merged. \"v1\", a legacy engine, generates per-segment results on data processes (Historical, realtime, MiddleManager) using a map which is partially on-heap (dimension keys and the map itself) and partially off-heap (the aggregated values). Data processes then merge the per-segment results using Druid's indexing mechanism. This merging is multi-threaded by default, but can optionally be single-threaded. The Broker merges the final result set using Druid's indexing mechanism again. The broker merging is always single-threaded. Because the Broker merges results using the indexing mechanism, it must materialize the full result set before returning any results. On both the data processes and the Broker, the merging index is fully on-heap by default, but it can optionally store aggregated values off-heap. Differences between v1 and v2 Query API and results are compatible between the two engines; however, there are some differences from a cluster configuration perspective: groupBy v1 controls resource usage using a row-based limit (maxResults) whereas groupBy v2 uses bytes-based limits. In addition, groupBy v1 merges results on-heap, whereas groupBy v2 merges results off-heap. These factors mean that memory tuning and resource limits behave differently between v1 and v2. In particular, due to this, some queries that can complete successfully in one engine may exceed resource limits and fail with the other engine. See the \"Memory tuning and resource limits\" section for more details. groupBy v1 imposes no limit on the number of concurrently running queries, whereas groupBy v2 controls memory usage by using a finite-sized merge buffer pool. By default, the number of merge buffers is 1/4 the number of processing threads. You can adjust this as necessary to balance concurrency and memory usage. groupBy v1 supports caching on either the Broker or Historical processes, whereas groupBy v2 only supports caching on Historical processes. groupBy v2 supports both array-based aggregation and hash-based aggregation. The array-based aggregation is used only when the grouping key is a single indexed string column. In array-based aggregation, the dictionary-encoded value is used as the index, so the aggregated values in the array can be accessed directly without finding buckets based on hashing. Memory tuning and resource limits When using groupBy v2, three parameters control resource usage and limits: druid.processing.buffer.sizeBytes: size of the off-heap hash table used for aggregation, per query, in bytes. At most druid.processing.numMergeBuffers of these will be created at once, which also serves as an upper limit on the number of concurrently running groupBy queries. druid.query.groupBy.maxMergingDictionarySize: size of the on-heap dictionary used when grouping on strings, per query, in bytes. Note that this is based on a rough estimate of the dictionary size, not the actual size. druid.query.groupBy.maxOnDiskStorage: amount of space on disk used for aggregation, per query, in bytes. By default, this is 0, which means aggregation will not use disk. If maxOnDiskStorage is 0 (the default) then a query that exceeds either the on-heap dictionary limit, or the off-heap aggregation table limit, will fail with a \"Resource limit exceeded\" error describing the limit that was exceeded. If maxOnDiskStorage is greater than 0, queries that exceed the in-memory limits will start using disk for aggregation. In this case, when either the on-heap dictionary or off-heap hash table fills up, partially aggregated records will be sorted and flushed to disk. Then, both in-memory structures will be cleared out for further aggregation. Queries that then go on to exceed maxOnDiskStorage will fail with a \"Resource limit exceeded\" error indicating that they ran out of disk space. With groupBy v2, cluster operators should make sure that the off-heap hash tables and on-heap merging dictionaries will not exceed available memory for the maximum possible concurrent query load (given by druid.processing.numMergeBuffers). See the basic cluster tuning guide for more details about direct memory usage, organized by Druid process type. Brokers do not need merge buffers for basic groupBy queries. Queries with subqueries (using a query dataSource) require one merge buffer if there is a single subquery, or two merge buffers if there is more than one layer of nested subqueries. Queries with subtotals need one merge buffer. These can stack on top of each other: a groupBy query with multiple layers of nested subqueries, and that also uses subtotals, will need three merge buffers. Historicals and ingestion tasks need one merge buffer for each groupBy query, unless parallel combination is enabled, in which case they need two merge buffers per query. When using groupBy v1, all aggregation is done on-heap, and resource limits are done through the parameter druid.query.groupBy.maxResults. This is a cap on the maximum number of results in a result set. Queries that exceed this limit will fail with a \"Resource limit exceeded\" error indicating they exceeded their row limit. Cluster operators should make sure that the on-heap aggregations will not exceed available JVM heap space for the expected concurrent query load. Performance tuning for groupBy v2 Limit pushdown optimization Druid pushes down the limit spec in groupBy queries to the segments on Historicals wherever possible to early prune unnecessary intermediate results and minimize the amount of data transferred to Brokers. By default, this technique is applied only when all fields in the orderBy spec is a subset of the grouping keys. This is because the limitPushDown doesn't guarantee the exact results if the orderBy spec includes any fields that are not in the grouping keys. However, you can enable this technique even in such cases if you can sacrifice some accuracy for fast query processing like in topN queries. See forceLimitPushDown in advanced groupBy v2 configurations. Optimizing hash table The groupBy v2 engine uses an open addressing hash table for aggregation. The hash table is initialized with a given initial bucket number and gradually grows on buffer full. On hash collisions, the linear probing technique is used. The default number of initial buckets is 1024 and the default max load factor of the hash table is 0.7. If you can see too many collisions in the hash table, you can adjust these numbers. See bufferGrouperInitialBuckets and bufferGrouperMaxLoadFactor in Advanced groupBy v2 configurations. Parallel combine Once a Historical finishes aggregation using the hash table, it sorts the aggregated results and merges them before sending to the Broker for N-way merge aggregation in the broker. By default, Historicals use all their available processing threads (configured by druid.processing.numThreads) for aggregation, but use a single thread for sorting and merging aggregates which is an http thread to send data to Brokers. This is to prevent some heavy groupBy queries from blocking other queries. In Druid, the processing threads are shared between all submitted queries and they are not interruptible. It means, if a heavy query takes all available processing threads, all other queries might be blocked until the heavy query is finished. GroupBy queries usually take longer time than timeseries or topN queries, they should release processing threads as soon as possible. However, you might care about the performance of some really heavy groupBy queries. Usually, the performance bottleneck of heavy groupBy queries is merging sorted aggregates. In such cases, you can use processing threads for it as well. This is called parallel combine. To enable parallel combine, see numParallelCombineThreads in Advanced groupBy v2 configurations. Note that parallel combine can be enabled only when data is actually spilled (see Memory tuning and resource limits). Once parallel combine is enabled, the groupBy v2 engine can create a combining tree for merging sorted aggregates. Each intermediate node of the tree is a thread merging aggregates from the child nodes. The leaf node threads read and merge aggregates from hash tables including spilled ones. Usually, leaf processes are slower than intermediate nodes because they need to read data from disk. As a result, less threads are used for intermediate nodes by default. You can change the degree of intermediate nodes. See intermediateCombineDegree in Advanced groupBy v2 configurations. Please note that each Historical needs two merge buffers to process a groupBy v2 query with parallel combine: one for computing intermediate aggregates from each segment and another for combining intermediate aggregates in parallel. Alternatives There are some situations where other query types may be a better choice than groupBy. For queries with no \"dimensions\" (i.e. grouping by time only) the Timeseries query will generally be faster than groupBy. The major differences are that it is implemented in a fully streaming manner (taking advantage of the fact that segments are already sorted on time) and does not need to use a hash table for merging. For queries with a single \"dimensions\" element (i.e. grouping by one string dimension), the TopN query will sometimes be faster than groupBy. This is especially true if you are ordering by a metric and find approximate results acceptable. Nested groupBys Nested groupBys (dataSource of type \"query\") are performed differently for \"v1\" and \"v2\". The Broker first runs the inner groupBy query in the usual way. \"v1\" strategy then materializes the inner query's results on-heap with Druid's indexing mechanism, and runs the outer query on these materialized results. \"v2\" strategy runs the outer query on the inner query's results stream with off-heap fact map and on-heap string dictionary that can spill to disk. Both strategy perform the outer query on the Broker in a single-threaded fashion. Configurations This section describes the configurations for groupBy queries. You can set the runtime properties in the runtime.properties file on Broker, Historical, and MiddleManager processes. You can set the query context parameters through the query context. Configurations for groupBy v2 Supported runtime properties: Property Description Default druid.query.groupBy.maxMergingDictionarySize Maximum amount of heap space (approximately) to use for the string dictionary during merging. When the dictionary exceeds this size, a spill to disk will be triggered. 100000000 druid.query.groupBy.maxOnDiskStorage Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling. 0 (disabled) Supported query contexts: Key Description maxMergingDictionarySize Can be used to lower the value of druid.query.groupBy.maxMergingDictionarySize for this query. maxOnDiskStorage Can be used to lower the value of druid.query.groupBy.maxOnDiskStorage for this query. Advanced configurations Common configurations for all groupBy strategies Supported runtime properties: Property Description Default druid.query.groupBy.defaultStrategy Default groupBy query strategy. v2 druid.query.groupBy.singleThreaded Merge results using a single thread. false Supported query contexts: Key Description groupByStrategy Overrides the value of druid.query.groupBy.defaultStrategy for this query. groupByIsSingleThreaded Overrides the value of druid.query.groupBy.singleThreaded for this query. GroupBy v2 configurations Supported runtime properties: Property Description Default druid.query.groupBy.bufferGrouperInitialBuckets Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024). 0 druid.query.groupBy.bufferGrouperMaxLoadFactor Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7). 0 druid.query.groupBy.forceHashAggregation Force to use hash-based aggregation. false druid.query.groupBy.intermediateCombineDegree Number of intermediate nodes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful cpu cores. 8 druid.query.groupBy.numParallelCombineThreads Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(druid.query.groupBy.numParallelCombineThreads, druid.processing.numThreads). 1 (disabled) druid.query.groupBy.applyLimitPushDownToSegment If Broker pushes limit down to queryable data server (historicals, peons) then limit results during segment scan. If typically there are a large number of segments taking part in a query on a data server, this setting may counterintuitively reduce performance if enabled. false (disabled) Supported query contexts: Key Description Default bufferGrouperInitialBuckets Overrides the value of druid.query.groupBy.bufferGrouperInitialBuckets for this query. None bufferGrouperMaxLoadFactor Overrides the value of druid.query.groupBy.bufferGrouperMaxLoadFactor for this query. None forceHashAggregation Overrides the value of druid.query.groupBy.forceHashAggregation None intermediateCombineDegree Overrides the value of druid.query.groupBy.intermediateCombineDegree None numParallelCombineThreads Overrides the value of druid.query.groupBy.numParallelCombineThreads None sortByDimsFirst Sort the results first by dimension values and then by timestamp. false forceLimitPushDown When all fields in the orderby are part of the grouping key, the Broker will push limit application down to the Historical processes. When the sorting order uses fields that are not in the grouping key, applying this optimization can result in approximate results with unknown accuracy, so this optimization is disabled by default in that case. Enabling this context flag turns on limit push down for limit/orderbys that contain non-grouping key columns. false applyLimitPushDownToSegment If Broker pushes limit down to queryable nodes (historicals, peons) then limit results during segment scan. This context value can be used to override druid.query.groupBy.applyLimitPushDownToSegment. true GroupBy v1 configurations Supported runtime properties: Property Description Default druid.query.groupBy.maxIntermediateRows Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail. 50000 druid.query.groupBy.maxResults Maximum number of results. Queries that exceed this limit will fail. 500000 Supported query contexts: Key Description Default maxIntermediateRows Can be used to lower the value of druid.query.groupBy.maxIntermediateRows for this query. None maxResults Can be used to lower the value of druid.query.groupBy.maxResults for this query. None useOffheap Set to true to store aggregations off-heap when merging results. false Array based result rows Internally Druid always uses an array based representation of groupBy result rows, but by default this is translated into a map based result format at the Broker. To reduce the overhead of this translation, results may also be returned from the Broker directly in the array based format if resultAsArray is set to true on the query context. Each row is positional, and has the following fields, in order: Timestamp (optional; only if granularity != ALL) Dimensions (in order) Aggregators (in order) Post-aggregators (optional; in order, if present) This schema is not available on the response, so it must be computed from the issued query in order to properly read the results. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/scan-query.html":{"url":"querying/scan-query.html","title":"Scan","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type in the native language. For information about when Druid SQL will use this query type, refer to the SQL documentation. The Scan query returns raw Apache Druid rows in streaming mode. In addition to straightforward usage where a Scan query is issued to the Broker, the Scan query can also be issued directly to Historical processes or streaming ingestion tasks. This can be useful if you want to retrieve large amounts of data in parallel. An example Scan query object is shown below: { \"queryType\": \"scan\", \"dataSource\": \"wikipedia\", \"resultFormat\": \"list\", \"columns\":[], \"intervals\": [ \"2013-01-01/2013-01-02\" ], \"batchSize\":20480, \"limit\":3 } The following are the main parameters for Scan queries: property description required? queryType This String should always be \"scan\"; this is the first thing Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. yes resultFormat How the results are represented: list, compactedList or valueVector. Currently only list and compactedList are supported. Default is list no filter See Filters no columns A String array of dimensions and metrics to scan. If left empty, all dimensions and metrics are returned. no batchSize The maximum number of rows buffered before being returned to the client. Default is 20480 no limit How many rows to return. If not specified, all rows will be returned. no offset Skip this many rows when returning results. Skipped rows will still need to be generated internally and then discarded, meaning that raising offsets to high values can cause queries to use additional resources.Together, \"limit\" and \"offset\" can be used to implement pagination. However, note that if the underlying datasource is modified in between page fetches in ways that affect overall query results, then the different pages will not necessarily align with each other. no order The ordering of returned rows based on timestamp. \"ascending\", \"descending\", and \"none\" (default) are supported. Currently, \"ascending\" and \"descending\" are only supported for queries where the __time column is included in the columns field and the requirements outlined in the time ordering section are met. none legacy Return results consistent with the legacy \"scan-query\" contrib extension. Defaults to the value set by druid.query.scan.legacy, which in turn defaults to false. See Legacy mode for details. no context An additional JSON Object which can be used to specify certain flags (see the query context properties section below). no Example results The format of the result when resultFormat equals list: [{ \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\", \"columns\" : [ \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\" ], \"events\" : [ { \"timestamp\" : \"2013-01-01T00:00:00.000Z\", \"robot\" : \"1\", \"namespace\" : \"article\", \"anonymous\" : \"0\", \"unpatrolled\" : \"0\", \"page\" : \"11._korpus_(NOVJ)\", \"language\" : \"sl\", \"newpage\" : \"0\", \"user\" : \"EmausBot\", \"count\" : 1.0, \"added\" : 39.0, \"delta\" : 39.0, \"variation\" : 39.0, \"deleted\" : 0.0 }, { \"timestamp\" : \"2013-01-01T00:00:00.000Z\", \"robot\" : \"0\", \"namespace\" : \"article\", \"anonymous\" : \"0\", \"unpatrolled\" : \"0\", \"page\" : \"112_U.S._580\", \"language\" : \"en\", \"newpage\" : \"1\", \"user\" : \"MZMcBride\", \"count\" : 1.0, \"added\" : 70.0, \"delta\" : 70.0, \"variation\" : 70.0, \"deleted\" : 0.0 }, { \"timestamp\" : \"2013-01-01T00:00:00.000Z\", \"robot\" : \"0\", \"namespace\" : \"article\", \"anonymous\" : \"0\", \"unpatrolled\" : \"0\", \"page\" : \"113_U.S._243\", \"language\" : \"en\", \"newpage\" : \"1\", \"user\" : \"MZMcBride\", \"count\" : 1.0, \"added\" : 77.0, \"delta\" : 77.0, \"variation\" : 77.0, \"deleted\" : 0.0 } ] } ] The format of the result when resultFormat equals compactedList: [{ \"segmentId\" : \"wikipedia_editstream_2012-12-29T00:00:00.000Z_2013-01-10T08:00:00.000Z_2013-01-10T08:13:47.830Z_v9\", \"columns\" : [ \"timestamp\", \"robot\", \"namespace\", \"anonymous\", \"unpatrolled\", \"page\", \"language\", \"newpage\", \"user\", \"count\", \"added\", \"delta\", \"variation\", \"deleted\" ], \"events\" : [ [\"2013-01-01T00:00:00.000Z\", \"1\", \"article\", \"0\", \"0\", \"11._korpus_(NOVJ)\", \"sl\", \"0\", \"EmausBot\", 1.0, 39.0, 39.0, 39.0, 0.0], [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"112_U.S._580\", \"en\", \"1\", \"MZMcBride\", 1.0, 70.0, 70.0, 70.0, 0.0], [\"2013-01-01T00:00:00.000Z\", \"0\", \"article\", \"0\", \"0\", \"113_U.S._243\", \"en\", \"1\", \"MZMcBride\", 1.0, 77.0, 77.0, 77.0, 0.0] ] } ] Time ordering The Scan query currently supports ordering based on timestamp for non-legacy queries. Note that using time ordering will yield results that do not indicate which segment rows are from (segmentId will show up as null). Furthermore, time ordering is only supported where the result set limit is less than druid.query.scan.maxRowsQueuedForOrdering rows or all segments scanned have fewer than druid.query.scan.maxSegmentPartitionsOrderedInMemory partitions. Also, time ordering is not supported for queries issued directly to historicals unless a list of segments is specified. The reasoning behind these limitations is that the implementation of time ordering uses two strategies that can consume too much heap memory if left unbounded. These strategies (listed below) are chosen on a per-Historical basis depending on query result set limit and the number of segments being scanned. Priority Queue: Each segment on a Historical is opened sequentially. Every row is added to a bounded priority queue which is ordered by timestamp. For every row above the result set limit, the row with the earliest (if descending) or latest (if ascending) timestamp will be dequeued. After every row has been processed, the sorted contents of the priority queue are streamed back to the Broker(s) in batches. Attempting to load too many rows into memory runs the risk of Historical nodes running out of memory. The druid.query.scan.maxRowsQueuedForOrdering property protects from this by limiting the number of rows in the query result set when time ordering is used. N-Way Merge: For each segment, each partition is opened in parallel. Since each partition's rows are already time-ordered, an n-way merge can be performed on the results from each partition. This approach doesn't persist the entire result set in memory (like the Priority Queue) as it streams back batches as they are returned from the merge function. However, attempting to query too many partition could also result in high memory usage due to the need to open decompression and decoding buffers for each. The druid.query.scan.maxSegmentPartitionsOrderedInMemory limit protects from this by capping the number of partitions opened at any times when time ordering is used. Both druid.query.scan.maxRowsQueuedForOrdering and druid.query.scan.maxSegmentPartitionsOrderedInMemory are configurable and can be tuned based on hardware specs and number of dimensions being queried. These config properties can also be overridden using the maxRowsQueuedForOrdering and maxSegmentPartitionsOrderedInMemory properties in the query context (see the Query Context Properties section). Legacy mode The Scan query supports a legacy mode designed for protocol compatibility with the former scan-query contrib extension. In legacy mode you can expect the following behavior changes: The __time column is returned as \"timestamp\" rather than \"__time\". This will take precedence over any other column you may have that is named \"timestamp\". The __time column is included in the list of columns even if you do not specifically ask for it. Timestamps are returned as ISO8601 time strings rather than integers (milliseconds since 1970-01-01 00:00:00 UTC). Legacy mode can be triggered either by passing \"legacy\" : true in your query JSON, or by setting druid.query.scan.legacy = true on your Druid processes. If you were previously using the scan-query contrib extension, the best way to migrate is to activate legacy mode during a rolling upgrade, then switch it off after the upgrade is complete. Configuration Properties Configuration properties: property description values default druid.query.scan.maxRowsQueuedForOrdering The maximum number of rows returned when time ordering is used An integer in [1, 2147483647] 100000 druid.query.scan.maxSegmentPartitionsOrderedInMemory The maximum number of segments scanned per historical when time ordering is used An integer in [1, 2147483647] 50 druid.query.scan.legacy Whether legacy mode should be turned on for Scan queries true or false false Query context properties property description values default maxRowsQueuedForOrdering The maximum number of rows returned when time ordering is used. Overrides the identically named config. An integer in [1, 2147483647] druid.query.scan.maxRowsQueuedForOrdering maxSegmentPartitionsOrderedInMemory The maximum number of segments scanned per historical when time ordering is used. Overrides the identically named config. An integer in [1, 2147483647] druid.query.scan.maxSegmentPartitionsOrderedInMemory Sample query context JSON object: { \"maxRowsQueuedForOrdering\": 100001, \"maxSegmentPartitionsOrderedInMemory\": 100 } 本文源自Apache Druid，阅读原文 更新于： 2020-12-08 09:31:10 "},"querying/searchquery.html":{"url":"querying/searchquery.html","title":"Search","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type that is only available in the native language. A search query returns dimension values that match the search specification. { \"queryType\": \"search\", \"dataSource\": \"sample_datasource\", \"granularity\": \"day\", \"searchDimensions\": [ \"dim1\", \"dim2\" ], \"query\": { \"type\": \"insensitive_contains\", \"value\": \"Ke\" }, \"sort\" : { \"type\": \"lexicographic\" }, \"intervals\": [ \"2013-01-01T00:00:00.000/2013-01-03T00:00:00.000\" ] } There are several main parts to a search query: property description required? queryType This String should always be \"search\"; this is the first thing Apache Druid looks at to figure out how to interpret the query. yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes granularity Defines the granularity of the query. See Granularities. no (default to all) filter See Filters. no limit Defines the maximum number per Historical process (parsed as int) of search results to return. no (default to 1000) intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. yes searchDimensions The dimensions to run the search over. Excluding this means the search is run over all dimensions. no query See SearchQuerySpec. yes sort An object specifying how the results of the search should be sorted.Possible types are \"lexicographic\" (the default sort), \"alphanumeric\", \"strlen\", and \"numeric\".See Sorting Orders for more details. no context See Context no The format of the result is: [ { \"timestamp\": \"2013-01-01T00:00:00.000Z\", \"result\": [ { \"dimension\": \"dim1\", \"value\": \"Ke$ha\", \"count\": 3 }, { \"dimension\": \"dim2\", \"value\": \"Ke$haForPresident\", \"count\": 1 } ] }, { \"timestamp\": \"2013-01-02T00:00:00.000Z\", \"result\": [ { \"dimension\": \"dim1\", \"value\": \"SomethingThatContainsKe\", \"count\": 1 }, { \"dimension\": \"dim2\", \"value\": \"SomethingElseThatContainsKe\", \"count\": 2 } ] } ] Implementation details Strategies Search queries can be executed using two different strategies. The default strategy is determined by the \"druid.query.search.searchStrategy\" runtime property on the Broker. This can be overridden using \"searchStrategy\" in the query context. If neither the context field nor the property is set, the \"useIndexes\" strategy will be used. \"useIndexes\" strategy, the default, first categorizes search dimensions into two groups according to their support for bitmap indexes. And then, it applies index-only and cursor-based execution plans to the group of dimensions supporting bitmaps and others, respectively. The index-only plan uses only indexes for search query processing. For each dimension, it reads the bitmap index for each dimension value, evaluates the search predicate, and finally checks the time interval and filter predicates. For the cursor-based execution plan, please refer to the \"cursorOnly\" strategy. The index-only plan shows low performance for the search dimensions of large cardinality which means most values of search dimensions are unique. \"cursorOnly\" strategy generates a cursor-based execution plan. This plan creates a cursor which reads a row from a queryableIndexSegment, and then evaluates search predicates. If some filters support bitmap indexes, the cursor can read only the rows which satisfy those filters, thereby saving I/O cost. However, it might be slow with filters of low selectivity. \"auto\" strategy uses a cost-based planner for choosing an optimal search strategy. It estimates the cost of index-only and cursor-based execution plans, and chooses the optimal one. Currently, it is not enabled by default due to the overhead of cost estimation. Server configuration The following runtime properties apply: Property Description Default druid.query.search.searchStrategy Default search query strategy. useIndexes Query context The following query context parameters apply: Property Description searchStrategy Overrides the value of druid.query.search.searchStrategy for this query. SearchQuerySpec insensitive_contains If any part of a dimension value contains the value specified in this search query spec, regardless of case, a \"match\" occurs. The grammar is: { \"type\" : \"insensitive_contains\", \"value\" : \"some_value\" } fragment If any part of a dimension value contains all of the values specified in this search query spec, regardless of case by default, a \"match\" occurs. The grammar is: { \"type\" : \"fragment\", \"case_sensitive\" : false, \"values\" : [\"fragment1\", \"fragment2\"] } contains If any part of a dimension value contains the value specified in this search query spec, a \"match\" occurs. The grammar is: { \"type\" : \"contains\", \"case_sensitive\" : true, \"value\" : \"some_value\" } regex If any part of a dimension value contains the pattern specified in this search query spec, a \"match\" occurs. The grammar is: { \"type\" : \"regex\", \"pattern\" : \"some_pattern\" } 本文源自Apache Druid，阅读原文 更新于： 2020-12-08 09:31:10 "},"querying/timeboundaryquery.html":{"url":"querying/timeboundaryquery.html","title":"TimeBoundary","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type that is only available in the native language. Time boundary queries return the earliest and latest data points of a data set. The grammar is: { \"queryType\" : \"timeBoundary\", \"dataSource\": \"sample_datasource\", \"bound\" : # optional, defaults to returning both timestamps if not set \"filter\" : { \"type\": \"and\", \"fields\": [, , ...] } # optional } There are 3 main parts to a time boundary query: property description required? queryType This String should always be \"timeBoundary\"; this is the first thing Apache Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes bound Optional, set to maxTime or minTime to return only the latest or earliest timestamp. Default to returning both if not set no filter See Filters no context See Context no The format of the result is: [ { \"timestamp\" : \"2013-05-09T18:24:00.000Z\", \"result\" : { \"minTime\" : \"2013-05-09T18:24:00.000Z\", \"maxTime\" : \"2013-05-09T18:37:00.000Z\" } } ] 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/segmentmetadataquery.html":{"url":"querying/segmentmetadataquery.html","title":"SegmentMetaData","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type that is only available in the native language. However, Druid SQL contains similar functionality in its metadata tables. Segment metadata queries return per-segment information about: Number of rows stored inside the segment Interval the segment covers Estimated total segment byte size in if it was stored in a 'flat format' (e.g. a csv file) Segment id Is the segment rolled up Detailed per column information such as: type cardinality min/max values presence of null values estimated 'flat format' byte size { \"queryType\":\"segmentMetadata\", \"dataSource\":\"sample_datasource\", \"intervals\":[\"2013-01-01/2014-01-01\"] } There are several main parts to a segment metadata query: property description required? queryType This String should always be \"segmentMetadata\"; this is the first thing Apache Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes intervals A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. no toInclude A JSON Object representing what columns should be included in the result. Defaults to \"all\". no merge Merge all individual segment metadata results into a single result no context See Context no analysisTypes A list of Strings specifying what column properties (e.g. cardinality, size) should be calculated and returned in the result. Defaults to [\"cardinality\", \"interval\", \"minmax\"], but can be overridden with using the segment metadata query config. See section analysisTypes for more details. no lenientAggregatorMerge If true, and if the \"aggregators\" analysisType is enabled, aggregators will be merged leniently. See below for details. no The format of the result is: [ { \"id\" : \"some_id\", \"intervals\" : [ \"2013-05-13T00:00:00.000Z/2013-05-14T00:00:00.000Z\" ], \"columns\" : { \"__time\" : { \"type\" : \"LONG\", \"hasMultipleValues\" : false, \"hasNulls\": false, \"size\" : 407240380, \"cardinality\" : null, \"errorMessage\" : null }, \"dim1\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : false, \"hasNulls\": false, \"size\" : 100000, \"cardinality\" : 1944, \"errorMessage\" : null }, \"dim2\" : { \"type\" : \"STRING\", \"hasMultipleValues\" : true, \"hasNulls\": true, \"size\" : 100000, \"cardinality\" : 1504, \"errorMessage\" : null }, \"metric1\" : { \"type\" : \"FLOAT\", \"hasMultipleValues\" : false, \"hasNulls\": false, \"size\" : 100000, \"cardinality\" : null, \"errorMessage\" : null } }, \"aggregators\" : { \"metric1\" : { \"type\" : \"longSum\", \"name\" : \"metric1\", \"fieldName\" : \"metric1\" } }, \"queryGranularity\" : { \"type\": \"none\" }, \"size\" : 300000, \"numRows\" : 5000000 } ] Dimension columns will have type STRING, FLOAT, DOUBLE, or LONG. Metric columns will have type FLOAT, DOUBLE, or LONG, or the name of the underlying complex type such as hyperUnique in case of COMPLEX metric. Timestamp column will have type LONG. If the errorMessage field is non-null, you should not trust the other fields in the response. Their contents are undefined. Only columns which are dictionary encoded (i.e., have type STRING) will have any cardinality. Rest of the columns (timestamp and metric columns) will show cardinality as null. intervals If an interval is not specified, the query will use a default interval that spans a configurable period before the end time of the most recent segment. The length of this default time period is set in the Broker configuration via: druid.query.segmentMetadata.defaultHistory toInclude There are 3 types of toInclude objects. All The grammar is as follows: \"toInclude\": { \"type\": \"all\"} None The grammar is as follows: \"toInclude\": { \"type\": \"none\"} List The grammar is as follows: \"toInclude\": { \"type\": \"list\", \"columns\": []} analysisTypes This is a list of properties that determines the amount of information returned about the columns, i.e. analyses to be performed on the columns. By default, the \"cardinality\", \"interval\", and \"minmax\" types will be used. If a property is not needed, omitting it from this list will result in a more efficient query. The default analysis types can be set in the Broker configuration via: druid.query.segmentMetadata.defaultAnalysisTypes Types of column analyses are described below: cardinality cardinality in the result will return the size of the bitmap index or dictionary encoding for string dimensions, or null for other dimension types. If merge was set, the result will be the max of this value across segments. Only relevant for dimension columns. minmax Estimated min/max values for each column. Only relevant for dimension columns. size size in the result will contain the estimated total segment byte size as if the data were stored in text format interval intervals in the result will contain the list of intervals associated with the queried segments. timestampSpec timestampSpec in the result will contain timestampSpec of data stored in segments. this can be null if timestampSpec of segments was unknown or unmergeable (if merging is enabled). queryGranularity queryGranularity in the result will contain query granularity of data stored in segments. this can be null if query granularity of segments was unknown or unmergeable (if merging is enabled). aggregators aggregators in the result will contain the list of aggregators usable for querying metric columns. This may be null if the aggregators are unknown or unmergeable (if merging is enabled). Merging can be strict or lenient. See lenientAggregatorMerge below for details. The form of the result is a map of column name to aggregator. rollup rollup in the result is true/false/null. When merging is enabled, if some are rollup, others are not, result is null. lenientAggregatorMerge Conflicts between aggregator metadata across segments can occur if some segments have unknown aggregators, or if two segments use incompatible aggregators for the same column (e.g. longSum changed to doubleSum). Aggregators can be merged strictly (the default) or leniently. With strict merging, if there are any segments with unknown aggregators, or any conflicts of any kind, the merged aggregators list will be null. With lenient merging, segments with unknown aggregators will be ignored, and conflicts between aggregators will only null out the aggregator for that particular column. In particular, with lenient merging, it is possible for an individual column's aggregator to be null. This will not occur with strict merging. 本文源自Apache Druid，阅读原文 更新于： 2020-12-08 09:31:10 "},"querying/datasourcemetadataquery.html":{"url":"querying/datasourcemetadataquery.html","title":"DatasourceMetadata","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes a query type that is only available in the native language. Data Source Metadata queries return metadata information for a dataSource. These queries return information about: The timestamp of latest ingested event for the dataSource. This is the ingested event without any consideration of rollup. The grammar for these queries is: { \"queryType\" : \"dataSourceMetadata\", \"dataSource\": \"sample_datasource\" } There are 2 main parts to a Data Source Metadata query: property description required? queryType This String should always be \"dataSourceMetadata\"; this is the first thing Apache Druid looks at to figure out how to interpret the query yes dataSource A String or Object defining the data source to query, very similar to a table in a relational database. See DataSource for more information. yes context See Context no The format of the result is: [ { \"timestamp\" : \"2013-05-09T18:24:00.000Z\", \"result\" : { \"maxIngestedEventTime\" : \"2013-05-09T18:24:09.007Z\" } } ] 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/filters.html":{"url":"querying/filters.html","title":"过滤器","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about aggregators available in SQL, refer to the SQL documentation. A filter is a JSON object indicating which rows of data should be included in the computation for a query. It’s essentially the equivalent of the WHERE clause in SQL. Apache Druid supports the following types of filters. Note Filters are commonly applied on dimensions, but can be applied on aggregated metrics, for example, see filtered-aggregator and having-filters. Selector filter The simplest filter is a selector filter. The selector filter will match a specific dimension with a specific value. Selector filters can be used as the base filters for more complex Boolean expressions of filters. The grammar for a SELECTOR filter is as follows: \"filter\": { \"type\": \"selector\", \"dimension\": , \"value\": } This is the equivalent of WHERE = ''. The selector filter supports the use of extraction functions, see Filtering with Extraction Functions for details. Column Comparison filter The column comparison filter is similar to the selector filter, but instead compares dimensions to each other. For example: \"filter\": { \"type\": \"columnComparison\", \"dimensions\": [, ] } This is the equivalent of WHERE = . dimensions is list of DimensionSpecs, making it possible to apply an extraction function if needed. Regular expression filter The regular expression filter is similar to the selector filter, but using regular expressions. It matches the specified dimension with the given pattern. The pattern can be any standard Java regular expression. \"filter\": { \"type\": \"regex\", \"dimension\": , \"pattern\": } The regex filter supports the use of extraction functions, see Filtering with Extraction Functions for details. Logical expression filters AND The grammar for an AND filter is as follows: \"filter\": { \"type\": \"and\", \"fields\": [, , ...] } The filters in fields can be any other filter defined on this page. OR The grammar for an OR filter is as follows: \"filter\": { \"type\": \"or\", \"fields\": [, , ...] } The filters in fields can be any other filter defined on this page. NOT The grammar for a NOT filter is as follows: \"filter\": { \"type\": \"not\", \"field\": } The filter specified at field can be any other filter defined on this page. JavaScript filter The JavaScript filter matches a dimension against the specified JavaScript function predicate. The filter matches values for which the function returns true. The function takes a single argument, the dimension value, and returns either true or false. { \"type\" : \"javascript\", \"dimension\" : , \"function\" : \"function(value) { }\" } Example The following matches any dimension values for the dimension name between 'bar' and 'foo' { \"type\" : \"javascript\", \"dimension\" : \"name\", \"function\" : \"function(x) { return(x >= 'bar' && x The JavaScript filter supports the use of extraction functions, see Filtering with Extraction Functions for details. JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. Extraction filter The extraction filter is now deprecated. The selector filter with an extraction function specified provides identical functionality and should be used instead. Extraction filter matches a dimension using some specific Extraction function. The following filter matches the values for which the extraction function has transformation entry input_key=output_value where output_value is equal to the filter value and input_key is present as dimension. Example The following matches dimension values in [product_1, product_3, product_5] for the column product { \"filter\": { \"type\": \"extraction\", \"dimension\": \"product\", \"value\": \"bar_1\", \"extractionFn\": { \"type\": \"lookup\", \"lookup\": { \"type\": \"map\", \"map\": { \"product_1\": \"bar_1\", \"product_5\": \"bar_1\", \"product_3\": \"bar_1\" } } } } } Search filter Search filters can be used to filter on partial string matches. { \"filter\": { \"type\": \"search\", \"dimension\": \"product\", \"query\": { \"type\": \"insensitive_contains\", \"value\": \"foo\" } } } property description required? type This String should always be \"search\". yes dimension The dimension to perform the search over. yes query A JSON object for the type of search. See below for more information. yes extractionFn Extraction function to apply to the dimension no The search filter supports the use of extraction functions, see Filtering with Extraction Functions for details. Search query spec Contains property description required? type This String should always be \"contains\". yes value A String value to run the search over. yes caseSensitive Whether two string should be compared as case sensitive or not no (default == false) Insensitive Contains property description required? type This String should always be \"insensitive_contains\". yes value A String value to run the search over. yes Note that an \"insensitive_contains\" search is equivalent to a \"contains\" search with \"caseSensitive\": false (or not provided). Fragment property description required? type This String should always be \"fragment\". yes values A JSON array of String values to run the search over. yes caseSensitive Whether strings should be compared as case sensitive or not. Default: false(insensitive) no In filter In filter can be used to express the following SQL query: SELECT COUNT(*) AS 'Count' FROM `table` WHERE `outlaw` IN ('Good', 'Bad', 'Ugly') The grammar for a IN filter is as follows: { \"type\": \"in\", \"dimension\": \"outlaw\", \"values\": [\"Good\", \"Bad\", \"Ugly\"] } The IN filter supports the use of extraction functions, see Filtering with Extraction Functions for details. If an empty values array is passed to the IN filter, it will simply return an empty result. If the dimension is a multi-valued dimension, the IN filter will return true if one of the dimension values is in the values array. Like filter Like filters can be used for basic wildcard searches. They are equivalent to the SQL LIKE operator. Special characters supported are \"%\" (matches any number of characters) and \"_\" (matches any one character). property type description required? type String This should always be \"like\". yes dimension String The dimension to filter on yes pattern String LIKE pattern, such as \"foo%\" or \"___bar\". yes escape String An escape character that can be used to escape special characters. no extractionFn Extraction function Extraction function to apply to the dimension no Like filters support the use of extraction functions, see Filtering with Extraction Functions for details. This Like filter expresses the condition last_name LIKE \"D%\" (i.e. last_name starts with \"D\"). { \"type\": \"like\", \"dimension\": \"last_name\", \"pattern\": \"D%\" } Bound filter Bound filters can be used to filter on ranges of dimension values. It can be used for comparison filtering like greater than, less than, greater than or equal to, less than or equal to, and \"between\" (if both \"lower\" and \"upper\" are set). property type description required? type String This should always be \"bound\". yes dimension String The dimension to filter on yes lower String The lower bound for the filter no upper String The upper bound for the filter no lowerStrict Boolean Perform strict comparison on the lower bound (\">\" instead of \">=\") no, default: false upperStrict Boolean Perform strict comparison on the upper bound (\" no, default: false ordering String Specifies the sorting order to use when comparing values against the bound. Can be one of the following values: \"lexicographic\", \"alphanumeric\", \"numeric\", \"strlen\", \"version\". See Sorting Orders for more details. no, default: \"lexicographic\" extractionFn Extraction function Extraction function to apply to the dimension no Bound filters support the use of extraction functions, see Filtering with Extraction Functions for details. The following bound filter expresses the condition 21 : { \"type\": \"bound\", \"dimension\": \"age\", \"lower\": \"21\", \"upper\": \"31\" , \"ordering\": \"numeric\" } This filter expresses the condition foo , using the default lexicographic sorting order. { \"type\": \"bound\", \"dimension\": \"name\", \"lower\": \"foo\", \"upper\": \"hoo\" } Using strict bounds, this filter expresses the condition 21 { \"type\": \"bound\", \"dimension\": \"age\", \"lower\": \"21\", \"lowerStrict\": true, \"upper\": \"31\" , \"upperStrict\": true, \"ordering\": \"numeric\" } The user can also specify a one-sided bound by omitting \"upper\" or \"lower\". This filter expresses age . { \"type\": \"bound\", \"dimension\": \"age\", \"upper\": \"31\" , \"upperStrict\": true, \"ordering\": \"numeric\" } Likewise, this filter expresses age >= 18 { \"type\": \"bound\", \"dimension\": \"age\", \"lower\": \"18\" , \"ordering\": \"numeric\" } Interval Filter The Interval filter enables range filtering on columns that contain long millisecond values, with the boundaries specified as ISO 8601 time intervals. It is suitable for the __time column, long metric columns, and dimensions with values that can be parsed as long milliseconds. This filter converts the ISO 8601 intervals to long millisecond start/end ranges and translates to an OR of Bound filters on those millisecond ranges, with numeric comparison. The Bound filters will have left-closed and right-open matching (i.e., start property type description required? type String This should always be \"interval\". yes dimension String The dimension to filter on yes intervals Array A JSON array containing ISO-8601 interval strings. This defines the time ranges to filter on. yes extractionFn Extraction function Extraction function to apply to the dimension no The interval filter supports the use of extraction functions, see Filtering with Extraction Functions for details. If an extraction function is used with this filter, the extraction function should output values that are parseable as long milliseconds. The following example filters on the time ranges of October 1-7, 2014 and November 15-16, 2014. { \"type\" : \"interval\", \"dimension\" : \"__time\", \"intervals\" : [ \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\", \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\" ] } The filter above is equivalent to the following OR of Bound filters: { \"type\": \"or\", \"fields\": [ { \"type\": \"bound\", \"dimension\": \"__time\", \"lower\": \"1412121600000\", \"lowerStrict\": false, \"upper\": \"1412640000000\" , \"upperStrict\": true, \"ordering\": \"numeric\" }, { \"type\": \"bound\", \"dimension\": \"__time\", \"lower\": \"1416009600000\", \"lowerStrict\": false, \"upper\": \"1416096000000\" , \"upperStrict\": true, \"ordering\": \"numeric\" } ] } Filtering with Extraction Functions All filters except the \"spatial\" filter support extraction functions. An extraction function is defined by setting the \"extractionFn\" field on a filter. See Extraction function for more details on extraction functions. If specified, the extraction function will be used to transform input values before the filter is applied. The example below shows a selector filter combined with an extraction function. This filter will transform input values according to the values defined in the lookup map; transformed values will then be matched with the string \"bar_1\". Example The following matches dimension values in [product_1, product_3, product_5] for the column product { \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"bar_1\", \"extractionFn\": { \"type\": \"lookup\", \"lookup\": { \"type\": \"map\", \"map\": { \"product_1\": \"bar_1\", \"product_5\": \"bar_1\", \"product_3\": \"bar_1\" } } } } } Column types Druid supports filtering on timestamp, string, long, and float columns. Note that only string columns have bitmap indexes. Therefore, queries that filter on other column types will need to scan those columns. Filtering on numeric columns When filtering on numeric columns, you can write filters as if they were strings. In most cases, your filter will be converted into a numeric predicate and will be applied to the numeric column values directly. In some cases (such as the \"regex\" filter) the numeric column values will be converted to strings during the scan. For example, filtering on a specific value, myFloatColumn = 10.1: \"filter\": { \"type\": \"selector\", \"dimension\": \"myFloatColumn\", \"value\": \"10.1\" } Filtering on a range of values, 10 : \"filter\": { \"type\": \"bound\", \"dimension\": \"myFloatColumn\", \"ordering\": \"numeric\", \"lower\": \"10\", \"lowerStrict\": false, \"upper\": \"20\", \"upperStrict\": true } Filtering on the Timestamp Column Query filters can also be applied to the timestamp column. The timestamp column has long millisecond values. To refer to the timestamp column, use the string __time as the dimension name. Like numeric dimensions, timestamp filters should be specified as if the timestamp values were strings. If the user wishes to interpret the timestamp with a specific format, timezone, or locale, the Time Format Extraction Function is useful. For example, filtering on a long timestamp value: \"filter\": { \"type\": \"selector\", \"dimension\": \"__time\", \"value\": \"124457387532\" } Filtering on day of week: \"filter\": { \"type\": \"selector\", \"dimension\": \"__time\", \"value\": \"Friday\", \"extractionFn\": { \"type\": \"timeFormat\", \"format\": \"EEEE\", \"timeZone\": \"America/New_York\", \"locale\": \"en\" } } Filtering on a set of ISO 8601 intervals: { \"type\" : \"interval\", \"dimension\" : \"__time\", \"intervals\" : [ \"2014-10-01T00:00:00.000Z/2014-10-07T00:00:00.000Z\", \"2014-11-15T00:00:00.000Z/2014-11-16T00:00:00.000Z\" ] } True Filter The true filter is a filter which matches all values. It can be used to temporarily disable other filters without removing the filter. { \"type\" : \"true\" } 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/granularities.html":{"url":"querying/granularities.html","title":"时间粒度","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about time functions available in SQL, refer to the SQL documentation. The granularity field determines how data gets bucketed across the time dimension, or how it gets aggregated by hour, day, minute, etc. It can be specified either as a string for simple granularities or as an object for arbitrary granularities. Simple Granularities Simple granularities are specified as a string and bucket timestamps by their UTC time (e.g., days start at 00:00 UTC). Supported granularity strings are: all, none, second, minute, fifteen_minute, thirty_minute, hour, day, week, month, quarter and year. all buckets everything into a single bucket none does not bucket data (it actually uses the granularity of the index - minimum here is none which means millisecond granularity). Using none in a TimeseriesQuery is currently not recommended (the system will try to generate 0 values for all milliseconds that didn’t exist, which is often a lot). Example: Suppose you have data below stored in Apache Druid with millisecond ingestion granularity, {\"timestamp\": \"2013-08-31T01:02:33Z\", \"page\": \"AAA\", \"language\" : \"en\"} {\"timestamp\": \"2013-09-01T01:02:33Z\", \"page\": \"BBB\", \"language\" : \"en\"} {\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"} {\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"} After submitting a groupBy query with hour granularity, { \"queryType\":\"groupBy\", \"dataSource\":\"my_dataSource\", \"granularity\":\"hour\", \"dimensions\":[ \"language\" ], \"aggregations\":[ { \"type\":\"count\", \"name\":\"count\" } ], \"intervals\":[ \"2000-01-01T00:00Z/3000-01-01T00:00Z\" ] } you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T01:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T01:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T23:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-03T03:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] Note that all the empty buckets are discarded. If you change the granularity to day, you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-03T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] If you change the granularity to none, you will get the same results as setting it to the ingestion granularity. [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T01:02:33.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T01:02:33.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T23:32:45.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-03T03:32:45.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] Having a query time granularity that is smaller than the queryGranularity parameter set at ingestion time is unreasonable because information about that smaller granularity is not present in the indexed data. So, if the query time granularity is smaller than the ingestion time query granularity, Druid produces results that are equivalent to having set granularity to queryGranularity. If you change the granularity to all, you will get everything aggregated in 1 bucket, [ { \"version\" : \"v1\", \"timestamp\" : \"2000-01-01T00:00:00.000Z\", \"event\" : { \"count\" : 4, \"language\" : \"en\" } } ] Duration Granularities Duration granularities are specified as an exact duration in milliseconds and timestamps are returned as UTC. Duration granularity values are in millis. They also support specifying an optional origin, which defines where to start counting time buckets from (defaults to 1970-01-01T00:00:00Z). {\"type\": \"duration\", \"duration\": 7200000} This chunks up every 2 hours. {\"type\": \"duration\", \"duration\": 3600000, \"origin\": \"2012-01-01T00:30:00Z\"} This chunks up every hour on the half-hour. Example: Reusing the data in the previous example, after submitting a groupBy query with 24 hours duration, { \"queryType\":\"groupBy\", \"dataSource\":\"my_dataSource\", \"granularity\":{\"type\": \"duration\", \"duration\": \"86400000\"}, \"dimensions\":[ \"language\" ], \"aggregations\":[ { \"type\":\"count\", \"name\":\"count\" } ], \"intervals\":[ \"2000-01-01T00:00Z/3000-01-01T00:00Z\" ] } you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-03T00:00:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] if you set the origin for the granularity to 2012-01-01T00:30:00Z, \"granularity\":{\"type\": \"duration\", \"duration\": \"86400000\", \"origin\":\"2012-01-01T00:30:00Z\"} you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T00:30:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T00:30:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T00:30:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-03T00:30:00.000Z\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] Note that the timestamp for each bucket starts at the 30th minute. Period Granularities Period granularities are specified as arbitrary period combinations of years, months, weeks, hours, minutes and seconds (e.g. P2W, P3M, PT1H30M, PT0.750S) in ISO8601 format. They support specifying a time zone which determines where period boundaries start as well as the timezone of the returned timestamps. By default, years start on the first of January, months start on the first of the month and weeks start on Mondays unless an origin is specified. Time zone is optional (defaults to UTC). Origin is optional (defaults to 1970-01-01T00:00:00 in the given time zone). {\"type\": \"period\", \"period\": \"P2D\", \"timeZone\": \"America/Los_Angeles\"} This will bucket by two-day chunks in the Pacific timezone. {\"type\": \"period\", \"period\": \"P3M\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"2012-02-01T00:00:00-08:00\"} This will bucket by 3-month chunks in the Pacific timezone where the three-month quarters are defined as starting from February. Example Reusing the data in the previous example, if you submit a groupBy query with 1 day period in Pacific timezone, { \"queryType\":\"groupBy\", \"dataSource\":\"my_dataSource\", \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\"}, \"dimensions\":[ \"language\" ], \"aggregations\":[ { \"type\":\"count\", \"name\":\"count\" } ], \"intervals\":[ \"1999-12-31T16:00:00.000-08:00/2999-12-31T16:00:00.000-08:00\" ] } you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-30T00:00:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-08-31T00:00:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T00:00:00.000-07:00\", \"event\" : { \"count\" : 2, \"language\" : \"en\" } } ] Note that the timestamp for each bucket has been converted to Pacific time. Row {\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"} and {\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"} are put in the same bucket because they are in the same day in Pacific time. Also note that the intervals in groupBy query will not be converted to the timezone specified, the timezone specified in granularity is only applied on the query results. If you set the origin for the granularity to 1970-01-01T20:30:00-08:00, \"granularity\":{\"type\": \"period\", \"period\": \"P1D\", \"timeZone\": \"America/Los_Angeles\", \"origin\": \"1970-01-01T20:30:00-08:00\"} you will get [ { \"version\" : \"v1\", \"timestamp\" : \"2013-08-29T20:30:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-08-30T20:30:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-01T20:30:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } }, { \"version\" : \"v1\", \"timestamp\" : \"2013-09-02T20:30:00.000-07:00\", \"event\" : { \"count\" : 1, \"language\" : \"en\" } } ] Note that the origin you specified has nothing to do with the timezone, it only serves as a starting point for locating the very first granularity bucket. In this case, Row {\"timestamp\": \"2013-09-02T23:32:45Z\", \"page\": \"CCC\", \"language\" : \"en\"} and {\"timestamp\": \"2013-09-03T03:32:45Z\", \"page\": \"DDD\", \"language\" : \"en\"} are not in the same bucket. Supported Time Zones Timezone support is provided by the Joda Time library, which uses the standard IANA time zones. See the Joda Time supported timezones. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/dimensionspecs.html":{"url":"querying/dimensionspecs.html","title":"数据维度","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. The following JSON fields can be used in a query to operate on dimension values. DimensionSpec DimensionSpecs define how dimension values get transformed prior to aggregation. Default DimensionSpec Returns dimension values as is and optionally renames the dimension. { \"type\" : \"default\", \"dimension\" : , \"outputName\": , \"outputType\": } When specifying a DimensionSpec on a numeric column, the user should include the type of the column in the outputType field. If left unspecified, the outputType defaults to STRING. Please refer to the Output Types section for more details. Extraction DimensionSpec Returns dimension values transformed using the given extraction function. { \"type\" : \"extraction\", \"dimension\" : , \"outputName\" : , \"outputType\": , \"extractionFn\" : } outputType may also be specified in an ExtractionDimensionSpec to apply type conversion to results before merging. If left unspecified, the outputType defaults to STRING. Please refer to the Output Types section for more details. Filtered DimensionSpecs These are only useful for multi-value dimensions. If you have a row in Apache Druid that has a multi-value dimension with values [\"v1\", \"v2\", \"v3\"] and you send a groupBy/topN query grouping by that dimension with query filter for value \"v1\". In the response you will get 3 rows containing \"v1\", \"v2\" and \"v3\". This behavior might be unintuitive for some use cases. It happens because \"query filter\" is internally used on the bitmaps and only used to match the row to be included in the query result processing. With multi-value dimensions, \"query filter\" behaves like a contains check, which will match the row with dimension value [\"v1\", \"v2\", \"v3\"]. Please see the section on \"Multi-value columns\" in segment for more details. Then groupBy/topN processing pipeline \"explodes\" all multi-value dimensions resulting 3 rows for \"v1\", \"v2\" and \"v3\" each. In addition to \"query filter\" which efficiently selects the rows to be processed, you can use the filtered dimension spec to filter for specific values within the values of a multi-value dimension. These dimensionSpecs take a delegate DimensionSpec and a filtering criteria. From the \"exploded\" rows, only rows matching the given filtering criteria are returned in the query result. The following filtered dimension spec acts as a whitelist or blacklist for values as per the \"isWhitelist\" attribute value. { \"type\" : \"listFiltered\", \"delegate\" : , \"values\": , \"isWhitelist\": } Following filtered dimension spec retains only the values matching regex. Note that listFiltered is faster than this and one should use that for whitelist or blacklist use case. { \"type\" : \"regexFiltered\", \"delegate\" : , \"pattern\": } Following filtered dimension spec retains only the values starting with the same prefix. { \"type\" : \"prefixFiltered\", \"delegate\" : , \"prefix\": } For more details and examples, see multi-value dimensions. Lookup DimensionSpecs Lookups are an experimental feature. Lookup DimensionSpecs can be used to define directly a lookup implementation as dimension spec. Generally speaking there is two different kind of lookups implementations. The first kind is passed at the query time like map implementation. { \"type\":\"lookup\", \"dimension\":\"dimensionName\", \"outputName\":\"dimensionOutputName\", \"replaceMissingValueWith\":\"missing_value\", \"retainMissingValue\":false, \"lookup\":{\"type\": \"map\", \"map\":{\"key\":\"value\"}, \"isOneToOne\":false} } A property of retainMissingValue and replaceMissingValueWith can be specified at query time to hint how to handle missing values. Setting replaceMissingValueWith to \"\" has the same effect as setting it to null or omitting the property. Setting retainMissingValue to true will use the dimension's original value if it is not found in the lookup. The default values are replaceMissingValueWith = null and retainMissingValue = false which causes missing values to be treated as missing. It is illegal to set retainMissingValue = true and also specify a replaceMissingValueWith. A property optimize can be supplied to allow optimization of lookup based extraction filter (by default optimize = true). The second kind where it is not possible to pass at query time due to their size, will be based on an external lookup table or resource that is already registered via configuration file or/and Coordinator. { \"type\":\"lookup\", \"dimension\":\"dimensionName\", \"outputName\":\"dimensionOutputName\", \"name\":\"lookupName\" } Output Types The dimension specs provide an option to specify the output type of a column's values. This is necessary as it is possible for a column with given name to have different value types in different segments; results will be converted to the type specified by outputType before merging. Note that not all use cases for DimensionSpec currently support outputType, the table below shows which use cases support this option: Query Type Supported? GroupBy (v1) no GroupBy (v2) yes TopN yes Search no Select no Cardinality Aggregator no Extraction Functions Extraction functions define the transformation applied to each dimension value. Transformations can be applied to both regular (string) dimensions, as well as the special __time dimension, which represents the current time bucket according to the query aggregation granularity. Note: for functions taking string values (such as regular expressions), __time dimension values will be formatted in ISO-8601 format before getting passed to the extraction function. Regular Expression Extraction Function Returns the first matching group for the given regular expression. If there is no match, it returns the dimension value as is. { \"type\" : \"regex\", \"expr\" : , \"index\" : \"replaceMissingValue\" : true, \"replaceMissingValueWith\" : \"foobar\" } For example, using \"expr\" : \"(\\\\w\\\\w\\\\w).*\" will transform 'Monday', 'Tuesday', 'Wednesday' into 'Mon', 'Tue', 'Wed'. If \"index\" is set, it will control which group from the match to extract. Index zero extracts the string matching the entire pattern. If the replaceMissingValue property is true, the extraction function will transform dimension values that do not match the regex pattern to a user-specified String. Default value is false. The replaceMissingValueWith property sets the String that unmatched dimension values will be replaced with, if replaceMissingValue is true. If replaceMissingValueWith is not specified, unmatched dimension values will be replaced with nulls. For example, if expr is \"(a\\w+)\" in the example JSON above, a regex that matches words starting with the letter a, the extraction function will convert a dimension value like banana to foobar. Partial Extraction Function Returns the dimension value unchanged if the regular expression matches, otherwise returns null. { \"type\" : \"partial\", \"expr\" : } Search query extraction function Returns the dimension value unchanged if the given SearchQuerySpec matches, otherwise returns null. { \"type\" : \"searchQuery\", \"query\" : } Substring Extraction Function Returns a substring of the dimension value starting from the supplied index and of the desired length. Both index and length are measured in the number of Unicode code units present in the string as if it were encoded in UTF-16. Note that some Unicode characters may be represented by two code units. This is the same behavior as the Java String class's \"substring\" method. If the desired length exceeds the length of the dimension value, the remainder of the string starting at index will be returned. If index is greater than the length of the dimension value, null will be returned. { \"type\" : \"substring\", \"index\" : 1, \"length\" : 4 } The length may be omitted for substring to return the remainder of the dimension value starting from index, or null if index greater than the length of the dimension value. { \"type\" : \"substring\", \"index\" : 3 } Strlen Extraction Function Returns the length of dimension values, as measured in the number of Unicode code units present in the string as if it were encoded in UTF-16. Note that some Unicode characters may be represented by two code units. This is the same behavior as the Java String class's \"length\" method. null strings are considered as having zero length. { \"type\" : \"strlen\" } Time Format Extraction Function Returns the dimension value formatted according to the given format string, time zone, and locale. For __time dimension values, this formats the time value bucketed by the aggregation granularity For a regular dimension, it assumes the string is formatted in ISO-8601 date and time format. format : date time format for the resulting dimension value, in Joda Time DateTimeFormat, or null to use the default ISO8601 format. locale : locale (language and country) to use, given as a IETF BCP 47 language tag, e.g. en-US, en-GB, fr-FR, fr-CA, etc. timeZone : time zone to use in IANA tz database format, e.g. Europe/Berlin (this can possibly be different than the aggregation time-zone) granularity : granularity to apply before formatting, or omit to not apply any granularity. asMillis : boolean value, set to true to treat input strings as millis rather than ISO8601 strings. Additionally, if format is null or not specified, output will be in millis rather than ISO8601. { \"type\" : \"timeFormat\", \"format\" : (optional), \"timeZone\" : (optional, default UTC), \"locale\" : (optional, default current locale), \"granularity\" : (optional, default none) }, \"asMillis\" : (optional) } For example, the following dimension spec returns the day of the week for Montréal in French: { \"type\" : \"extraction\", \"dimension\" : \"__time\", \"outputName\" : \"dayOfWeek\", \"extractionFn\" : { \"type\" : \"timeFormat\", \"format\" : \"EEEE\", \"timeZone\" : \"America/Montreal\", \"locale\" : \"fr\" } } Time Parsing Extraction Function Parses dimension values as timestamps using the given input format, and returns them formatted using the given output format. Note, if you are working with the __time dimension, you should consider using the time extraction function instead instead, which works on time value directly as opposed to string values. If \"joda\" is true, time formats are described in the Joda DateTimeFormat documentation. If \"joda\" is false (or unspecified) then formats are described in the SimpleDateFormat documentation. In general, we recommend setting \"joda\" to true since Joda format strings are more common in Druid APIs and since Joda handles certain edge cases (like weeks and weekyears near the start and end of calendar years) in a more ISO8601 compliant way. If a value cannot be parsed using the provided timeFormat, it will be returned as-is. { \"type\" : \"time\", \"timeFormat\" : , \"resultFormat\" : , \"joda\" : } JavaScript Extraction Function Returns the dimension value, as transformed by the given JavaScript function. For regular dimensions, the input value is passed as a string. For the __time dimension, the input value is passed as a number representing the number of milliseconds since January 1, 1970 UTC. Example for a regular dimension { \"type\" : \"javascript\", \"function\" : \"function(str) { return str.substr(0, 3); }\" } { \"type\" : \"javascript\", \"function\" : \"function(str) { return str + '!!!'; }\", \"injective\" : true } A property of injective specifies if the JavaScript function preserves uniqueness. The default value is false meaning uniqueness is not preserved Example for the __time dimension: { \"type\" : \"javascript\", \"function\" : \"function(t) { return 'Second ' + Math.floor((t % 60000) / 1000); }\" } JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. Registered lookup extraction function Lookups are a concept in Druid where dimension values are (optionally) replaced with new values. For more documentation on using lookups, please see Lookups. The \"registeredLookup\" extraction function lets you refer to a lookup that has been registered in the cluster-wide configuration. An example: { \"type\":\"registeredLookup\", \"lookup\":\"some_lookup_name\", \"retainMissingValue\":true } A property of retainMissingValue and replaceMissingValueWith can be specified at query time to hint how to handle missing values. Setting replaceMissingValueWith to \"\" has the same effect as setting it to null or omitting the property. Setting retainMissingValue to true will use the dimension's original value if it is not found in the lookup. The default values are replaceMissingValueWith = null and retainMissingValue = false which causes missing values to be treated as missing. It is illegal to set retainMissingValue = true and also specify a replaceMissingValueWith. A property of injective can override the lookup's own sense of whether or not it is injective. If left unspecified, Druid will use the registered cluster-wide lookup configuration. A property optimize can be supplied to allow optimization of lookup based extraction filter (by default optimize = true). The optimization layer will run on the Broker and it will rewrite the extraction filter as clause of selector filters. For instance the following filter { \"filter\": { \"type\": \"selector\", \"dimension\": \"product\", \"value\": \"bar_1\", \"extractionFn\": { \"type\": \"registeredLookup\", \"optimize\": true, \"lookup\": \"some_lookup_name\" } } } will be rewritten as the following simpler query, assuming a lookup that maps \"product_1\" and \"product_3\" to the value \"bar_1\": { \"filter\":{ \"type\":\"or\", \"fields\":[ { \"filter\":{ \"type\":\"selector\", \"dimension\":\"product\", \"value\":\"product_1\" } }, { \"filter\":{ \"type\":\"selector\", \"dimension\":\"product\", \"value\":\"product_3\" } } ] } } A null dimension value can be mapped to a specific value by specifying the empty string as the key in your lookup file. This allows distinguishing between a null dimension and a lookup resulting in a null. For example, specifying {\"\":\"bar\",\"bat\":\"baz\"} with dimension values [null, \"foo\", \"bat\"] and replacing missing values with \"oof\" will yield results of [\"bar\", \"oof\", \"baz\"]. Omitting the empty string key will cause the missing value to take over. For example, specifying {\"bat\":\"baz\"} with dimension values [null, \"foo\", \"bat\"] and replacing missing values with \"oof\" will yield results of [\"oof\", \"oof\", \"baz\"]. Inline lookup extraction function Lookups are a concept in Druid where dimension values are (optionally) replaced with new values. For more documentation on using lookups, please see Lookups. The \"lookup\" extraction function lets you specify an inline lookup map without registering one in the cluster-wide configuration. Examples: { \"type\":\"lookup\", \"lookup\":{ \"type\":\"map\", \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"} }, \"retainMissingValue\":true, \"injective\":true } { \"type\":\"lookup\", \"lookup\":{ \"type\":\"map\", \"map\":{\"foo\":\"bar\", \"baz\":\"bat\"} }, \"retainMissingValue\":false, \"injective\":false, \"replaceMissingValueWith\":\"MISSING\" } The inline lookup should be of type map. The properties retainMissingValue, replaceMissingValueWith, injective, and optimize behave similarly to the registered lookup extraction function. Cascade Extraction Function Provides chained execution of extraction functions. A property of extractionFns contains an array of any extraction functions, which is executed in the array index order. Example for chaining regular expression extraction function, JavaScript extraction function, and substring extraction function is as followings. { \"type\" : \"cascade\", \"extractionFns\": [ { \"type\" : \"regex\", \"expr\" : \"/([^/]+)/\", \"replaceMissingValue\": false, \"replaceMissingValueWith\": null }, { \"type\" : \"javascript\", \"function\" : \"function(str) { return \\\"the \\\".concat(str) }\" }, { \"type\" : \"substring\", \"index\" : 0, \"length\" : 7 } ] } It will transform dimension values with specified extraction functions in the order named. For example, '/druid/prod/historical' is transformed to 'the dru' as regular expression extraction function first transforms it to 'druid' and then, JavaScript extraction function transforms it to 'the druid', and lastly, substring extraction function transforms it to 'the dru'. String Format Extraction Function Returns the dimension value formatted according to the given format string. { \"type\" : \"stringFormat\", \"format\" : , \"nullHandling\" : } For example if you want to concat \"[\" and \"]\" before and after the actual dimension value, you need to specify \"[%s]\" as format string. \"nullHandling\" can be one of nullString, emptyString or returnNull. With \"[%s]\" format, each configuration will result [null], [], null. Default is nullString. Upper and Lower extraction functions. Returns the dimension values as all upper case or lower case. Optionally user can specify the language to use in order to perform upper or lower transformation { \"type\" : \"upper\", \"locale\":\"fr\" } or without setting \"locale\" (in this case, the current value of the default locale for this instance of the Java Virtual Machine.) { \"type\" : \"lower\" } Bucket Extraction Function Bucket extraction function is used to bucket numerical values in each range of the given size by converting them to the same base value. Non numeric values are converted to null. size : the size of the buckets (optional, default 1) offset : the offset for the buckets (optional, default 0) The following extraction function creates buckets of 5 starting from 2. In this case, values in the range of [2, 7) will be converted to 2, values in [7, 12) will be converted to 7, etc. { \"type\" : \"bucket\", \"size\" : 5, \"offset\" : 2 } 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/aggregations.html":{"url":"querying/aggregations.html","title":"聚合","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about aggregators available in SQL, refer to the SQL documentation. Aggregations can be provided at ingestion time as part of the ingestion spec as a way of summarizing data before it enters Apache Druid. Aggregations can also be specified as part of many queries at query time. Available aggregations are: Count aggregator count computes the count of Druid rows that match the filters. { \"type\" : \"count\", \"name\" : } Please note the count aggregator counts the number of Druid rows, which does not always reflect the number of raw events ingested. This is because Druid can be configured to roll up data at ingestion time. To count the number of ingested rows of data, include a count aggregator at ingestion time, and a longSum aggregator at query time. Sum aggregators longSum aggregator computes the sum of values as a 64-bit, signed integer { \"type\" : \"longSum\", \"name\" : , \"fieldName\" : } name – output name for the summed value fieldName – name of the metric column to sum over doubleSum aggregator Computes and stores the sum of values as 64-bit floating point value. Similar to longSum { \"type\" : \"doubleSum\", \"name\" : , \"fieldName\" : } floatSum aggregator Computes and stores the sum of values as 32-bit floating point value. Similar to longSum and doubleSum { \"type\" : \"floatSum\", \"name\" : , \"fieldName\" : } Min / Max aggregators doubleMin aggregator doubleMin computes the minimum of all metric values and Double.POSITIVE_INFINITY { \"type\" : \"doubleMin\", \"name\" : , \"fieldName\" : } doubleMax aggregator doubleMax computes the maximum of all metric values and Double.NEGATIVE_INFINITY { \"type\" : \"doubleMax\", \"name\" : , \"fieldName\" : } floatMin aggregator floatMin computes the minimum of all metric values and Float.POSITIVE_INFINITY { \"type\" : \"floatMin\", \"name\" : , \"fieldName\" : } floatMax aggregator floatMax computes the maximum of all metric values and Float.NEGATIVE_INFINITY { \"type\" : \"floatMax\", \"name\" : , \"fieldName\" : } longMin aggregator longMin computes the minimum of all metric values and Long.MAX_VALUE { \"type\" : \"longMin\", \"name\" : , \"fieldName\" : } longMax aggregator longMax computes the maximum of all metric values and Long.MIN_VALUE { \"type\" : \"longMax\", \"name\" : , \"fieldName\" : } doubleMean aggregator Computes and returns arithmetic mean of a column values as 64 bit float value. This is a query time aggregator only and should not be used during indexing. { \"type\" : \"doubleMean\", \"name\" : , \"fieldName\" : } First / Last aggregator (Double/Float/Long) First and Last aggregator cannot be used in ingestion spec, and should only be specified as part of queries. Note that queries with first/last aggregators on a segment created with rollup enabled will return the rolled up value, and not the last value within the raw ingested data. doubleFirst aggregator doubleFirst computes the metric value with the minimum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"doubleFirst\", \"name\" : , \"fieldName\" : } doubleLast aggregator doubleLast computes the metric value with the maximum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"doubleLast\", \"name\" : , \"fieldName\" : } floatFirst aggregator floatFirst computes the metric value with the minimum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"floatFirst\", \"name\" : , \"fieldName\" : } floatLast aggregator floatLast computes the metric value with the maximum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"floatLast\", \"name\" : , \"fieldName\" : } longFirst aggregator longFirst computes the metric value with the minimum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"longFirst\", \"name\" : , \"fieldName\" : } longLast aggregator longLast computes the metric value with the maximum timestamp or 0 in default mode or null in SQL compatible mode if no row exist { \"type\" : \"longLast\", \"name\" : , \"fieldName\" : , } stringFirst aggregator stringFirst computes the metric value with the minimum timestamp or null if no row exist { \"type\" : \"stringFirst\", \"name\" : , \"fieldName\" : , \"maxStringBytes\" : # (optional, defaults to 1024) } stringLast aggregator stringLast computes the metric value with the maximum timestamp or null if no row exist { \"type\" : \"stringLast\", \"name\" : , \"fieldName\" : , \"maxStringBytes\" : # (optional, defaults to 1024) } ANY aggregator (Double/Float/Long/String) ANY aggregator cannot be used in ingestion spec, and should only be specified as part of queries. Returns any value including null. This aggregator can simplify and optimize the performance by returning the first encountered value (including null) doubleAny aggregator doubleAny returns any double metric value { \"type\" : \"doubleAny\", \"name\" : , \"fieldName\" : } floatAny aggregator floatAny returns any float metric value { \"type\" : \"floatAny\", \"name\" : , \"fieldName\" : } longAny aggregator longAny returns any long metric value { \"type\" : \"longAny\", \"name\" : , \"fieldName\" : , } stringAny aggregator stringAny returns any string metric value { \"type\" : \"stringAny\", \"name\" : , \"fieldName\" : , \"maxStringBytes\" : # (optional, defaults to 1024), } JavaScript aggregator Computes an arbitrary JavaScript function over a set of columns (both metrics and dimensions are allowed). Your JavaScript functions are expected to return floating-point values. { \"type\": \"javascript\", \"name\": \"\", \"fieldNames\" : [ , , ... ], \"fnAggregate\" : \"function(current, column1, column2, ...) { return }\", \"fnCombine\" : \"function(partialA, partialB) { return ; }\", \"fnReset\" : \"function() { return ; }\" } Example { \"type\": \"javascript\", \"name\": \"sum(log(x)*y) + 10\", \"fieldNames\": [\"x\", \"y\"], \"fnAggregate\" : \"function(current, a, b) { return current + (Math.log(a) * b); }\", \"fnCombine\" : \"function(partialA, partialB) { return partialA + partialB; }\", \"fnReset\" : \"function() { return 10; }\" } JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. Approximate Aggregations Count distinct Apache DataSketches Theta Sketch The DataSketches Theta Sketch extension-provided aggregator gives distinct count estimates with support for set union, intersection, and difference post-aggregators, using Theta sketches from the Apache DataSketches library. Apache DataSketches HLL Sketch The DataSketches HLL Sketch extension-provided aggregator gives distinct count estimates using the HyperLogLog algorithm. Compared to the Theta sketch, the HLL sketch does not support set operations and has slightly slower update and merge speed, but requires significantly less space. Cardinality, hyperUnique For new use cases, we recommend evaluating DataSketches Theta Sketch or DataSketches HLL Sketch instead. The DataSketches aggregators are generally able to offer more flexibility and better accuracy than the classic Druid cardinality and hyperUnique aggregators. The Cardinality and HyperUnique aggregators are older aggregator implementations available by default in Druid that also provide distinct count estimates using the HyperLogLog algorithm. The newer DataSketches Theta and HLL extension-provided aggregators described above have superior accuracy and performance and are recommended instead. The DataSketches team has published a comparison study between Druid's original HLL algorithm and the DataSketches HLL algorithm. Based on the demonstrated advantages of the DataSketches implementation, we are recommending using them in preference to Druid's original HLL-based aggregators. However, to ensure backwards compatibility, we will continue to support the classic aggregators. Please note that hyperUnique aggregators are not mutually compatible with Datasketches HLL or Theta sketches. Multi-column handling Note the DataSketches Theta and HLL aggregators currently only support single-column inputs. If you were previously using the Cardinality aggregator with multiple-column inputs, equivalent operations using Theta or HLL sketches are described below: Multi-column byValue Cardinality can be replaced with a union of Theta sketches on the individual input columns Multi-column byRow Cardinality can be replaced with a Theta or HLL sketch on a single virtual column that combines the individual input columns. Histograms and quantiles DataSketches Quantiles Sketch The DataSketches Quantiles Sketch extension-provided aggregator provides quantile estimates and histogram approximations using the numeric quantiles DoublesSketch from the datasketches library. We recommend this aggregator in general for quantiles/histogram use cases, as it provides formal error bounds and has distribution-independent accuracy. Moments Sketch (Experimental) The Moments Sketch extension-provided aggregator is an experimental aggregator that provides quantile estimates using the Moments Sketch. The Moments Sketch aggregator is provided as an experimental option. It is optimized for merging speed and it can have higher aggregation performance compared to the DataSketches quantiles aggregator. However, the accuracy of the Moments Sketch is distribution-dependent, so users will need to empirically verify that the aggregator is suitable for their input data. As a general guideline for experimentation, the Moments Sketch paper points out that this algorithm works better on inputs with high entropy. In particular, the algorithm is not a good fit when the input data consists of a small number of clustered discrete values. Fixed Buckets Histogram Druid also provides a simple histogram implementation that uses a fixed range and fixed number of buckets with support for quantile estimation, backed by an array of bucket count values. The fixed buckets histogram can perform well when the distribution of the input data allows a small number of buckets to be used. We do not recommend the fixed buckets histogram for general use, as its usefulness is extremely data dependent. However, it is made available for users that have already identified use cases where a fixed buckets histogram is suitable. Approximate Histogram (deprecated) The Approximate Histogram aggregator is deprecated. There are a number of other quantile estimation algorithms that offer better performance, accuracy, and memory footprint. We recommend using DataSketches Quantiles instead. The Approximate Histogram extension-provided aggregator also provides quantile estimates and histogram approximations, based on http://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf. The algorithm used by this deprecated aggregator is highly distribution-dependent and its output is subject to serious distortions when the input does not fit within the algorithm's limitations. A study published by the DataSketches team demonstrates some of the known failure modes of this algorithm: The algorithm's quantile calculations can fail to provide results for a large range of rank values (all ranks less than 0.89 in the example used in the study), returning all zeroes instead. The algorithm can completely fail to record spikes in the tail ends of the distribution In general, the histogram produced by the algorithm can deviate significantly from the true histogram, with no bounds on the errors. It is not possible to determine a priori how well this aggregator will behave for a given input stream, nor does the aggregator provide any indication that serious distortions are present in the output. For these reasons, we have deprecated this aggregator and recommend using the DataSketches Quantiles aggregator instead for new and existing use cases, although we will continue to support Approximate Histogram for backwards compatibility. Miscellaneous Aggregations Filtered Aggregator A filtered aggregator wraps any given aggregator, but only aggregates the values for which the given dimension filter matches. This makes it possible to compute the results of a filtered and an unfiltered aggregation simultaneously, without having to issue multiple queries, and use both results as part of post-aggregations. Note: If only the filtered results are required, consider putting the filter on the query itself, which will be much faster since it does not require scanning all the data. { \"type\" : \"filtered\", \"filter\" : { \"type\" : \"selector\", \"dimension\" : , \"value\" : }, \"aggregator\" : } Grouping Aggregator A grouping aggregator can only be used as part of GroupBy queries which have a subtotal spec. It returns a number for each output row that lets you infer whether a particular dimension is included in the sub-grouping used for that row. You can pass a non-empty list of dimensions to this aggregator which must be a subset of dimensions that you are grouping on. E.g if the aggregator has [\"dim1\", \"dim2\"] as input dimensions and [[\"dim1\", \"dim2\"], [\"dim1\"], [\"dim2\"], []] as subtotals, following can be the possible output of the aggregator subtotal used in query Output (bits representation) [\"dim1\", \"dim2\"] 0 (00) [\"dim1\"] 1 (01) [\"dim2\"] 2 (10) [] 3 (11) As illustrated in above example, output number can be thought of as an unsigned n bit number where n is the number of dimensions passed to the aggregator. The bit at position X is set in this number to 0 if a dimension at position X in input to aggregators is included in the sub-grouping. Otherwise, this bit is set to 1. { \"type\" : \"grouping\", \"name\" : , \"groupings\" : [] } 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/post-aggregations.html":{"url":"querying/post-aggregations.html","title":"后置聚合","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. Post-aggregations are specifications of processing that should happen on aggregated values as they come out of Apache Druid. If you include a post aggregation as part of a query, make sure to include all aggregators the post-aggregator requires. There are several post-aggregators available. Arithmetic post-aggregator The arithmetic post-aggregator applies the provided function to the given fields from left to right. The fields can be aggregators or other post aggregators. Supported functions are +, -, *, /, and quotient. Note: / division always returns 0 if dividing by0, regardless of the numerator. quotient division behaves like regular floating point division Arithmetic post-aggregators may also specify an ordering, which defines the order of resulting values when sorting results (this can be useful for topN queries for instance): If no ordering (or null) is specified, the default floating point ordering is used. numericFirst ordering always returns finite values first, followed by NaN, and infinite values last. The grammar for an arithmetic post aggregation is: postAggregation : { \"type\" : \"arithmetic\", \"name\" : , \"fn\" : , \"fields\": [, , ...], \"ordering\" : } Field accessor post-aggregators These post-aggregators return the value produced by the specified aggregator. fieldName refers to the output name of the aggregator given in the aggregations portion of the query. For complex aggregators, like \"cardinality\" and \"hyperUnique\", the type of the post-aggregator determines what the post-aggregator will return. Use type \"fieldAccess\" to return the raw aggregation object, or use type \"finalizingFieldAccess\" to return a finalized value, such as an estimated cardinality. { \"type\" : \"fieldAccess\", \"name\": , \"fieldName\" : } or { \"type\" : \"finalizingFieldAccess\", \"name\": , \"fieldName\" : } Constant post-aggregator The constant post-aggregator always returns the specified value. { \"type\" : \"constant\", \"name\" : , \"value\" : } Greatest / Least post-aggregators doubleGreatest and longGreatest computes the maximum of all fields and Double.NEGATIVE_INFINITY. doubleLeast and longLeast computes the minimum of all fields and Double.POSITIVE_INFINITY. The difference between the doubleMax aggregator and the doubleGreatest post-aggregator is that doubleMax returns the highest value of all rows for one specific column while doubleGreatest returns the highest value of multiple columns in one row. These are similar to the SQL MAX and GREATEST functions. Example: { \"type\" : \"doubleGreatest\", \"name\" : , \"fields\": [, , ...] } JavaScript post-aggregator Applies the provided JavaScript function to the given fields. Fields are passed as arguments to the JavaScript function in the given order. postAggregation : { \"type\": \"javascript\", \"name\": , \"fieldNames\" : [, , ...], \"function\": } Example JavaScript aggregator: { \"type\": \"javascript\", \"name\": \"absPercent\", \"fieldNames\": [\"delta\", \"total\"], \"function\": \"function(delta, total) { return 100 * Math.abs(delta) / total; }\" } JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. HyperUnique Cardinality post-aggregator The hyperUniqueCardinality post aggregator is used to wrap a hyperUnique object such that it can be used in post aggregations. { \"type\" : \"hyperUniqueCardinality\", \"name\": , \"fieldName\" : } It can be used in a sample calculation as so: \"aggregations\" : [{ {\"type\" : \"count\", \"name\" : \"rows\"}, {\"type\" : \"hyperUnique\", \"name\" : \"unique_users\", \"fieldName\" : \"uniques\"} }], \"postAggregations\" : [{ \"type\" : \"arithmetic\", \"name\" : \"average_users_per_row\", \"fn\" : \"/\", \"fields\" : [ { \"type\" : \"hyperUniqueCardinality\", \"fieldName\" : \"unique_users\" }, { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" } ] }] This post-aggregator will inherit the rounding behavior of the aggregator it references. Note that this inheritance is only effective if you directly reference an aggregator. Going through another post-aggregator, for example, will cause the user-specified rounding behavior to get lost and default to \"no rounding\". Example Usage In this example, let’s calculate a simple percentage using post aggregators. Let’s imagine our data set has a metric called \"total\". The format of the query JSON is as follows: { ... \"aggregations\" : [ { \"type\" : \"count\", \"name\" : \"rows\" }, { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" } ], \"postAggregations\" : [{ \"type\" : \"arithmetic\", \"name\" : \"average\", \"fn\" : \"/\", \"fields\" : [ { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" }, { \"type\" : \"fieldAccess\", \"name\" : \"rows\", \"fieldName\" : \"rows\" } ] }] ... } { ... \"aggregations\" : [ { \"type\" : \"doubleSum\", \"name\" : \"tot\", \"fieldName\" : \"total\" }, { \"type\" : \"doubleSum\", \"name\" : \"part\", \"fieldName\" : \"part\" } ], \"postAggregations\" : [{ \"type\" : \"arithmetic\", \"name\" : \"part_percentage\", \"fn\" : \"*\", \"fields\" : [ { \"type\" : \"arithmetic\", \"name\" : \"ratio\", \"fn\" : \"/\", \"fields\" : [ { \"type\" : \"fieldAccess\", \"name\" : \"part\", \"fieldName\" : \"part\" }, { \"type\" : \"fieldAccess\", \"name\" : \"tot\", \"fieldName\" : \"tot\" } ] }, { \"type\" : \"constant\", \"name\": \"const\", \"value\" : 100 } ] }] ... } 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"misc/math-expr.html":{"url":"misc/math-expr.html","title":"数学函数","keywords":"","body":" Apache Druid supports two query languages: native queries and Druid SQL. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. Expressions are used in various places in the native query language, including virtual columns and join conditions. They are also generated by most Druid SQL functions during the query translation process. This expression language supports the following operators (listed in decreasing order of precedence). Operators Description !, - Unary NOT and Minus ^ Binary power op *, /, % Binary multiplicative +, - Binary additive , >=, ==, != Binary Comparison &&, || Binary Logical AND, OR Long, double, and string data types are supported. If a number contains a dot, it is interpreted as a double, otherwise it is interpreted as a long. That means, always add a '.' to your number if you want it interpreted as a double value. String literals should be quoted by single quotation marks. Additionally, the expression language supports long, double, and string arrays. Array literals are created by wrapping square brackets around a list of scalar literals values delimited by a comma or space character. All values in an array literal must be the same type, however null values are accepted. Typed empty arrays may be defined by prefixing with their type in angle brackets: [], [], or []. Expressions can contain variables. Variable names may contain letters, digits, '_' and '$'. Variable names must not begin with a digit. To escape other special characters, you can quote it with double quotation marks. For logical operators, a number is true if and only if it is positive (0 or negative value means false). For string type, it's the evaluation result of 'Boolean.valueOf(string)'. Multi-value string dimensions are supported and may be treated as either scalar or array typed values. When treated as a scalar type, an expression will automatically be transformed to apply the scalar operation across all values of the multi-valued type, to mimic Druid's native behavior. Values that result in arrays will be coerced back into the native Druid string type for aggregation. Druid aggregations on multi-value string dimensions on the individual values, not the 'array', behaving similar to the UNNEST operator available in many SQL dialects. However, by using the array_to_string function, aggregations may be done on a stringified version of the complete array, allowing the complete row to be preserved. Using string_to_array in an expression post-aggregator, allows transforming the stringified dimension back into the true native array type. The following built-in functions are available. General functions name description cast cast(expr,'LONG' or 'DOUBLE' or 'STRING' or 'LONG_ARRAY', or 'DOUBLE_ARRAY' or 'STRING_ARRAY') returns expr with specified type. exception can be thrown. Scalar types may be cast to array types and will take the form of a single element list (null will still be null). if if(predicate,then,else) returns 'then' if 'predicate' evaluates to a positive number, otherwise it returns 'else' nvl nvl(expr,expr-for-null) returns 'expr-for-null' if 'expr' is null (or empty string for string type) like like(expr, pattern[, escape]) is equivalent to SQL expr LIKE pattern case_searched case_searched(expr1, result1, [[expr2, result2, ...], else-result]) case_simple case_simple(expr, value1, result1, [[value2, result2, ...], else-result]) bloom_filter_test bloom_filter_test(expr, filter) tests the value of 'expr' against 'filter', a bloom filter serialized as a base64 string. See bloom filter extension documentation for additional details. String functions name description concat concat(expr, expr...) concatenate a list of strings format format(pattern[, args...]) returns a string formatted in the manner of Java's String.format. like like(expr, pattern[, escape]) is equivalent to SQL expr LIKE pattern lookup lookup(expr, lookup-name) looks up expr in a registered query-time lookup parse_long parse_long(string[, radix]) parses a string as a long with the given radix, or 10 (decimal) if a radix is not provided. regexp_extract regexp_extract(expr, pattern[, index]) applies a regular expression pattern and extracts a capture group index, or null if there is no match. If index is unspecified or zero, returns the substring that matched the pattern. The pattern may match anywhere inside expr; if you want to match the entire string instead, use the ^ and $ markers at the start and end of your pattern. regexp_like regexp_like(expr, pattern) returns whether expr matches regular expression pattern. The pattern may match anywhere inside expr; if you want to match the entire string instead, use the ^ and $ markers at the start and end of your pattern. contains_string contains_string(expr, string) returns whether expr contains string as a substring. This method is case-sensitive. icontains_string contains_string(expr, string) returns whether expr contains string as a substring. This method is case-insensitive. replace replace(expr, pattern, replacement) replaces pattern with replacement substring substring(expr, index, length) behaves like java.lang.String's substring right right(expr, length) returns the rightmost length characters from a string left left(expr, length) returns the leftmost length characters from a string strlen strlen(expr) returns length of a string in UTF-16 code units strpos strpos(haystack, needle[, fromIndex]) returns the position of the needle within the haystack, with indexes starting from 0. The search will begin at fromIndex, or 0 if fromIndex is not specified. If the needle is not found then the function returns -1. trim trim(expr[, chars]) remove leading and trailing characters from expr if they are present in chars. chars defaults to ' ' (space) if not provided. ltrim ltrim(expr[, chars]) remove leading characters from expr if they are present in chars. chars defaults to ' ' (space) if not provided. rtrim rtrim(expr[, chars]) remove trailing characters from expr if they are present in chars. chars defaults to ' ' (space) if not provided. lower lower(expr) converts a string to lowercase upper upper(expr) converts a string to uppercase reverse reverse(expr) reverses a string repeat repeat(expr, N) repeats a string N times lpad lpad(expr, length, chars) returns a string of length from expr left-padded with chars. If length is shorter than the length of expr, the result is expr which is truncated to length. The result will be null if either expr or chars is null. If chars is an empty string, no padding is added, however expr may be trimmed if necessary. rpad rpad(expr, length, chars) returns a string of length from expr right-padded with chars. If length is shorter than the length of expr, the result is expr which is truncated to length. The result will be null if either expr or chars is null. If chars is an empty string, no padding is added, however expr may be trimmed if necessary. Time functions name description timestamp timestamp(expr[,format-string]) parses string expr into date then returns milliseconds from java epoch. without 'format-string' it's regarded as ISO datetime format unix_timestamp same with 'timestamp' function but returns seconds instead timestamp_ceil timestamp_ceil(expr, period, [origin, [timezone]]) rounds up a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". timestamp_floor timestamp_floor(expr, period, [origin, [timezone]]) rounds down a timestamp, returning it as a new timestamp. Period can be any ISO8601 period, like P3M (quarters) or PT12H (half-days). The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". timestamp_shift timestamp_shift(expr, period, step, [timezone]) shifts a timestamp by a period (step times), returning it as a new timestamp. Period can be any ISO8601 period. Step may be negative. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". timestamp_extract timestamp_extract(expr, unit, [timezone]) extracts a time part from expr, returning it as a number. Unit can be EPOCH (number of seconds since 1970-01-01 00:00:00 UTC), SECOND, MINUTE, HOUR, DAY (day of month), DOW (day of week), DOY (day of year), WEEK (week of week year), MONTH (1 through 12), QUARTER (1 through 4), or YEAR. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\" timestamp_parse timestamp_parse(string expr, [pattern, [timezone]]) parses a string into a timestamp using a given Joda DateTimeFormat pattern. If the pattern is not provided, this parses time strings in either ISO8601 or SQL format. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\", and will be used as the time zone for strings that do not include a time zone offset. Pattern and time zone must be literals. Strings that cannot be parsed as timestamps will be returned as nulls. timestamp_format timestamp_format(expr, [pattern, [timezone]]) formats a timestamp as a string with a given Joda DateTimeFormat pattern, or ISO8601 if the pattern is not provided. The time zone, if provided, should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". Pattern and time zone must be literals. Math functions See javadoc of java.lang.Math for detailed explanation for each function. name description abs abs(x) would return the absolute value of x acos acos(x) would return the arc cosine of x asin asin(x) would return the arc sine of x atan atan(x) would return the arc tangent of x atan2 atan2(y, x) would return the angle theta from the conversion of rectangular coordinates (x, y) to polar * coordinates (r, theta) cbrt cbrt(x) would return the cube root of x ceil ceil(x) would return the smallest (closest to negative infinity) double value that is greater than or equal to x and is equal to a mathematical integer copysign copysign(x) would return the first floating-point argument with the sign of the second floating-point argument cos cos(x) would return the trigonometric cosine of x cosh cosh(x) would return the hyperbolic cosine of x cot cot(x) would return the trigonometric cotangent of an angle x div div(x,y) is integer division of x by y exp exp(x) would return Euler's number raised to the power of x expm1 expm1(x) would return e^x-1 floor floor(x) would return the largest (closest to positive infinity) double value that is less than or equal to x and is equal to a mathematical integer getExponent getExponent(x) would return the unbiased exponent used in the representation of x hypot hypot(x, y) would return sqrt(x^2+y^2) without intermediate overflow or underflow log log(x) would return the natural logarithm of x log10 log10(x) would return the base 10 logarithm of x log1p log1p(x) would the natural logarithm of x + 1 max max(x, y) would return the greater of two values min min(x, y) would return the smaller of two values nextafter nextafter(x, y) would return the floating-point number adjacent to the x in the direction of the y nextUp nextUp(x) would return the floating-point value adjacent to x in the direction of positive infinity pi pi would return the constant value of the π pow pow(x, y) would return the value of the x raised to the power of y remainder remainder(x, y) would return the remainder operation on two arguments as prescribed by the IEEE 754 standard rint rint(x) would return value that is closest in value to x and is equal to a mathematical integer round round(x, y) would return the value of the x rounded to the y decimal places. While x can be an integer or floating-point number, y must be an integer. The type of the return value is specified by that of x. y defaults to 0 if omitted. When y is negative, x is rounded on the left side of the y decimal points. If x is NaN, x will return 0. If x is infinity, x will be converted to the nearest finite double. scalb scalb(d, sf) would return d * 2^sf rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set signum signum(x) would return the signum function of the argument x sin sin(x) would return the trigonometric sine of an angle x sinh sinh(x) would return the hyperbolic sine of x sqrt sqrt(x) would return the correctly rounded positive square root of x tan tan(x) would return the trigonometric tangent of an angle x tanh tanh(x) would return the hyperbolic tangent of x todegrees todegrees(x) converts an angle measured in radians to an approximately equivalent angle measured in degrees toradians toradians(x) converts an angle measured in degrees to an approximately equivalent angle measured in radians ulp ulp(x) would return the size of an ulp of the argument x Array functions function description array(expr1,expr ...) constructs an array from the expression arguments, using the type of the first argument as the output array type array_length(arr) returns length of array expression array_offset(arr,long) returns the array element at the 0 based index supplied, or null for an out of range index array_ordinal(arr,long) returns the array element at the 1 based index supplied, or null for an out of range index array_contains(arr,expr) returns 1 if the array contains the element specified by expr, or contains all elements specified by expr if expr is an array, else 0 array_overlap(arr1,arr2) returns 1 if arr1 and arr2 have any elements in common, else 0 array_offset_of(arr,expr) returns the 0 based index of the first occurrence of expr in the array, or -1 or null if druid.generic.useDefaultValueForNull=falseif no matching elements exist in the array. array_ordinal_of(arr,expr) returns the 1 based index of the first occurrence of expr in the array, or -1 or null if druid.generic.useDefaultValueForNull=false if no matching elements exist in the array. array_prepend(expr,arr) adds expr to arr at the beginning, the resulting array type determined by the type of the array array_append(arr1,expr) appends expr to arr, the resulting array type determined by the type of the first array array_concat(arr1,arr2) concatenates 2 arrays, the resulting array type determined by the type of the first array array_slice(arr,start,end) return the subarray of arr from the 0 based index start(inclusive) to end(exclusive), or null, if start is less than 0, greater than length of arr or less than end array_to_string(arr,str) joins all elements of arr by the delimiter specified by str string_to_array(str1,str2) splits str1 into an array on the delimiter specified by str2 Apply functions function description map(lambda,arr) applies a transform specified by a single argument lambda expression to all elements of arr, returning a new array cartesian_map(lambda,arr1,arr2,...) applies a transform specified by a multi argument lambda expression to all elements of the Cartesian product of all input arrays, returning a new array; the number of lambda arguments and array inputs must be the same filter(lambda,arr) filters arr by a single argument lambda, returning a new array with all matching elements, or null if no elements match fold(lambda,arr) folds a 2 argument lambda across arr. The first argument of the lambda is the array element and the second the accumulator, returning a single accumulated value. cartesian_fold(lambda,arr1,arr2,...) folds a multi argument lambda across the Cartesian product of all input arrays. The first arguments of the lambda is the array element and the last is the accumulator, returning a single accumulated value. any(lambda,arr) returns 1 if any element in the array matches the lambda expression, else 0 all(lambda,arr) returns 1 if all elements in the array matches the lambda expression, else 0 Reduction functions Reduction functions operate on zero or more expressions and return a single expression. If no expressions are passed as arguments, then the result is NULL. The expressions must all be convertible to a common data type, which will be the type of the result: If all arguments are NULL, the result is NULL. Otherwise, NULL arguments are ignored. If the arguments comprise a mix of numbers and strings, the arguments are interpreted as strings. If all arguments are integer numbers, the arguments are interpreted as longs. If all arguments are numbers and at least one argument is a double, the arguments are interpreted as doubles. function description greatest([expr1, ...]) Evaluates zero or more expressions and returns the maximum value based on comparisons as described above. least([expr1, ...]) Evaluates zero or more expressions and returns the minimum value based on comparisons as described above. IP address functions For the IPv4 address functions, the address argument can either be an IPv4 dotted-decimal string (e.g., \"192.168.0.1\") or an IP address represented as a long (e.g., 3232235521). The subnet argument should be a string formatted as an IPv4 address subnet in CIDR notation (e.g., \"192.168.0.0/16\"). function description ipv4_match(address, subnet) Returns 1 if the address belongs to the subnet literal, else 0. If address is not a valid IPv4 address, then 0 is returned. This function is more efficient if address is a long instead of a string. ipv4_parse(address) Parses address into an IPv4 address stored as a long. If address is a long that is a valid IPv4 address, then it is passed through. Returns null if address cannot be represented as an IPv4 address. ipv4_stringify(address) Converts address into an IPv4 address dotted-decimal string. If address is a string that is a valid IPv4 address, then it is passed through. Returns null if address cannot be represented as an IPv4 address. Vectorization Support A number of expressions support 'vectorized' query engines supported features: constants and identifiers are supported for any column type cast is supported for numeric and string types math operators: +,-,*,/,%,^ are supported for numeric types comparison operators: =, !=, >, >=, , are supported for numeric types math functions: abs, acos, asin, atan, cbrt, ceil, cos, cosh, cot, exp, expm1, floor, getExponent, log, log10, log1p, nextUp, rint, signum, sin, sinh, sqrt, tan, tanh, toDegrees, toRadians, ulp, atan2, copySign, div, hypot, max, min, nextAfter, pow, remainder, scalb are supported for numeric types time functions: timestamp_floor (with constant granularity argument) is supported for numeric types other: parse_long is supported for numeric and string types 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"querying/having.html":{"url":"querying/having.html","title":"表达式与函数","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. A having clause is a JSON object identifying which rows from a groupBy query should be returned, by specifying conditions on aggregated values. It is essentially the equivalent of the HAVING clause in SQL. Apache Druid supports the following types of having clauses. Query filters Query filter HavingSpecs allow all Druid query filters to be used in the Having part of the query. The grammar for a query filter HavingSpec is: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\" : \"filter\", \"filter\" : } } For example, to use a selector filter: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\" : \"filter\", \"filter\" : { \"type\": \"selector\", \"dimension\" : \"\", \"value\" : \"\" } } } You can use \"filter\" HavingSpecs to filter on the timestamp of result rows by applying a filter to the \"__time\" column. Numeric filters The simplest having clause is a numeric filter. Numeric filters can be used as the base filters for more complex boolean expressions of filters. Here's an example of a having-clause numeric filter: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"greaterThan\", \"aggregation\": \"\", \"value\": } } Equal To The equalTo filter will match rows with a specific aggregate value. The grammar for an equalTo filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"equalTo\", \"aggregation\": \"\", \"value\": } } This is the equivalent of HAVING = . Greater Than The greaterThan filter will match rows with aggregate values greater than the given value. The grammar for a greaterThan filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"greaterThan\", \"aggregation\": \"\", \"value\": } } This is the equivalent of HAVING > . Less Than The lessThan filter will match rows with aggregate values less than the specified value. The grammar for a greaterThan filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"lessThan\", \"aggregation\": \"\", \"value\": } } This is the equivalent of HAVING . Dimension Selector Filter dimSelector The dimSelector filter will match rows with dimension values equal to the specified value. The grammar for a dimSelector filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"dimSelector\", \"dimension\": \"\", \"value\": } } Logical expression filters AND The grammar for an AND filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"and\", \"havingSpecs\": [ { \"type\": \"greaterThan\", \"aggregation\": \"\", \"value\": }, { \"type\": \"lessThan\", \"aggregation\": \"\", \"value\": } ] } } OR The grammar for an OR filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"or\", \"havingSpecs\": [ { \"type\": \"greaterThan\", \"aggregation\": \"\", \"value\": }, { \"type\": \"equalTo\", \"aggregation\": \"\", \"value\": } ] } } NOT The grammar for a NOT filter is as follows: { \"queryType\": \"groupBy\", \"dataSource\": \"sample_datasource\", ... \"having\": { \"type\": \"not\", \"havingSpec\": { \"type\": \"equalTo\", \"aggregation\": \"\", \"value\": } } } 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/limitspec.html":{"url":"querying/limitspec.html","title":"Having过滤器","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about sorting in SQL, refer to the SQL documentation. The limitSpec field provides the functionality to sort and limit the set of results from a groupBy query. If you group by a single dimension and are ordering by a single metric, we highly recommend using TopN Queries instead. The performance will be substantially better. Available options are: DefaultLimitSpec The default limit spec takes a limit and the list of columns to do an orderBy operation over. The grammar is: { \"type\" : \"default\", \"limit\" : , \"offset\" : , \"columns\" : [], } The \"limit\" parameter is the maximum number of rows to return. The \"offset\" parameter tells Druid to skip this many rows when returning results. If both \"limit\" and \"offset\" are provided, then \"offset\" will be applied first, followed by \"limit\". For example, a spec with limit 100 and offset 10 will return 100 rows starting from row number 10. Internally, the query is executed by extending the limit by the offset and then discarding a number of rows equal to the offset. This means that raising the offset will increase resource usage by an amount similar to increasing the limit. Together, \"limit\" and \"offset\" can be used to implement pagination. However, note that if the underlying datasource is modified in between page fetches in ways that affect overall query results, then the different pages will not necessarily align with each other. OrderByColumnSpec OrderByColumnSpecs indicate how to do order by operations. Each order-by condition can be a jsonString or a map of the following form: { \"dimension\" : \"\", \"direction\" : , \"dimensionOrder\" : } If only the dimension is provided (as a JSON string), the default order-by is ascending with lexicographic sorting. See Sorting Orders for more information on the sorting orders specified by \"dimensionOrder\". 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/topnmetricspec.html":{"url":"querying/topnmetricspec.html","title":"TopN排序","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about sorting in SQL, refer to the SQL documentation. In Apache Druid, the topN metric spec specifies how topN values should be sorted. Numeric TopNMetricSpec The simplest metric specification is a String value indicating the metric to sort topN results by. They are included in a topN query with: \"metric\": \"\" The metric field can also be given as a JSON object. The grammar for dimension values sorted by numeric value is shown below: \"metric\": { \"type\": \"numeric\", \"metric\": \"\" } property description required? type this indicates a numeric sort yes metric the actual metric field in which results will be sorted by yes Dimension TopNMetricSpec This metric specification sorts TopN results by dimension value, using one of the sorting orders described here: Sorting Orders property type description required? type String this indicates a sort a dimension's values yes, must be 'dimension' ordering String Specifies the sorting order. Can be one of the following values: \"lexicographic\", \"alphanumeric\", \"numeric\", \"strlen\". See Sorting Orders for more details. no, default: \"lexicographic\" previousStop String the starting point of the sort. For example, if a previousStop value is 'b', all values before 'b' are discarded. This field can be used to paginate through all the dimension values. no The following metricSpec uses lexicographic sorting. \"metric\": { \"type\": \"dimension\", \"ordering\": \"lexicographic\", \"previousStop\": \"\" } Note that in earlier versions of Druid, the functionality provided by the DimensionTopNMetricSpec was handled by two separate spec types, Lexicographic and Alphanumeric (when only two sorting orders were supported). These spec types have been deprecated but are still usable. Inverted TopNMetricSpec Sort dimension values in inverted order, i.e inverts the order of the delegate metric spec. It can be used to sort the values in ascending order. \"metric\": { \"type\": \"inverted\", \"metric\": } property description required? type this indicates an inverted sort yes metric the delegate metric spec. yes 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"querying/sorting-orders.html":{"url":"querying/sorting-orders.html","title":"排序","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. These sorting orders are used by the TopNMetricSpec, SearchQuery, GroupByQuery's LimitSpec, and BoundFilter. Lexicographic Sorts values by converting Strings to their UTF-8 byte array representations and comparing lexicographically, byte-by-byte. Alphanumeric Suitable for strings with both numeric and non-numeric content, e.g.: \"file12 sorts after file2\" See https://github.com/amjjd/java-alphanum for more details on how this ordering sorts values. This ordering is not suitable for numbers with decimal points or negative numbers. For example, \"1.3\" precedes \"1.15\" in this ordering because \"15\" has more significant digits than \"3\". Negative numbers are sorted after positive numbers (because numeric characters precede the \"-\" in the negative numbers). Numeric Sorts values as numbers, supports integers and floating point values. Negative values are supported. This sorting order will try to parse all string values as numbers. Unparseable values are treated as nulls, and nulls precede numbers. When comparing two unparseable values (e.g., \"hello\" and \"world\"), this ordering will sort by comparing the unparsed strings lexicographically. Strlen Sorts values by the their string lengths. When there is a tie, this comparator falls back to using the String compareTo method. Version Sorts values as versions, e.g.: \"10.0 sorts after 9.0\", \"1.0.0-SNAPSHOT sorts after 1.0.0\". See https://maven.apache.org/ref/3.6.0/maven-artifact/apidocs/org/apache/maven/artifact/versioning/ComparableVersion.html for more details on how this ordering sorts values. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"querying/virtual-columns.html":{"url":"querying/virtual-columns.html","title":"虚拟字段","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes the native language. For information about functions available in SQL, refer to the SQL documentation. Virtual columns are queryable column \"views\" created from a set of columns during a query. A virtual column can potentially draw from multiple underlying columns, although a virtual column always presents itself as a single column. Virtual columns can be used as dimensions or as inputs to aggregators. Each Apache Druid query can accept a list of virtual columns as a parameter. The following scan query is provided as an example: { \"queryType\": \"scan\", \"dataSource\": \"page_data\", \"columns\":[], \"virtualColumns\": [ { \"type\": \"expression\", \"name\": \"fooPage\", \"expression\": \"concat('foo' + page)\", \"outputType\": \"STRING\" }, { \"type\": \"expression\", \"name\": \"tripleWordCount\", \"expression\": \"wordCount * 3\", \"outputType\": \"LONG\" } ], \"intervals\": [ \"2013-01-01/2019-01-02\" ] } Virtual column types Expression virtual column The expression virtual column has the following syntax: { \"type\": \"expression\", \"name\": , \"expression\": , \"outputType\": } property description required? name The name of the virtual column. yes expression An expression that takes a row as input and outputs a value for the virtual column. yes outputType The expression's output will be coerced to this type. Can be LONG, FLOAT, DOUBLE, or STRING. no, default is FLOAT 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"development/geo.html":{"url":"development/geo.html","title":"空间过滤","keywords":"","body":" Apache Druid supports two query languages: Druid SQL and native queries. This document describes functionality that is only available in the native language. Apache Druid supports filtering specially spatially indexed columns based on an origin and a bound. Spatial indexing In any of the data specs, there is the option of providing spatial dimensions. For example, for a JSON data spec, spatial dimensions can be specified as follows: { \"type\": \"hadoop\", \"dataSchema\": { \"dataSource\": \"DatasourceName\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"timestamp\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [], \"spatialDimensions\": [{ \"dimName\": \"coordinates\", \"dims\": [\"lat\", \"long\"] }] } } } } } Spatial filters property description required? dimName The name of the spatial dimension. A spatial dimension may be constructed from multiple other dimensions or it may already exist as part of an event. If a spatial dimension already exists, it must be an array of coordinate values. yes dims A list of dimension names that comprise a spatial dimension. no The grammar for a spatial filter is as follows: \"filter\" : { \"type\": \"spatial\", \"dimension\": \"spatialDim\", \"bound\": { \"type\": \"rectangular\", \"minCoords\": [10.0, 20.0], \"maxCoords\": [30.0, 40.0] } } Bound types rectangular property description required? minCoords List of minimum dimension coordinates for coordinates [x, y, z, …] yes maxCoords List of maximum dimension coordinates for coordinates [x, y, z, …] yes radius property description required? coords Origin coordinates in the form [x, y, z, …] yes radius The float radius value yes polygon property description required? abscissa Horizontal coordinate for corners of the polygon yes ordinate Vertical coordinate for corners of the polygon yes 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"configuration/":{"url":"configuration/","title":"概述","keywords":"","body":" This page documents all of the configuration properties for each Druid service type. Recommended Configuration File Organization A recommended way of organizing Druid configuration files can be seen in the conf directory in the Druid package root, shown below: $ ls -R conf druid conf/druid: _common broker coordinator historical middleManager overlord conf/druid/_common: common.runtime.properties log4j2.xml conf/druid/broker: jvm.config runtime.properties conf/druid/coordinator: jvm.config runtime.properties conf/druid/historical: jvm.config runtime.properties conf/druid/middleManager: jvm.config runtime.properties conf/druid/overlord: jvm.config runtime.properties Each directory has a runtime.properties file containing configuration properties for the specific Druid process corresponding to the directory (e.g., historical). The jvm.config files contain JVM flags such as heap sizing properties for each service. Common properties shared by all services are placed in _common/common.runtime.properties. Common Configurations The properties under this section are common configurations that should be shared across all Druid services in a cluster. JVM Configuration Best Practices There are four JVM parameters that we set on all of our processes: -Duser.timezone=UTC This sets the default timezone of the JVM to UTC. We always set this and do not test with other default timezones, so local timezones might work, but they also might uncover weird and interesting bugs. To issue queries in a non-UTC timezone, see query granularities -Dfile.encoding=UTF-8 This is similar to timezone, we test assuming UTF-8. Local encodings might work, but they also might result in weird and interesting bugs. -Djava.io.tmpdir= Various parts of the system that interact with the file system do it via temporary files, and these files can get somewhat large. Many production systems are set up to have small (but fast) /tmp directories, which can be problematic with Druid so we recommend pointing the JVM’s tmp directory to something with a little more meat. This directory should not be volatile tmpfs. This directory should also have good read and write speed and hence NFS mount should strongly be avoided. -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager This allows log4j2 to handle logs for non-log4j2 components (like jetty) which use standard java logging. Extensions Many of Druid's external dependencies can be plugged in as modules. Extensions can be provided using the following configs: Property Description Default druid.extensions.directory The root extension directory where user can put extensions related files. Druid will load extensions stored under this directory. extensions (This is a relative path to Druid's working directory) druid.extensions.hadoopDependenciesDir The root hadoop dependencies directory where user can put hadoop related dependencies files. Druid will load the dependencies based on the hadoop coordinate specified in the hadoop index task. hadoop-dependencies (This is a relative path to Druid's working directory druid.extensions.loadList A JSON array of extensions to load from extension directories by Druid. If it is not specified, its value will be null and Druid will load all the extensions under druid.extensions.directory. If its value is empty list [], then no extensions will be loaded at all. It is also allowed to specify absolute path of other custom extensions not stored in the common extensions directory. null druid.extensions.searchCurrentClassloader This is a boolean flag that determines if Druid will search the main classloader for extensions. It defaults to true but can be turned off if you have reason to not automatically add all modules on the classpath. true druid.extensions.useExtensionClassloaderFirst This is a boolean flag that determines if Druid extensions should prefer loading classes from their own jars rather than jars bundled with Druid. If false, extensions must be compatible with classes provided by any jars bundled with Druid. If true, extensions may depend on conflicting versions. false druid.extensions.hadoopContainerDruidClasspath Hadoop Indexing launches hadoop jobs and this configuration provides way to explicitly set the user classpath for the hadoop job. By default this is computed automatically by druid based on the druid process classpath and set of extensions. However, sometimes you might want to be explicit to resolve dependency conflicts between druid and hadoop. null druid.extensions.addExtensionsToHadoopContainer Only applicable if druid.extensions.hadoopContainerDruidClasspath is provided. If set to true, then extensions specified in the loadList are added to hadoop container classpath. Note that when druid.extensions.hadoopContainerDruidClasspath is not provided then extensions are always added to hadoop container classpath. false Modules Property Description Default druid.modules.excludeList A JSON array of canonical class names (e.g., \"org.apache.druid.somepackage.SomeModule\") of module classes which shouldn't be loaded, even if they are found in extensions specified by druid.extensions.loadList, or in the list of core modules specified to be loaded on a particular Druid process type. Useful when some useful extension contains some module, which shouldn't be loaded on some Druid process type because some dependencies of that module couldn't be satisfied. [] Zookeeper We recommend just setting the base ZK path and the ZK service host, but all ZK paths that Druid uses can be overwritten to absolute paths. Property Description Default druid.zk.paths.base Base Zookeeper path. /druid druid.zk.service.host The ZooKeeper hosts to connect to. This is a REQUIRED property and therefore a host address must be supplied. none druid.zk.service.user The username to authenticate with ZooKeeper. This is an optional property. none druid.zk.service.pwd The Password Provider or the string password to authenticate with ZooKeeper. This is an optional property. none druid.zk.service.authScheme digest is the only authentication scheme supported. digest Zookeeper Behavior Property Description Default druid.zk.service.sessionTimeoutMs ZooKeeper session timeout, in milliseconds. 30000 druid.zk.service.connectionTimeoutMs ZooKeeper connection timeout, in milliseconds. 15000 druid.zk.service.compress Boolean flag for whether or not created Znodes should be compressed. true druid.zk.service.acl Boolean flag for whether or not to enable ACL security for ZooKeeper. If ACL is enabled, zNode creators will have all permissions. false Path Configuration Druid interacts with ZK through a set of standard path configurations. We recommend just setting the base ZK path, but all ZK paths that Druid uses can be overwritten to absolute paths. Property Description Default druid.zk.paths.base Base Zookeeper path. /druid druid.zk.paths.propertiesPath Zookeeper properties path. ${druid.zk.paths.base}/properties druid.zk.paths.announcementsPath Druid process announcement path. ${druid.zk.paths.base}/announcements druid.zk.paths.liveSegmentsPath Current path for where Druid processes announce their segments. ${druid.zk.paths.base}/segments druid.zk.paths.loadQueuePath Entries here cause Historical processes to load and drop segments. ${druid.zk.paths.base}/loadQueue druid.zk.paths.coordinatorPath Used by the Coordinator for leader election. ${druid.zk.paths.base}/coordinator druid.zk.paths.servedSegmentsPath @Deprecated. Legacy path for where Druid processes announce their segments. ${druid.zk.paths.base}/servedSegments The indexing service also uses its own set of paths. These configs can be included in the common configuration. Property Description Default druid.zk.paths.indexer.base Base zookeeper path for ${druid.zk.paths.base}/indexer druid.zk.paths.indexer.announcementsPath Middle managers announce themselves here. ${druid.zk.paths.indexer.base}/announcements druid.zk.paths.indexer.tasksPath Used to assign tasks to MiddleManagers. ${druid.zk.paths.indexer.base}/tasks druid.zk.paths.indexer.statusPath Parent path for announcement of task statuses. ${druid.zk.paths.indexer.base}/status If druid.zk.paths.base and druid.zk.paths.indexer.base are both set, and none of the other druid.zk.paths.* or druid.zk.paths.indexer.* values are set, then the other properties will be evaluated relative to their respective base. For example, if druid.zk.paths.base is set to /druid1 and druid.zk.paths.indexer.base is set to /druid2 then druid.zk.paths.announcementsPath will default to /druid1/announcements while druid.zk.paths.indexer.announcementsPath will default to /druid2/announcements. The following path is used for service discovery. It is not affected by druid.zk.paths.base and must be specified separately. Property Description Default druid.discovery.curator.path Services announce themselves under this ZooKeeper path. /druid/discovery Exhibitor Exhibitor is a supervisor system for ZooKeeper. Exhibitor can dynamically scale-up/down the cluster of ZooKeeper servers. Druid can update self-owned list of ZooKeeper servers through Exhibitor without restarting. That is, it allows Druid to keep the connections of Exhibitor-supervised ZooKeeper servers. Property Description Default druid.exhibitor.service.hosts A JSON array which contains the hostnames of Exhibitor instances. Please specify this property if you want to use Exhibitor-supervised cluster. none druid.exhibitor.service.port The REST port used to connect to Exhibitor. 8080 druid.exhibitor.service.restUriPath The path of the REST call used to get the server set. /exhibitor/v1/cluster/list druid.exhibitor.service.useSsl Boolean flag for whether or not to use https protocol. false druid.exhibitor.service.pollingMs How often to poll the exhibitors for the list 10000 Note that druid.zk.service.host is used as a backup in case an Exhibitor instance can't be contacted and therefore should still be set. TLS General Configuration Property Description Default druid.enablePlaintextPort Enable/Disable HTTP connector. true druid.enableTlsPort Enable/Disable HTTPS connector. false Although not recommended but both HTTP and HTTPS connectors can be enabled at a time and respective ports are configurable using druid.plaintextPort and druid.tlsPort properties on each process. Please see Configuration section of individual processes to check the valid and default values for these ports. Jetty Server TLS Configuration Druid uses Jetty as an embedded web server. To get familiar with TLS/SSL in general and related concepts like Certificates etc. reading this Jetty documentation might be helpful. To get more in depth knowledge of TLS/SSL support in Java in general, please refer to this guide. The documentation here can help in understanding TLS/SSL configurations listed below. This document lists all the possible values for the below mentioned configs among others provided by Java implementation. Property Description Default Required druid.server.https.keyStorePath The file path or URL of the TLS/SSL Key store. none yes druid.server.https.keyStoreType The type of the key store. none yes druid.server.https.certAlias Alias of TLS/SSL certificate for the connector. none yes druid.server.https.keyStorePassword The Password Provider or String password for the Key Store. none yes Following table contains non-mandatory advanced configuration options, use caution. Property Description Default Required druid.server.https.keyManagerFactoryAlgorithm Algorithm to use for creating KeyManager, more details here. javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm() no druid.server.https.keyManagerPassword The Password Provider or String password for the Key Manager. none no druid.server.https.includeCipherSuites List of cipher suite names to include. You can either use the exact cipher suite name or a regular expression. Jetty's default include cipher list no druid.server.https.excludeCipherSuites List of cipher suite names to exclude. You can either use the exact cipher suite name or a regular expression. Jetty's default exclude cipher list no druid.server.https.includeProtocols List of exact protocols names to include. Jetty's default include protocol list no druid.server.https.excludeProtocols List of exact protocols names to exclude. Jetty's default exclude protocol list no Internal Client TLS Configuration (requires simple-client-sslcontext extension) These properties apply to the SSLContext that will be provided to the internal HTTP client that Druid services use to communicate with each other. These properties require the simple-client-sslcontext extension to be loaded. Without it, Druid services will be unable to communicate with each other when TLS is enabled. Property Description Default Required druid.client.https.protocol SSL protocol to use. TLSv1.2 no druid.client.https.trustStoreType The type of the key store where trusted root certificates are stored. java.security.KeyStore.getDefaultType() no druid.client.https.trustStorePath The file path or URL of the TLS/SSL Key store where trusted root certificates are stored. none yes druid.client.https.trustStoreAlgorithm Algorithm to be used by TrustManager to validate certificate chains javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm() no druid.client.https.trustStorePassword The Password Provider or String password for the Trust Store. none yes This document lists all the possible values for the above mentioned configs among others provided by Java implementation. Authentication and Authorization Property Type Description Default Required druid.auth.authenticatorChain JSON List of Strings List of Authenticator type names [\"allowAll\"] no druid.escalator.type String Type of the Escalator that should be used for internal Druid communications. This Escalator must use an authentication scheme that is supported by an Authenticator in druid.auth.authenticatorChain. \"noop\" no druid.auth.authorizers JSON List of Strings List of Authorizer type names [\"allowAll\"] no druid.auth.unsecuredPaths List of Strings List of paths for which security checks will not be performed. All requests to these paths will be allowed. [] no druid.auth.allowUnauthenticatedHttpOptions Boolean If true, skip authentication checks for HTTP OPTIONS requests. This is needed for certain use cases, such as supporting CORS pre-flight requests. Note that disabling authentication checks for OPTIONS requests will allow unauthenticated users to determine what Druid endpoints are valid (by checking if the OPTIONS request returns a 200 instead of 404), so enabling this option may reveal information about server configuration, including information about what extensions are loaded (if those extensions add endpoints). false no For more information, please see Authentication and Authorization. For configuration options for specific auth extensions, please refer to the extension documentation. Startup Logging All processes can log debugging information on startup. Property Description Default druid.startup.logging.logProperties Log all properties on startup (from common.runtime.properties, runtime.properties, and the JVM command line). false druid.startup.logging.maskProperties Masks sensitive properties (passwords, for example) containing theses words. [\"password\"] Note that some sensitive information may be logged if these settings are enabled. Request Logging All processes that can serve queries can also log the query requests they see. Broker processes can additionally log the SQL requests (both from HTTP and JDBC) they see. Property Description Default druid.request.logging.type Choices: noop, file, emitter, slf4j, filtered, composing, switching. How to log every query request. [required to configure request logging] Note that, you can enable sending all the HTTP requests to log by setting \"org.apache.druid.jetty.RequestLog\" to DEBUG level. See Logging File Request Logging Daily request logs are stored on disk. Property Description Default druid.request.logging.dir Historical, Realtime and Broker processes maintain request logs of all of the requests they get (interaction is via POST, so normal request logs don’t generally capture information about the actual query), this specifies the directory to store the request logs in none druid.request.logging.filePattern Joda datetime format for each file \"yyyy-MM-dd'.log'\" The format of request logs is TSV, one line per requests, with five fields: timestamp, remote_addr, native_query, query_context, sql_query. For native JSON request, the sql_query field is empty. Example 2019-01-14T10:00:00.000Z 127.0.0.1 {\"queryType\":\"topN\",\"dataSource\":{\"type\":\"table\",\"name\":\"wikiticker\"},\"virtualColumns\":[],\"dimension\":{\"type\":\"LegacyDimensionSpec\",\"dimension\":\"page\",\"outputName\":\"page\",\"outputType\":\"STRING\"},\"metric\":{\"type\":\"LegacyTopNMetricSpec\",\"metric\":\"count\"},\"threshold\":10,\"intervals\":{\"type\":\"LegacySegmentSpec\",\"intervals\":[\"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z\"]},\"filter\":null,\"granularity\":{\"type\":\"all\"},\"aggregations\":[{\"type\":\"count\",\"name\":\"count\"}],\"postAggregations\":[],\"context\":{\"queryId\":\"74c2d540-d700-4ebd-b4a9-3d02397976aa\"},\"descending\":false} {\"query/time\":100,\"query/bytes\":800,\"success\":true,\"identity\":\"user1\"} For SQL query request, the native_query field is empty. Example 2019-01-14T10:00:00.000Z 127.0.0.1 {\"sqlQuery/time\":100,\"sqlQuery/bytes\":600,\"success\":true,\"identity\":\"user1\"} {\"query\":\"SELECT page, COUNT(*) AS Edits FROM wikiticker WHERE __time BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\",\"context\":{\"sqlQueryId\":\"c9d035a0-5ffd-4a79-a865-3ffdadbb5fdd\",\"nativeQueryIds\":\"[490978e4-f5c7-4cf6-b174-346e63cf8863]\"}} Emitter Request Logging Every request is emitted to some external location. Property Description Default druid.request.logging.feed Feed name for requests. none SLF4J Request Logging Every request is logged via SLF4J. Native queries are serialized into JSON in the log message regardless of the SLF4J format specification. They will be logged under the class org.apache.druid.server.log.LoggingRequestLogger. Property Description Default druid.request.logging.setMDC If MDC entries should be set in the log entry. Your logging setup still has to be configured to handle MDC to format this data false druid.request.logging.setContextMDC If the druid query context should be added to the MDC entries. Has no effect unless setMDC is true false For native query, the following MDC fields are populated with setMDC: MDC field Description queryId The query ID sqlQueryId The SQL query ID if this query is part of a SQL request dataSource The datasource the query was against queryType The type of the query hasFilters If the query has any filters remoteAddr The remote address of the requesting client duration The duration of the query interval resultOrdering The ordering of results descending If the query is a descending query Filtered Request Logging Filtered Request Logger filters requests based on a configurable query/time threshold (for native query) and sqlQuery/time threshold (for SQL query). For native query, only request logs where query/time is above the threshold are emitted. For SQL query, only request logs where sqlQuery/time is above the threshold are emitted. Property Description Default druid.request.logging.queryTimeThresholdMs Threshold value for query/time in milliseconds. 0, i.e., no filtering druid.request.logging.sqlQueryTimeThresholdMs Threshold value for sqlQuery/time in milliseconds. 0, i.e., no filtering druid.request.logging.mutedQueryTypes Query requests of these types are not logged. Query types are defined as string objects corresponding to the \"queryType\" value for the specified query in the Druid's native JSON query API. Misspelled query types will be ignored. Example to ignore scan and timeBoundary queries: [\"scan\", \"timeBoundary\"] [] druid.request.logging.delegate.type Type of delegate request logger to log requests. none Composite Request Logging Composite Request Logger emits request logs to multiple request loggers. Property Description Default druid.request.logging.loggerProviders List of request loggers for emitting request logs. none Switching Request Logging Switching Request Logger routes native query's request logs to one request logger and SQL query's request logs to another request logger. Property Description Default druid.request.logging.nativeQueryLogger request logger for emitting native query's request logs. none druid.request.logging.sqlQueryLogger request logger for emitting SQL query's request logs. none Audit Logging Coordinator and Overlord log changes to lookups, segment load/drop rules, dynamic configuration changes for auditing Property Description Default druid.audit.manager.auditHistoryMillis Default duration for querying audit history. 1 week druid.audit.manager.includePayloadAsDimensionInMetric Boolean flag on whether to add payload column in service metric. false Enabling Metrics Druid processes periodically emit metrics and different metrics monitors can be included. Each process can overwrite the default list of monitors. Property Description Default druid.monitoring.emissionPeriod How often metrics are emitted. PT1M druid.monitoring.monitors Sets list of Druid monitors used by a process. See below for names and more information. For example, you can specify monitors for a Broker with druid.monitoring.monitors=[\"org.apache.druid.java.util.metrics.SysMonitor\",\"org.apache.druid.java.util.metrics.JvmMonitor\"]. none (no monitors) The following monitors are available: Name Description org.apache.druid.client.cache.CacheMonitor Emits metrics (to logs) about the segment results cache for Historical and Broker processes. Reports typical cache statistics include hits, misses, rates, and size (bytes and number of entries), as well as timeouts and and errors. org.apache.druid.java.util.metrics.SysMonitor This uses the SIGAR library to report on various system activities and statuses. org.apache.druid.server.metrics.HistoricalMetricsMonitor Reports statistics on Historical processes. org.apache.druid.java.util.metrics.JvmMonitor Reports various JVM-related statistics. org.apache.druid.java.util.metrics.JvmCpuMonitor Reports statistics of CPU consumption by the JVM. org.apache.druid.java.util.metrics.CpuAcctDeltaMonitor Reports consumed CPU as per the cpuacct cgroup. org.apache.druid.java.util.metrics.JvmThreadsMonitor Reports Thread statistics in the JVM, like numbers of total, daemon, started, died threads. org.apache.druid.segment.realtime.RealtimeMetricsMonitor Reports statistics on Realtime processes. org.apache.druid.server.metrics.EventReceiverFirehoseMonitor Reports how many events have been queued in the EventReceiverFirehose. org.apache.druid.server.metrics.QueryCountStatsMonitor Reports how many queries have been successful/failed/interrupted. org.apache.druid.server.emitter.HttpEmittingMonitor Reports internal metrics of http or parametrized emitter (see below). Must not be used with another emitter type. See the description of the metrics here: https://github.com/apache/druid/pull/4973. org.apache.druid.server.metrics.TaskCountStatsMonitor Reports how many ingestion tasks are currently running/pending/waiting and also the number of successful/failed tasks per emission period. Emitting Metrics The Druid servers emit various metrics and alerts via something we call an Emitter. There are three emitter implementations included with the code, a \"noop\" emitter (the default if none is specified), one that just logs to log4j (\"logging\"), and one that does POSTs of JSON events to a server (\"http\"). The properties for using the logging emitter are described below. Property Description Default druid.emitter Setting this value to \"noop\", \"logging\", \"http\" or \"parametrized\" will initialize one of the emitter modules. The value \"composing\" can be used to initialize multiple emitter modules. noop Logging Emitter Module Property Description Default druid.emitter.logging.loggerClass Choices: HttpPostEmitter, LoggingEmitter, NoopServiceEmitter, ServiceEmitter. The class used for logging. LoggingEmitter druid.emitter.logging.logLevel Choices: debug, info, warn, error. The log level at which message are logged. info Http Emitter Module Property Description Default druid.emitter.http.flushMillis How often the internal message buffer is flushed (data is sent). 60000 druid.emitter.http.flushCount How many messages the internal message buffer can hold before flushing (sending). 500 druid.emitter.http.basicAuthentication Password Provider for providing Login and password for authentication in \"login:password\" form, e.g., druid.emitter.http.basicAuthentication=admin:adminpassword uses Default Password Provider which allows plain text passwords. not specified = no authentication druid.emitter.http.flushTimeOut The timeout after which an event should be sent to the endpoint, even if internal buffers are not filled, in milliseconds. not specified = no timeout druid.emitter.http.batchingStrategy The strategy of how the batch is formatted. \"ARRAY\" means [event1,event2], \"NEWLINES\" means event1\\nevent2, ONLY_EVENTS means event1event2. ARRAY druid.emitter.http.maxBatchSize The maximum batch size, in bytes. the minimum of (10% of JVM heap size divided by 2) or (5191680 (i. e. 5 MB)) druid.emitter.http.batchQueueSizeLimit The maximum number of batches in emitter queue, if there are problems with emitting. the maximum of (2) or (10% of the JVM heap size divided by 5MB) druid.emitter.http.minHttpTimeoutMillis If the speed of filling batches imposes timeout smaller than that, not even trying to send batch to endpoint, because it will likely fail, not being able to send the data that fast. Configure this depending based on emitter/successfulSending/minTimeMs metric. Reasonable values are 10ms..100ms. 0 druid.emitter.http.recipientBaseUrl The base URL to emit messages to. Druid will POST JSON to be consumed at the HTTP endpoint specified by this property. none, required config Http Emitter Module TLS Overrides When emitting events to a TLS-enabled receiver, the Http Emitter will by default use an SSLContext obtained via the process described at Druid's internal communication over TLS, i.e., the same SSLContext that would be used for internal communications between Druid processes. In some use cases it may be desirable to have the Http Emitter use its own separate truststore configuration. For example, there may be organizational policies that prevent the TLS-enabled metrics receiver's certificate from being added to the same truststore used by Druid's internal HTTP client. The following properties allow the Http Emitter to use its own truststore configuration when building its SSLContext. Property Description Default druid.emitter.http.ssl.useDefaultJavaContext If set to true, the HttpEmitter will use SSLContext.getDefault(), the default Java SSLContext, and all other properties below are ignored. false druid.emitter.http.ssl.trustStorePath The file path or URL of the TLS/SSL Key store where trusted root certificates are stored. If this is unspecified, the Http Emitter will use the same SSLContext as Druid's internal HTTP client, as described in the beginning of this section, and all other properties below are ignored. null druid.emitter.http.ssl.trustStoreType The type of the key store where trusted root certificates are stored. java.security.KeyStore.getDefaultType() druid.emitter.http.ssl.trustStoreAlgorithm Algorithm to be used by TrustManager to validate certificate chains javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm() druid.emitter.http.ssl.trustStorePassword The Password Provider or String password for the Trust Store. none druid.emitter.http.ssl.protocol TLS protocol to use. \"TLSv1.2\" Parametrized Http Emitter Module druid.emitter.parametrized.httpEmitting.* configs correspond to the configs of Http Emitter Modules, see above. Except recipientBaseUrl. E.g., druid.emitter.parametrized.httpEmitting.flushMillis, druid.emitter.parametrized.httpEmitting.flushCount, druid.emitter.parametrized.httpEmitting.ssl.trustStorePath, etc. The additional configs are: Property Description Default druid.emitter.parametrized.recipientBaseUrlPattern The URL pattern to send an event to, based on the event's feed. E.g., http://foo.bar/{feed}, that will send event to http://foo.bar/metrics if the event's feed is \"metrics\". none, required config Composing Emitter Module Property Description Default druid.emitter.composing.emitters List of emitter modules to load, e.g., [\"logging\",\"http\"]. [] Graphite Emitter To use graphite as emitter set druid.emitter=graphite. For configuration details please follow this link. Metadata storage These properties specify the JDBC connection and other configuration around the metadata storage. The only processes that connect to the metadata storage with these properties are the Coordinator and Overlord. Property Description Default druid.metadata.storage.type The type of metadata storage to use. Choose from \"mysql\", \"postgresql\", or \"derby\". derby druid.metadata.storage.connector.connectURI The JDBC URI for the database to connect to none druid.metadata.storage.connector.user The username to connect with. none druid.metadata.storage.connector.password The Password Provider or String password used to connect with. none druid.metadata.storage.connector.createTables If Druid requires a table and it doesn't exist, create it? true druid.metadata.storage.tables.base The base name for tables. druid druid.metadata.storage.tables.dataSource The table to use to look for dataSources which created by Kafka Indexing Service. druid_dataSource druid.metadata.storage.tables.pendingSegments The table to use to look for pending segments. druid_pendingSegments druid.metadata.storage.tables.segments The table to use to look for segments. druid_segments druid.metadata.storage.tables.rules The table to use to look for segment load/drop rules. druid_rules druid.metadata.storage.tables.config The table to use to look for configs. druid_config druid.metadata.storage.tables.tasks Used by the indexing service to store tasks. druid_tasks druid.metadata.storage.tables.taskLog Used by the indexing service to store task logs. druid_taskLog druid.metadata.storage.tables.taskLock Used by the indexing service to store task locks. druid_taskLock druid.metadata.storage.tables.supervisors Used by the indexing service to store supervisor configurations. druid_supervisors druid.metadata.storage.tables.audit The table to use for audit history of configuration changes, e.g., Coordinator rules. druid_audit Deep storage The configurations concern how to push and pull Segments from deep storage. Property Description Default druid.storage.type Choices:local, noop, s3, hdfs, c*. The type of deep storage to use. local Local Deep Storage Local deep storage uses the local filesystem. Property Description Default druid.storage.storageDirectory Directory on disk to use as deep storage. /tmp/druid/localStorage Noop Deep Storage This deep storage doesn't do anything. There are no configs. S3 Deep Storage This deep storage is used to interface with Amazon's S3. Note that the druid-s3-extensions extension must be loaded. The below table shows some important configurations for S3. See S3 Deep Storage for full configurations. Property Description Default druid.storage.bucket S3 bucket name. none druid.storage.baseKey S3 object key prefix for storage. none druid.storage.disableAcl Boolean flag for ACL. If this is set to false, the full control would be granted to the bucket owner. This may require to set additional permissions. See S3 permissions settings. false druid.storage.archiveBucket S3 bucket name for archiving when running the archive task. none druid.storage.archiveBaseKey S3 object key prefix for archiving. none druid.storage.sse.type Server-side encryption type. Should be one of s3, kms, and custom. See the below Server-side encryption section for more details. None druid.storage.sse.kms.keyId AWS KMS key ID. This is used only when druid.storage.sse.type is kms and can be empty to use the default key ID. None druid.storage.sse.custom.base64EncodedKey Base64-encoded key. Should be specified if druid.storage.sse.type is custom. None druid.storage.useS3aSchema If true, use the \"s3a\" filesystem when using Hadoop-based ingestion. If false, the \"s3n\" filesystem will be used. Only affects Hadoop-based ingestion. false HDFS Deep Storage This deep storage is used to interface with HDFS. Note that the druid-hdfs-storage extension must be loaded. Property Description Default druid.storage.storageDirectory HDFS directory to use as deep storage. none Cassandra Deep Storage This deep storage is used to interface with Cassandra. Note that the druid-cassandra-storage extension must be loaded. Property Description Default druid.storage.host Cassandra host. none druid.storage.keyspace Cassandra key space. none Ingestion Security Configuration HDFS input source You can set the following property to specify permissible protocols for the HDFS input source and the HDFS firehose. Property Possible Values Description Default druid.ingestion.hdfs.allowedProtocols List of protocols Allowed protocols for the HDFS input source and HDFS firehose. [\"hdfs\"] HTTP input source You can set the following property to specify permissible protocols for the HTTP input source and the HTTP firehose. Property Possible Values Description Default druid.ingestion.http.allowedProtocols List of protocols Allowed protocols for the HTTP input source and HTTP firehose. [\"http\", \"https\"] External Data Access Security Configuration JDBC Connections to External Databases You can use the following properties to specify permissible JDBC options for: SQL input source SQL firehose, globally cached JDBC lookups JDBC Data Fetcher for per-lookup caching. These properties do not apply to metadata storage connections. Property Possible Values Description Default druid.access.jdbc.enforceAllowedProperties Boolean When true, Druid applies druid.access.jdbc.allowedProperties to JDBC connections starting with jdbc:postgresql: or jdbc:mysql:. When false, Druid allows any kind of JDBC connections without JDBC property validation. This config is for backward compatibility especially during upgrades since enforcing allow list can break existing ingestion jobs or lookups based on JDBC. This config is deprecated and will be removed in a future release. true druid.access.jdbc.allowedProperties List of JDBC properties Defines a list of allowed JDBC properties. Druid always enforces the list for all JDBC connections starting with jdbc:postgresql: or jdbc:mysql: if druid.access.jdbc.enforceAllowedProperties is set to true.This option is tested against MySQL connector 5.1.48 and PostgreSQL connector 42.2.14. Other connector versions might not work. [\"useSSL\", \"requireSSL\", \"ssl\", \"sslmode\"] druid.access.jdbc.allowUnknownJdbcUrlFormat Boolean When false, Druid only accepts JDBC connections starting with jdbc:postgresql: or jdbc:mysql:. When true, Druid allows JDBC connections to any kind of database, but only enforces druid.access.jdbc.allowedProperties for PostgreSQL and MySQL. true Task Logging If you are running the indexing service in remote mode, the task logs must be stored in S3, Azure Blob Store, Google Cloud Storage or HDFS. Property Description Default druid.indexer.logs.type Choices:noop, s3, azure, google, hdfs, file. Where to store task logs file You can also configure the Overlord to automatically retain the task logs in log directory and entries in task-related metadata storage tables only for last x milliseconds by configuring following additional properties. Caution: Automatic log file deletion typically works based on log file modification timestamp on the backing store, so large clock skews between druid processes and backing store nodes might result in unintended behavior. Property Description Default druid.indexer.logs.kill.enabled Boolean value for whether to enable deletion of old task logs. If set to true, Overlord will submit kill tasks periodically based on druid.indexer.logs.kill.delay specified, which will delete task logs from the log directory as well as tasks and tasklogs table entries in metadata storage except for tasks created in the last druid.indexer.logs.kill.durationToRetain period. false druid.indexer.logs.kill.durationToRetain Required if kill is enabled. In milliseconds, task logs and entries in task-related metadata storage tables to be retained created in last x milliseconds. None druid.indexer.logs.kill.initialDelay Optional. Number of milliseconds after Overlord start when first auto kill is run. random value less than 300000 (5 mins) druid.indexer.logs.kill.delay Optional. Number of milliseconds of delay between successive executions of auto kill run. 21600000 (6 hours) File Task Logs Store task logs in the local filesystem. Property Description Default druid.indexer.logs.directory Local filesystem path. log S3 Task Logs Store task logs in S3. Note that the druid-s3-extensions extension must be loaded. Property Description Default druid.indexer.logs.s3Bucket S3 bucket name. none druid.indexer.logs.s3Prefix S3 key prefix. none druid.indexer.logs.disableAcl Boolean flag for ACL. If this is set to false, the full control would be granted to the bucket owner. If the task logs bucket is the same as the deep storage (S3) bucket, then the value of this property will need to be set to true if druid.storage.disableAcl has been set to true. false Azure Blob Store Task Logs Store task logs in Azure Blob Store. Note: The druid-azure-extensions extension must be loaded, and this uses the same storage account as the deep storage module for azure. Property Description Default druid.indexer.logs.container The Azure Blob Store container to write logs to none druid.indexer.logs.prefix The path to prepend to logs none Google Cloud Storage Task Logs Store task logs in Google Cloud Storage. Note: The druid-google-extensions extension must be loaded, and this uses the same storage settings as the deep storage module for google. Property Description Default druid.indexer.logs.bucket The Google Cloud Storage bucket to write logs to none druid.indexer.logs.prefix The path to prepend to logs none HDFS Task Logs Store task logs in HDFS. Note that the druid-hdfs-storage extension must be loaded. Property Description Default druid.indexer.logs.directory The directory to store logs. none Overlord Discovery This config is used to find the Overlord using Curator service discovery. Only required if you are actually running an Overlord. Property Description Default druid.selectors.indexing.serviceName The druid.service name of the Overlord process. To start the Overlord with a different name, set it with this property. druid/overlord Coordinator Discovery This config is used to find the Coordinator using Curator service discovery. This config is used by the realtime indexing processes to get information about the segments loaded in the cluster. Property Description Default druid.selectors.coordinator.serviceName The druid.service name of the Coordinator process. To start the Coordinator with a different name, set it with this property. druid/coordinator Announcing Segments You can configure how to announce and unannounce Znodes in ZooKeeper (using Curator). For normal operations you do not need to override any of these configs. Batch Data Segment Announcer In current Druid, multiple data segments may be announced under the same Znode. Property Description Default druid.announcer.segmentsPerNode Each Znode contains info for up to this many segments. 50 druid.announcer.maxBytesPerNode Max byte size for Znode. 524288 druid.announcer.skipDimensionsAndMetrics Skip Dimensions and Metrics list from segment announcements. NOTE: Enabling this will also remove the dimensions and metrics list from Coordinator and Broker endpoints. false druid.announcer.skipLoadSpec Skip segment LoadSpec from segment announcements. NOTE: Enabling this will also remove the loadspec from Coordinator and Broker endpoints. false JavaScript Druid supports dynamic runtime extension through JavaScript functions. This functionality can be configured through the following properties. Property Description Default druid.javascript.enabled Set to \"true\" to enable JavaScript functionality. This affects the JavaScript parser, filter, extractionFn, aggregator, post-aggregator, router strategy, and worker selection strategy. false JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. Double Column storage Prior to version 0.13.0, Druid's storage layer used a 32-bit float representation to store columns created by the doubleSum, doubleMin, and doubleMax aggregators at indexing time. Starting from version 0.13.0 the default will be 64-bit floats for Double columns. Using 64-bit representation for double column will lead to avoid precision loss at the cost of doubling the storage size of such columns. To keep the old format set the system-wide property druid.indexing.doubleStorage=float. You can also use floatSum, floatMin and floatMax to use 32-bit float representation. Support for 64-bit floating point columns was released in Druid 0.11.0, so if you use this feature then older versions of Druid will not be able to read your data segments. Property Description Default druid.indexing.doubleStorage Set to \"float\" to use 32-bit double representation for double columns. double SQL compatible null handling Prior to version 0.13.0, Druid string columns treated '' and null values as interchangeable, and numeric columns were unable to represent null values, coercing null to 0. Druid 0.13.0 introduced a mode which enabled SQL compatible null handling, allowing string columns to distinguish empty strings from nulls, and numeric columns to contain null rows. Property Description Default druid.generic.useDefaultValueForNull When set to true, null values will be stored as '' for string columns and 0 for numeric columns. Set to false to store and query data in SQL compatible mode. true This mode does have a storage size and query performance cost, see segment documentation for more details. HTTP Client All Druid components can communicate with each other over HTTP. Property Description Default druid.global.http.numConnections Size of connection pool per destination URL. If there are more HTTP requests than this number that all need to speak to the same URL, then they will queue up. 20 druid.global.http.compressionCodec Compression codec to communicate with others. May be \"gzip\" or \"identity\". gzip druid.global.http.readTimeout The timeout for data reads. PT15M druid.global.http.unusedConnectionTimeout The timeout for idle connections in connection pool. The connection in the pool will be closed after this timeout and a new one will be established. This timeout should be less than druid.global.http.readTimeout. Set this timeout = ~90% of druid.global.http.readTimeout PT4M druid.global.http.numMaxThreads Maximum number of I/O worker threads max(10, ((number of cores * 17) / 16 + 2) + 30) Master Server This section contains the configuration options for the processes that reside on Master servers (Coordinators and Overlords) in the suggested three-server configuration. Coordinator For general Coordinator Process information, see here. Static Configuration These Coordinator static configurations can be defined in the coordinator/runtime.properties file. Coordinator Process Config Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8081 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8281 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/coordinator Coordinator Operation Property Description Default druid.coordinator.period The run period for the Coordinator. The Coordinator operates by maintaining the current state of the world in memory and periodically looking at the set of \"used\" segments and segments being served to make decisions about whether any changes need to be made to the data topology. This property sets the delay between each of these runs. PT60S druid.coordinator.period.indexingPeriod How often to send compact/merge/conversion tasks to the indexing service. It's recommended to be longer than druid.manager.segments.pollDuration PT1800S (30 mins) druid.coordinator.startDelay The operation of the Coordinator works on the assumption that it has an up-to-date view of the state of the world when it runs, the current ZK interaction code, however, is written in a way that doesn’t allow the Coordinator to know for a fact that it’s done loading the current state of the world. This delay is a hack to give it enough time to believe that it has all the data. PT300S druid.coordinator.load.timeout The timeout duration for when the Coordinator assigns a segment to a Historical process. PT15M druid.coordinator.kill.pendingSegments.on Boolean flag for whether or not the Coordinator clean up old entries in the pendingSegments table of metadata store. If set to true, Coordinator will check the created time of most recently complete task. If it doesn't exist, it finds the created time of the earliest running/pending/waiting tasks. Once the created time is found, then for all dataSources not in the killPendingSegmentsSkipList (see Dynamic configuration), Coordinator will ask the Overlord to clean up the entries 1 day or more older than the found created time in the pendingSegments table. This will be done periodically based on druid.coordinator.period.indexingPeriod specified. true druid.coordinator.kill.on Boolean flag for whether or not the Coordinator should submit kill task for unused segments, that is, hard delete them from metadata store and deep storage. If set to true, then for all whitelisted dataSources (or optionally all), Coordinator will submit tasks periodically based on period specified. These kill tasks will delete all unused segments except for the last durationToRetain period. Whitelist or All can be set via dynamic configuration killAllDataSources and killDataSourceWhitelist described later. false druid.coordinator.kill.period How often to send kill tasks to the indexing service. Value must be greater than druid.coordinator.period.indexingPeriod. Only applies if kill is turned on. P1D (1 Day) druid.coordinator.kill.durationToRetain Do not kill unused segments in last durationToRetain, must be greater or equal to 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid. PT-1S (-1 seconds) druid.coordinator.kill.maxSegments Kill at most n unused segments per kill task submission, must be greater than 0. Only applies and MUST be specified if kill is turned on. Note that default value is invalid. 0 druid.coordinator.balancer.strategy Specify the type of balancing strategy that the coordinator should use to distribute segments among the historicals. cachingCost is logically equivalent to cost but is more CPU-efficient on large clusters and will replace cost in the future versions, users are invited to try it. Use diskNormalized to distribute segments among processes so that the disks fill up uniformly and use random to randomly pick processes to distribute segments. cost druid.coordinator.balancer.cachingCost.awaitInitialization Whether to wait for segment view initialization before creating the cachingCost balancing strategy. This property is enabled only when druid.coordinator.balancer.strategy is cachingCost. If set to 'true', the Coordinator will not start to assign segments, until the segment view is initialized. If set to 'false', the Coordinator will fallback to use the cost balancing strategy only if the segment view is not initialized yet. Notes, it may take much time to wait for the initialization since the cachingCost balancing strategy involves much computing to build itself. false druid.coordinator.loadqueuepeon.repeatDelay The start and repeat delay for the loadqueuepeon, which manages the load and drop of segments. PT0.050S (50 ms) druid.coordinator.asOverlord.enabled Boolean value for whether this Coordinator process should act like an Overlord as well. This configuration allows users to simplify a druid cluster by not having to deploy any standalone Overlord processes. If set to true, then Overlord console is available at http://coordinator-host:port/console.html and be sure to set druid.coordinator.asOverlord.overlordService also. See next. false druid.coordinator.asOverlord.overlordService Required, if druid.coordinator.asOverlord.enabled is true. This must be same value as druid.service on standalone Overlord processes and druid.selectors.indexing.serviceName on Middle Managers. NULL Segment Management Property Possible Values Description Default druid.serverview.type batch or http Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper. batch druid.coordinator.loadqueuepeon.type curator or http Whether to use \"http\" or \"curator\" implementation to assign segment loads/drops to historical curator druid.coordinator.segment.awaitInitializationOnStart true or false Whether the Coordinator will wait for its view of segments to fully initialize before starting up. If set to 'true', the Coordinator's HTTP server will not start up, and the Coordinator will not announce itself as available, until the server view is initialized. true Additional config when \"http\" loadqueuepeon is used Property Description Default druid.coordinator.loadqueuepeon.http.batchSize Number of segment load/drop requests to batch in one HTTP request. Note that it must be smaller than druid.segmentCache.numLoadingThreads config on Historical process. 1 Metadata Retrieval Property Description Default druid.manager.config.pollDuration How often the manager polls the config table for updates. PT1M druid.manager.segments.pollDuration The duration between polls the Coordinator does for updates to the set of active segments. Generally defines the amount of lag time it can take for the Coordinator to notice new segments. PT1M druid.manager.rules.pollDuration The duration between polls the Coordinator does for updates to the set of active rules. Generally defines the amount of lag time it can take for the Coordinator to notice rules. PT1M druid.manager.rules.defaultRule The default rule for the cluster _default druid.manager.rules.alertThreshold The duration after a failed poll upon which an alert should be emitted. PT10M Dynamic Configuration The Coordinator has dynamic configuration to change certain behavior on the fly. The Coordinator uses a JSON spec object from the Druid metadata storage config table. This object is detailed below: It is recommended that you use the Coordinator Console to configure these parameters. However, if you need to do it via HTTP, the JSON object can be submitted to the Coordinator via a POST request at: http://:/druid/coordinator/v1/config Optional Header Parameters for auditing the config change can also be specified. Header Param Name Description Default X-Druid-Author author making the config change \"\" X-Druid-Comment comment describing the change being done \"\" A sample Coordinator dynamic config JSON object is shown below: { \"millisToWaitBeforeDeleting\": 900000, \"mergeBytesLimit\": 100000000, \"mergeSegmentsLimit\" : 1000, \"maxSegmentsToMove\": 5, \"percentOfSegmentsToConsiderPerMove\": 100, \"replicantLifetime\": 15, \"replicationThrottleLimit\": 10, \"emitBalancingStats\": false, \"killDataSourceWhitelist\": [\"wikipedia\", \"testDatasource\"], \"decommissioningNodes\": [\"localhost:8182\", \"localhost:8282\"], \"decommissioningMaxPercentOfMaxSegmentsToMove\": 70, \"pauseCoordination\": false } Issuing a GET request at the same URL will return the spec that is currently in place. A description of the config setup spec is shown below. Property Description Default millisToWaitBeforeDeleting How long does the Coordinator need to be a leader before it can start marking overshadowed segments as unused in metadata storage. 900000 (15 mins) mergeBytesLimit The maximum total uncompressed size in bytes of segments to merge. 524288000L mergeSegmentsLimit The maximum number of segments that can be in a single append task. 100 maxSegmentsToMove The maximum number of segments that can be moved at any given time. 5 percentOfSegmentsToConsiderPerMove The percentage of the total number of segments in the cluster that are considered every time a segment needs to be selected for a move. Druid orders servers by available capacity ascending (the least available capacity first) and then iterates over the servers. For each server, Druid iterates over the segments on the server, considering them for moving. The default config of 100% means that every segment on every server is a candidate to be moved. This should make sense for most small to medium-sized clusters. However, an admin may find it preferable to drop this value lower if they don't think that it is worthwhile to consider every single segment in the cluster each time it is looking for a segment to move. 100 replicantLifetime The maximum number of Coordinator runs for a segment to be replicated before we start alerting. 15 replicationThrottleLimit The maximum number of segments that can be replicated at one time. 10 balancerComputeThreads Thread pool size for computing moving cost of segments in segment balancing. Consider increasing this if you have a lot of segments and moving segments starts to get stuck. 1 emitBalancingStats Boolean flag for whether or not we should emit balancing stats. This is an expensive operation. false killDataSourceWhitelist List of specific data sources for which kill tasks are sent if property druid.coordinator.kill.on is true. This can be a list of comma-separated data source names or a JSON array. none killAllDataSources Send kill tasks for ALL dataSources if property druid.coordinator.kill.on is true. If this is set to true then killDataSourceWhitelist must not be specified or be empty list. false killPendingSegmentsSkipList List of data sources for which pendingSegments are NOT cleaned up if property druid.coordinator.kill.pendingSegments.on is true. This can be a list of comma-separated data sources or a JSON array. none maxSegmentsInNodeLoadingQueue The maximum number of segments that could be queued for loading to any given server. This parameter could be used to speed up segments loading process, especially if there are \"slow\" nodes in the cluster (with low loading speed) or if too much segments scheduled to be replicated to some particular node (faster loading could be preferred to better segments distribution). Desired value depends on segments loading speed, acceptable replication time and number of nodes. Value 1000 could be a start point for a rather big cluster. Default value is 0 (loading queue is unbounded) 0 decommissioningNodes List of historical servers to 'decommission'. Coordinator will not assign new segments to 'decommissioning' servers, and segments will be moved away from them to be placed on non-decommissioning servers at the maximum rate specified by decommissioningMaxPercentOfMaxSegmentsToMove. none decommissioningMaxPercentOfMaxSegmentsToMove The maximum number of segments that may be moved away from 'decommissioning' servers to non-decommissioning (that is, active) servers during one Coordinator run. This value is relative to the total maximum segment movements allowed during one run which is determined by maxSegmentsToMove. If decommissioningMaxPercentOfMaxSegmentsToMove is 0, segments will neither be moved from or to 'decommissioning' servers, effectively putting them in a sort of \"maintenance\" mode that will not participate in balancing or assignment by load rules. Decommissioning can also become stalled if there are no available active servers to place the segments. By leveraging the maximum percent of decommissioning segment movements, an operator can prevent active servers from overload by prioritizing balancing, or decrease decommissioning time instead. The value should be between 0 and 100. 70 pauseCoordination Boolean flag for whether or not the coordinator should execute its various duties of coordinating the cluster. Setting this to true essentially pauses all coordination work while allowing the API to remain up. Duties that are paused include all classes that implement the CoordinatorDuty Interface. Such duties include: Segment balancing, Segment compaction, Emission of metrics controlled by the dynamic coordinator config emitBalancingStats, Submitting kill tasks for unused segments (if enabled), Logging of used segments in the cluster, Marking of newly unused or overshadowed segments, Matching and execution of load/drop rules for used segments, Unloading segments that are no longer marked as used from Historical servers. An example of when an admin may want to pause coordination would be if they are doing deep storage maintenance on HDFS Name Nodes with downtime and don't want the coordinator to be directing Historical Nodes to hit the Name Node with API requests until maintenance is done and the deep store is declared healthy for use again. false To view the audit history of Coordinator dynamic config issue a GET request to the URL - http://:/druid/coordinator/v1/config/history?interval= default value of interval can be specified by setting druid.audit.manager.auditHistoryMillis (1 week if not configured) in Coordinator runtime.properties To view last entries of the audit history of Coordinator dynamic config issue a GET request to the URL - http://:/druid/coordinator/v1/config/history?count= Lookups Dynamic Configuration These configuration options control the behavior of the Lookup dynamic configuration described in the lookups page Property Description Default druid.manager.lookups.hostDeleteTimeout How long to wait for a DELETE request to a particular process before considering the DELETE a failure PT1S druid.manager.lookups.hostUpdateTimeout How long to wait for a POST request to a particular process before considering the POST a failure PT10S druid.manager.lookups.deleteAllTimeout How long to wait for all DELETE requests to finish before considering the delete attempt a failure PT10S druid.manager.lookups.updateAllTimeout How long to wait for all POST requests to finish before considering the attempt a failure PT60S druid.manager.lookups.threadPoolSize How many processes can be managed concurrently (concurrent POST and DELETE requests). Requests this limit will wait in a queue until a slot becomes available. 10 druid.manager.lookups.period How many milliseconds between checks for configuration changes 30_000 Compaction Dynamic Configuration Compaction configurations can also be set or updated dynamically using Coordinator's API without restarting Coordinators. For details about segment compaction, please check Segment Size Optimization. A description of the compaction config is: Property Description Required dataSource dataSource name to be compacted. yes taskPriority Priority of compaction task. no (default = 25) inputSegmentSizeBytes Maximum number of total segment bytes processed per compaction task. Since a time chunk must be processed in its entirety, if the segments for a particular time chunk have a total size in bytes greater than this parameter, compaction will not run for that time chunk. Because each compaction task runs with a single thread, setting this value too far above 1–2GB will result in compaction tasks taking an excessive amount of time. no (default = 419430400) maxRowsPerSegment Max number of rows per segment after compaction. no skipOffsetFromLatest The offset for searching segments to be compacted. Strongly recommended to set for realtime dataSources. no (default = \"P1D\") tuningConfig Tuning config for compaction tasks. See below Compaction Task TuningConfig. no taskContext Task context for compaction tasks. no An example of compaction config is: { \"dataSource\": \"wikiticker\" } Note that compaction tasks can fail if their locks are revoked by other tasks of higher priorities. Since realtime tasks have a higher priority than compaction task by default, it can be problematic if there are frequent conflicts between compaction tasks and realtime tasks. If this is the case, the coordinator's automatic compaction might get stuck because of frequent compaction task failures. This kind of problem may happen especially in Kafka/Kinesis indexing systems which allow late data arrival. If you see this problem, it's recommended to set skipOffsetFromLatest to some large enough value to avoid such conflicts between compaction tasks and realtime tasks. Compaction TuningConfig Auto compaction supports a subset of the tuningConfig for Parallel task. The below is a list of the supported configurations for auto compaction. Property Description Required type The task type, this should always be index_parallel. yes maxRowsInMemory Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set. no (default = 1000000) maxBytesInMemory Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists) no (default = 1/6 of max JVM memory) splitHintSpec Used to give a hint to control the amount of data that each first phase task reads. This hint could be ignored depending on the implementation of the input source. See Split hint spec for more details. no (default = size-based split hint spec) partitionsSpec Defines how to partition data in each time chunk, see PartitionsSpec no (default = dynamic) indexSpec Defines segment storage format options to be used at indexing time, see IndexSpec no indexSpecForIntermediatePersists Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. this can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. however, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see IndexSpec for possible values. no maxPendingPersists Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists). no (default = 0, meaning one persist can be running concurrently with ingestion, and none can be queued up) pushTimeout Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever. no (default = 0) segmentWriteOutMediumFactory Segment write-out medium to use when creating segments. See SegmentWriteOutMediumFactory. no (default is the value from druid.peon.defaultSegmentWriteOutMediumFactory.type is used) maxNumConcurrentSubTasks Maximum number of worker tasks which can be run in parallel at the same time. The supervisor task would spawn worker tasks up to maxNumConcurrentSubTasks regardless of the current available task slots. If this value is set to 1, the supervisor task processes data ingestion on its own instead of spawning worker tasks. If this value is set to too large, too many worker tasks can be created which might block other ingestion. Check Capacity Planning for more details. no (default = 1) maxRetry Maximum number of retries on task failures. no (default = 3) maxNumSegmentsToMerge Max limit for the number of segments that a single task can merge at the same time in the second phase. Used only with hashed or single_dim partitionsSpec. no (default = 100) totalNumMergeTasks Total number of tasks to merge segments in the merge phase when partitionsSpec is set to hashed or single_dim. no (default = 10) taskStatusCheckPeriodMs Polling period in milliseconds to check running task statuses. no (default = 1000) chatHandlerTimeout Timeout for reporting the pushed segments in worker tasks. no (default = PT10S) chatHandlerNumRetries Retries for reporting the pushed segments in worker tasks. no (default = 5) Overlord For general Overlord Process information, see here. Overlord Static Configuration These Overlord static configurations can be defined in the overlord/runtime.properties file. Overlord Process Configs Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8090 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8290 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/overlord Overlord Operations Property Description Default druid.indexer.runner.type Choices \"local\" or \"remote\". Indicates whether tasks should be run locally or in a distributed environment. Experimental task runner \"httpRemote\" is also available which is same as \"remote\" but uses HTTP to interact with Middle Managers instead of Zookeeper. local druid.indexer.storage.type Choices are \"local\" or \"metadata\". Indicates whether incoming tasks should be stored locally (in heap) or in metadata storage. Storing incoming tasks in metadata storage allows for tasks to be resumed if the Overlord should fail. local druid.indexer.storage.recentlyFinishedThreshold A duration of time to store task results. PT24H druid.indexer.tasklock.forceTimeChunkLock Setting this to false is still experimental If set, all tasks are enforced to use time chunk lock. If not set, each task automatically chooses a lock type to use. This configuration can be overwritten by setting forceTimeChunkLock in the task context. See Task Locking & Priority for more details about locking in tasks. true druid.indexer.queue.maxSize Maximum number of active tasks at one time. Integer.MAX_VALUE druid.indexer.queue.startDelay Sleep this long before starting Overlord queue management. This can be useful to give a cluster time to re-orient itself after e.g. a widespread network issue. PT1M druid.indexer.queue.restartDelay Sleep this long when Overlord queue management throws an exception before trying again. PT30S druid.indexer.queue.storageSyncRate Sync Overlord state this often with an underlying task persistence mechanism. PT1M The following configs only apply if the Overlord is running in remote mode. For a description of local vs. remote mode, see Overlord Process. Property Description Default druid.indexer.runner.taskAssignmentTimeout How long to wait after a task as been assigned to a MiddleManager before throwing an error. PT5M druid.indexer.runner.minWorkerVersion The minimum MiddleManager version to send tasks to. \"0\" druid.indexer.runner.compressZnodes Indicates whether or not the Overlord should expect MiddleManagers to compress Znodes. true druid.indexer.runner.maxZnodeBytes The maximum size Znode in bytes that can be created in Zookeeper, should be in the range of [10KiB, 2GiB). Human-readable format is supported. 512 KiB druid.indexer.runner.taskCleanupTimeout How long to wait before failing a task after a MiddleManager is disconnected from Zookeeper. PT15M druid.indexer.runner.taskShutdownLinkTimeout How long to wait on a shutdown request to a MiddleManager before timing out PT1M druid.indexer.runner.pendingTasksRunnerNumThreads Number of threads to allocate pending-tasks to workers, must be at least 1. 1 druid.indexer.runner.maxRetriesBeforeBlacklist Number of consecutive times the MiddleManager can fail tasks, before the worker is blacklisted, must be at least 1 5 druid.indexer.runner.workerBlackListBackoffTime How long to wait before a task is whitelisted again. This value should be greater that the value set for taskBlackListCleanupPeriod. PT15M druid.indexer.runner.workerBlackListCleanupPeriod A duration after which the cleanup thread will startup to clean blacklisted workers. PT5M druid.indexer.runner.maxPercentageBlacklistWorkers The maximum percentage of workers to blacklist, this must be between 0 and 100. 20 There are additional configs for autoscaling (if it is enabled): Property Description Default druid.indexer.autoscale.strategy Choices are \"noop\", \"ec2\" or \"gce\". Sets the strategy to run when autoscaling is required. noop druid.indexer.autoscale.doAutoscale If set to \"true\" autoscaling will be enabled. false druid.indexer.autoscale.provisionPeriod How often to check whether or not new MiddleManagers should be added. PT1M druid.indexer.autoscale.terminatePeriod How often to check when MiddleManagers should be removed. PT5M druid.indexer.autoscale.originTime The starting reference timestamp that the terminate period increments upon. 2012-01-01T00:55:00.000Z druid.indexer.autoscale.workerIdleTimeout How long can a worker be idle (not a run task) before it can be considered for termination. PT90M druid.indexer.autoscale.maxScalingDuration How long the Overlord will wait around for a MiddleManager to show up before giving up. PT15M druid.indexer.autoscale.numEventsToTrack The number of autoscaling related events (node creation and termination) to track. 10 druid.indexer.autoscale.pendingTaskTimeout How long a task can be in \"pending\" state before the Overlord tries to scale up. PT30S druid.indexer.autoscale.workerVersion If set, will only create nodes of set version during autoscaling. Overrides dynamic configuration. null druid.indexer.autoscale.workerPort The port that MiddleManagers will run on. 8080 Supervisors Property Description Default druid.supervisor.healthinessThreshold The number of successful runs before an unhealthy supervisor is again considered healthy. 3 druid.supervisor.unhealthinessThreshold The number of failed runs before the supervisor is considered unhealthy. 3 druid.supervisor.taskHealthinessThreshold The number of consecutive task successes before an unhealthy supervisor is again considered healthy. 3 druid.supervisor.taskUnhealthinessThreshold The number of consecutive task failures before the supervisor is considered unhealthy. 3 druid.supervisor.storeStackTrace Whether full stack traces of supervisor exceptions should be stored and returned by the supervisor /status endpoint. false druid.supervisor.maxStoredExceptionEvents The maximum number of exception events that can be returned through the supervisor /status endpoint. max(healthinessThreshold, unhealthinessThreshold) Overlord Dynamic Configuration The Overlord can dynamically change worker behavior. The JSON object can be submitted to the Overlord via a POST request at: http://:/druid/indexer/v1/worker Optional Header Parameters for auditing the config change can also be specified. Header Param Name Description Default X-Druid-Author author making the config change \"\" X-Druid-Comment comment describing the change being done \"\" A sample worker config spec is shown below: { \"selectStrategy\": { \"type\": \"fillCapacity\", \"affinityConfig\": { \"affinity\": { \"datasource1\": [\"host1:port\", \"host2:port\"], \"datasource2\": [\"host3:port\"] } } }, \"autoScaler\": { \"type\": \"ec2\", \"minNumWorkers\": 2, \"maxNumWorkers\": 12, \"envConfig\": { \"availabilityZone\": \"us-east-1a\", \"nodeData\": { \"amiId\": \"${AMI}\", \"instanceType\": \"c3.8xlarge\", \"minInstances\": 1, \"maxInstances\": 1, \"securityGroupIds\": [\"${IDs}\"], \"keyName\": \"${KEY_NAME}\" }, \"userData\": { \"impl\": \"string\", \"data\": \"${SCRIPT_COMMAND}\", \"versionReplacementString\": \":VERSION:\", \"version\": null } } } } Issuing a GET request at the same URL will return the current worker config spec that is currently in place. The worker config spec list above is just a sample for EC2 and it is possible to extend the code base for other deployment environments. A description of the worker config spec is shown below. Property Description Default selectStrategy How to assign tasks to MiddleManagers. Choices are fillCapacity, equalDistribution, and javascript. equalDistribution autoScaler Only used if autoscaling is enabled. See below. null To view the audit history of worker config issue a GET request to the URL - http://:/druid/indexer/v1/worker/history?interval= default value of interval can be specified by setting druid.audit.manager.auditHistoryMillis (1 week if not configured) in Overlord runtime.properties. To view last entries of the audit history of worker config issue a GET request to the URL - http://:/druid/indexer/v1/worker/history?count= Worker Select Strategy Worker select strategies control how Druid assigns tasks to MiddleManagers. Equal Distribution Tasks are assigned to the MiddleManager with the most free slots at the time the task begins running. This is useful if you want work evenly distributed across your MiddleManagers. Property Description Default type equalDistribution. required; must be equalDistribution affinityConfig Affinity config object null (no affinity) Equal Distribution With Category Spec This strategy is a variant of Equal Distribution, which support workerCategorySpec field rather than affinityConfig. By specifying workerCategorySpec, you can assign tasks to run on different categories of MiddleManagers based on the tasks' taskType and dataSource name. This strategy can't work with AutoScaler since the behavior is undefined. Property Description Default type equalDistributionWithCategorySpec. required; must be equalDistributionWithCategorySpec workerCategorySpec Worker Category Spec object null (no worker category spec) Example: specify tasks default to run on c1 whose task type is \"index_kafka\", while dataSource \"ds1\" run on c2. { \"selectStrategy\": { \"type\": \"equalDistributionWithCategorySpec\", \"workerCategorySpec\": { \"strong\": false, \"categoryMap\": { \"index_kafka\": { \"defaultCategory\": \"c1\", \"categoryAffinity\": { \"ds1\": \"c2\" } } } } } } Fill Capacity Tasks are assigned to the worker with the most currently-running tasks at the time the task begins running. This is useful in situations where you are elastically auto-scaling MiddleManagers, since it will tend to pack some full and leave others empty. The empty ones can be safely terminated. Note that if druid.indexer.runner.pendingTasksRunnerNumThreads is set to N > 1, then this strategy will fill N MiddleManagers up to capacity simultaneously, rather than a single MiddleManager. Property Description Default type fillCapacity. required; must be fillCapacity affinityConfig Affinity config object null (no affinity) Fill Capacity With Category Spec This strategy is a variant of Fill Capacity, which support workerCategorySpec field rather than affinityConfig. The usage is the same with equalDistributionWithCategorySpec strategy. This strategy can't work with AutoScaler since the behavior is undefined. Property Description Default type fillCapacityWithCategorySpec. required; must be fillCapacityWithCategorySpec workerCategorySpec Worker Category Spec object null (no worker category spec) Before using the equalDistributionWithCategorySpec and fillCapacityWithCategorySpec strategies, you must upgrade overlord and all MiddleManagers to the version that support this feature. JavaScript Allows defining arbitrary logic for selecting workers to run task using a JavaScript function. The function is passed remoteTaskRunnerConfig, map of workerId to available workers and task to be executed and returns the workerId on which the task should be run or null if the task cannot be run. It can be used for rapid development of missing features where the worker selection logic is to be changed or tuned often. If the selection logic is quite complex and cannot be easily tested in JavaScript environment, its better to write a druid extension module with extending current worker selection strategies written in java. Property Description Default type javascript. required; must be javascript function String representing JavaScript function Example: a function that sends batch_index_task to workers 10.0.0.1 and 10.0.0.2 and all other tasks to other available workers. { \"type\":\"javascript\", \"function\":\"function (config, zkWorkers, task) {\\nvar batch_workers = new java.util.ArrayList();\\nbatch_workers.add(\\\"middleManager1_hostname:8091\\\");\\nbatch_workers.add(\\\"middleManager2_hostname:8091\\\");\\nworkers = zkWorkers.keySet().toArray();\\nvar sortedWorkers = new Array()\\n;for(var i = 0; i JavaScript-based functionality is disabled by default. Please refer to the Druid JavaScript programming guide for guidelines about using Druid's JavaScript functionality, including instructions on how to enable it. Affinity Affinity configs can be provided to the equalDistribution and fillCapacity strategies using the \"affinityConfig\" field. If not provided, the default is to not use affinity at all. Property Description Default affinity JSON object mapping a datasource String name to a list of indexing service MiddleManager host:port String values. Druid doesn't perform DNS resolution, so the 'host' value must match what is configured on the MiddleManager and what the MiddleManager announces itself as (examine the Overlord logs to see what your MiddleManager announces itself as). {} strong With weak affinity (the default), tasks for a dataSource may be assigned to other MiddleManagers if their affinity-mapped MiddleManagers are not able to run all pending tasks in the queue for that dataSource. With strong affinity, tasks for a dataSource will only ever be assigned to their affinity-mapped MiddleManagers, and will wait in the pending queue if necessary. false WorkerCategorySpec WorkerCategorySpec can be provided to the equalDistributionWithCategorySpec and fillCapacityWithCategorySpec strategies using the \"workerCategorySpec\" field. If not provided, the default is to not use it at all. Property Description Default categoryMap A JSON map object mapping a task type String name to a CategoryConfig object, by which you can specify category config for different task type. {} strong With weak workerCategorySpec (the default), tasks for a dataSource may be assigned to other MiddleManagers if the MiddleManagers specified in categoryMap are not able to run all pending tasks in the queue for that dataSource. With strong workerCategorySpec, tasks for a dataSource will only ever be assigned to their specified MiddleManagers, and will wait in the pending queue if necessary. false CategoryConfig Property Description Default defaultCategory Specify default category for a task type. null categoryAffinity A JSON map object mapping a datasource String name to a category String name of the MiddleManager. If category isn't specified for a datasource, then using the defaultCategory. If no specified category and the defaultCategory is also null, then tasks can run on any available MiddleManagers. null Autoscaler Amazon's EC2 together with Google's GCE are currently the only supported autoscalers. EC2's autoscaler properties are: Property Description Default minNumWorkers The minimum number of workers that can be in the cluster at any given time. 0 maxNumWorkers The maximum number of workers that can be in the cluster at any given time. 0 availabilityZone What availability zone to run in. none nodeData A JSON object that describes how to launch new nodes. none; required userData A JSON object that describes how to configure new nodes. If you have set druid.indexer.autoscale.workerVersion, this must have a versionReplacementString. Otherwise, a versionReplacementString is not necessary. none; optional For GCE's properties, please refer to the gce-extensions. Data Server This section contains the configuration options for the processes that reside on Data servers (MiddleManagers/Peons and Historicals) in the suggested three-server configuration. Configuration options for the experimental Indexer process are also provided here. MiddleManager and Peons These MiddleManager and Peon configurations can be defined in the middleManager/runtime.properties file. MiddleManager Process Config Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8091 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8291 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/middlemanager MiddleManager Configuration Middle managers pass their configurations down to their child peons. The MiddleManager requires the following configs: Property Description Default druid.indexer.runner.allowedPrefixes Whitelist of prefixes for configs that can be passed down to child peons. \"com.metamx\", \"druid\", \"org.apache.druid\", \"user.timezone\", \"file.encoding\", \"java.io.tmpdir\", \"hadoop\" druid.indexer.runner.compressZnodes Indicates whether or not the MiddleManagers should compress Znodes. true druid.indexer.runner.classpath Java classpath for the peon. System.getProperty(\"java.class.path\") druid.indexer.runner.javaCommand Command required to execute java. java druid.indexer.runner.javaOpts DEPRECATED A string of -X Java options to pass to the peon's JVM. Quotable parameters or parameters with spaces are encouraged to use javaOptsArray \"\" druid.indexer.runner.javaOptsArray A JSON array of strings to be passed in as options to the peon's JVM. This is additive to javaOpts and is recommended for properly handling arguments which contain quotes or spaces like [\"-XX:OnOutOfMemoryError=kill -9 %p\"] [] druid.indexer.runner.maxZnodeBytes The maximum size Znode in bytes that can be created in Zookeeper, should be in the range of [10KiB, 2GiB). Human-readable format is supported. 512KiB druid.indexer.runner.startPort Starting port used for peon processes, should be greater than 1023 and less than 65536. 8100 druid.indexer.runner.endPort Ending port used for peon processes, should be greater than or equal to druid.indexer.runner.startPort and less than 65536. 65535 druid.indexer.runner.ports A JSON array of integers to specify ports that used for peon processes. If provided and non-empty, ports for peon processes will be chosen from these ports. And druid.indexer.runner.startPort/druid.indexer.runner.endPort will be completely ignored. [] druid.worker.ip The IP of the worker. localhost druid.worker.version Version identifier for the MiddleManager. 0 druid.worker.capacity Maximum number of tasks the MiddleManager can accept. Number of CPUs on the machine - 1 druid.worker.category A string to name the category that the MiddleManager node belongs to. _default_worker_category Peon Processing Processing properties set on the Middlemanager will be passed through to Peons. Property Description Default druid.processing.buffer.sizeBytes This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. Human-readable format is supported. auto (max 1 GiB) druid.processing.buffer.poolCacheMaxCount processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary. Integer.MAX_VALUE druid.processing.formatString Realtime and Historical processes use this format string to name their processing threads. processing-%s druid.processing.numMergeBuffers The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these. max(2, druid.processing.numThreads / 4) druid.processing.numThreads The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1. Number of cores - 1 (or 1) druid.processing.columnCache.sizeBytes Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses. 0 (disabled) druid.processing.fifo If the processing queue should treat tasks of equal priority in a FIFO manner false druid.processing.tmpDir Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path. path represented by java.io.tmpdir The amount of direct memory needed by Druid is at least druid.processing.buffer.sizeBytes * (druid.processing.numMergeBuffers + druid.processing.numThreads + 1). You can ensure at least this amount of direct memory is available by providing -XX:MaxDirectMemorySize= in druid.indexer.runner.javaOptsArray as documented above. Peon query configuration See general query configuration. Peon Caching You can optionally configure caching to be enabled on the peons by setting caching configs here. Property Possible Values Description Default druid.realtime.cache.useCache true, false Enable the cache on the realtime. false druid.realtime.cache.populateCache true, false Populate the cache on the realtime. false druid.realtime.cache.unCacheable All druid query types All query types to not cache. [] druid.realtime.cache.maxEntrySize positive integer Maximum cache entry size in bytes. 1_000_000 See cache configuration for how to configure cache settings. Additional Peon Configuration Although peons inherit the configurations of their parent MiddleManagers, explicit child peon configs in MiddleManager can be set by prefixing them with: druid.indexer.fork.property Additional peon configs include: Property Description Default druid.peon.mode Choices are \"local\" and \"remote\". Setting this to local means you intend to run the peon as a standalone process (Not recommended). remote druid.indexer.task.baseDir Base temporary working directory. System.getProperty(\"java.io.tmpdir\") druid.indexer.task.baseTaskDir Base temporary working directory for tasks. ${druid.indexer.task.baseDir}/persistent/task druid.indexer.task.defaultHadoopCoordinates Hadoop version to use with HadoopIndexTasks that do not request a particular version. org.apache.hadoop:hadoop-client:2.8.5 druid.indexer.task.defaultRowFlushBoundary Highest row count before persisting to disk. Used for indexing generating tasks. 75000 druid.indexer.task.directoryLockTimeout Wait this long for zombie peons to exit before giving up on their replacements. PT10M druid.indexer.task.gracefulShutdownTimeout Wait this long on middleManager restart for restorable tasks to gracefully exit. PT5M druid.indexer.task.hadoopWorkingPath Temporary working directory for Hadoop tasks. /tmp/druid-indexing druid.indexer.task.restoreTasksOnRestart If true, MiddleManagers will attempt to stop tasks gracefully on shutdown and restore them on restart. false druid.indexer.server.maxChatRequests Maximum number of concurrent requests served by a task's chat handler. Set to 0 to disable limiting. 0 If the peon is running in remote mode, there must be an Overlord up and running. Peons in remote mode can set the following configurations: Property Description Default druid.peon.taskActionClient.retry.minWait The minimum retry time to communicate with Overlord. PT5S druid.peon.taskActionClient.retry.maxWait The maximum retry time to communicate with Overlord. PT1M druid.peon.taskActionClient.retry.maxRetryCount The maximum number of retries to communicate with Overlord. 60 SegmentWriteOutMediumFactory When new segments are created, Druid temporarily stores some preprocessed data in some buffers. Currently three types of medium exist for those buffers: temporary files, off-heap memory, and on-heap memory. Temporary files (tmpFile) are stored under the task working directory (see druid.indexer.task.baseTaskDir configuration above) and thus share it's mounting properties, e. g. they could be backed by HDD, SSD or memory (tmpfs). This type of medium may do unnecessary disk I/O and requires some disk space to be available. Off-heap memory medium (offHeapMemory) creates buffers in off-heap memory of a JVM process that is running a task. This type of medium is preferred, but it may require to allow the JVM to have more off-heap memory, by changing -XX:MaxDirectMemorySize configuration. It is not yet understood how does the required off-heap memory size relates to the size of the segments being created. But definitely it doesn't make sense to add more extra off-heap memory, than the configured maximum heap size (-Xmx) for the same JVM. On-heap memory medium (onHeapMemory) creates buffers using the allocated heap memory of the JVM process running a task. Using on-heap memory introduces garbage collection overhead and so is not recommended in most cases. This type of medium is most helpful for tasks run on external clusters where it may be difficult to allocate and work with direct memory effectively. For most types of tasks SegmentWriteOutMediumFactory could be configured per-task (see Tasks page, \"TuningConfig\" section), but if it's not specified for a task, or it's not supported for a particular task type, then the value from the configuration below is used: Property Description Default druid.peon.defaultSegmentWriteOutMediumFactory.type tmpFile, offHeapMemory, or onHeapMemory, see explanation above tmpFile Indexer Indexer Process Configuration Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8091 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8283 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/indexer Indexer General Configuration Property Description Default druid.worker.version Version identifier for the Indexer. 0 druid.worker.capacity Maximum number of tasks the Indexer can accept. Number of available processors - 1 druid.worker.globalIngestionHeapLimitBytes Total amount of heap available for ingestion processing. This is applied by automatically setting the maxBytesInMemory property on tasks. 60% of configured JVM heap druid.worker.numConcurrentMerges Maximum number of segment persist or merge operations that can run concurrently across all tasks. druid.worker.capacity / 2, rounded down druid.indexer.task.baseDir Base temporary working directory. System.getProperty(\"java.io.tmpdir\") druid.indexer.task.baseTaskDir Base temporary working directory for tasks. ${druid.indexer.task.baseDir}/persistent/tasks druid.indexer.task.defaultHadoopCoordinates Hadoop version to use with HadoopIndexTasks that do not request a particular version. org.apache.hadoop:hadoop-client:2.8.5 druid.indexer.task.gracefulShutdownTimeout Wait this long on Indexer restart for restorable tasks to gracefully exit. PT5M druid.indexer.task.hadoopWorkingPath Temporary working directory for Hadoop tasks. /tmp/druid-indexing druid.indexer.task.restoreTasksOnRestart If true, the Indexer will attempt to stop tasks gracefully on shutdown and restore them on restart. false druid.peon.taskActionClient.retry.minWait The minimum retry time to communicate with Overlord. PT5S druid.peon.taskActionClient.retry.maxWait The maximum retry time to communicate with Overlord. PT1M druid.peon.taskActionClient.retry.maxRetryCount The maximum number of retries to communicate with Overlord. 60 Indexer Concurrent Requests Druid uses Jetty to serve HTTP requests. Property Description Default druid.server.http.numThreads Number of threads for HTTP requests. Please see the Indexer Server HTTP threads documentation for more details on how the Indexer uses this configuration. max(10, (Number of cores * 17) / 16 + 2) + 30 druid.server.http.queueSize Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server. Unbounded druid.server.http.maxIdleTime The Jetty max idle time for a connection. PT5M druid.server.http.enableRequestLimit If enabled, no requests would be queued in jetty queue and \"HTTP 429 Too Many Requests\" error response would be sent. false druid.server.http.defaultQueryTimeout Query timeout in millis, beyond which unfinished queries will be cancelled 300000 druid.server.http.gracefulShutdownTimeout The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid). PT30S druid.server.http.unannouncePropagationDelay How long to wait for zookeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses. PT0S (do not wait) druid.server.http.maxQueryTimeout Maximum allowed value (in milliseconds) for timeout parameter. See query-context to know more about timeout. Query is rejected if the query context timeout is greater than this value. Long.MAX_VALUE druid.server.http.maxRequestHeaderSize Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks. 8 * 1024 druid.server.http.enableForwardedRequestCustomizer If enabled, adds Jetty ForwardedRequestCustomizer which reads X-Forwarded-* request headers to manipulate servlet request object when Druid is used behind a proxy. false druid.server.http.allowedHttpMethods List of HTTP methods that should be allowed in addition to the ones required by Druid APIs. Druid APIs require GET, PUT, POST, and DELETE, which are always allowed. This option is not useful unless you have installed an extension that needs these additional HTTP methods or that adds functionality related to CORS. None of Druid's bundled extensions require these methods. [] Indexer Processing Resources Property Description Default druid.processing.buffer.sizeBytes This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in the Indexer processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. Human-readable format is supported. auto (max 1GB) druid.processing.buffer.poolCacheMaxCount processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary. Integer.MAX_VALUE druid.processing.formatString Indexer processes use this format string to name their processing threads. processing-%s druid.processing.numMergeBuffers The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these. max(2, druid.processing.numThreads / 4) druid.processing.numThreads The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1. Number of cores - 1 (or 1) druid.processing.columnCache.sizeBytes Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses. 0 (disabled) druid.processing.fifo If the processing queue should treat tasks of equal priority in a FIFO manner false druid.processing.tmpDir Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path. path represented by java.io.tmpdir The amount of direct memory needed by Druid is at least druid.processing.buffer.sizeBytes * (druid.processing.numMergeBuffers + druid.processing.numThreads + 1). You can ensure at least this amount of direct memory is available by providing -XX:MaxDirectMemorySize= at the command line. Query Configurations See general query configuration. Indexer Caching You can optionally configure caching to be enabled on the Indexer by setting caching configs here. Property Possible Values Description Default druid.realtime.cache.useCache true, false Enable the cache on the realtime. false druid.realtime.cache.populateCache true, false Populate the cache on the realtime. false druid.realtime.cache.unCacheable All druid query types All query types to not cache. [] druid.realtime.cache.maxEntrySize positive integer Maximum cache entry size in bytes. 1_000_000 See cache configuration for how to configure cache settings. Note that only local caches such as the local-type cache and caffeine cache are supported. If a remote cache such as memcached is used, it will be ignored. Historical For general Historical Process information, see here. These Historical configurations can be defined in the historical/runtime.properties file. Historical Process Configuration Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8083 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8283 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/historical Historical General Configuration Property Description Default druid.server.maxSize The maximum number of bytes-worth of segments that the process wants assigned to it. The Coordinator process will attempt to assign segments to a Historical process only if this property is greater than the total size of segments served by it. Since this property defines the upper limit on the total segment size that can be assigned to a Historical, it is defaulted to the sum of all maxSize values specified within druid.segmentCache.locations property. Human-readable format is supported, see here. Sum of maxSize values defined within druid.segmentCache.locations druid.server.tier A string to name the distribution tier that the storage process belongs to. Many of the rules Coordinator processes use to manage segments can be keyed on tiers. _default_tier druid.server.priority In a tiered architecture, the priority of the tier, thus allowing control over which processes are queried. Higher numbers mean higher priority. The default (no priority) works for architecture with no cross replication (tiers that have no data-storage overlap). Data centers typically have equal priority. 0 Storing Segments Property Description Default druid.segmentCache.locations Segments assigned to a Historical process are first stored on the local file system (in a disk cache) and then served by the Historical process. These locations define where that local cache resides. This value cannot be NULL or EMPTY. Here is an example druid.segmentCache.locations=[{\"path\": \"/mnt/druidSegments\", \"maxSize\": \"10k\", \"freeSpacePercent\": 1.0}]. \"freeSpacePercent\" is optional, if provided then enforces that much of free disk partition space while storing segments. But, it depends on File.getTotalSpace() and File.getFreeSpace() methods, so enable if only if they work for your File System. none druid.segmentCache.locationSelector.strategy The strategy used to select a location from the configured druid.segmentCache.locations for segment distribution. Possible values are leastBytesUsed, roundRobin, random, or mostAvailableSize. leastBytesUsed druid.segmentCache.deleteOnRemove Delete segment files from cache once a process is no longer serving a segment. true druid.segmentCache.dropSegmentDelayMillis How long a process delays before completely dropping segment. 30000 (30 seconds) druid.segmentCache.infoDir Historical processes keep track of the segments they are serving so that when the process is restarted they can reload the same segments without waiting for the Coordinator to reassign. This path defines where this metadata is kept. Directory will be created if needed. ${first_location}/info_dir druid.segmentCache.announceIntervalMillis How frequently to announce segments while segments are loading from cache. Set this value to zero to wait for all segments to be loaded before announcing. 5000 (5 seconds) druid.segmentCache.numLoadingThreads How many segments to drop or load concurrently from deep storage. Note that the work of loading segments involves downloading segments from deep storage, decompressing them and loading them to a memory mapped location. So the work is not all I/O Bound. Depending on CPU and network load, one could possibly increase this config to a higher value. max(1,Number of cores / 6) druid.segmentCache.numBootstrapThreads How many segments to load concurrently during historical startup. druid.segmentCache.numLoadingThreads druid.segmentCache.lazyLoadOnStart Whether or not to load segment columns metadata lazily during historical startup. When set to true, Historical startup time will be dramatically improved by deferring segment loading until the first time that segment takes part in a query, which will incur this cost instead. One catch is that if there were corrupted segments which historical served before on historical disk, this requires manual intervention to delete corrupted files. When the flag is set true, historical startup would complete successfully and queries using this segment would fail at runtime. false druid.coordinator.loadqueuepeon.curator.numCallbackThreads Number of threads for executing callback actions associated with loading or dropping of segments. One might want to increase this number when noticing clusters are lagging behind w.r.t. balancing segments across historical nodes. 2 In druid.segmentCache.locations, freeSpacePercent was added because maxSize setting is only a theoretical limit and assumes that much space will always be available for storing segments. In case of any druid bug leading to unaccounted segment files left alone on disk or some other process writing stuff to disk, This check can start failing segment loading early before filling up the disk completely and leaving the host usable otherwise. In druid.segmentCache.locationSelector.strategy, one of leastBytesUsed, roundRobin, random, or mostAvailableSize could be specified to represent the strategy to distribute segments across multiple segment cache locations. Strategy Description leastBytesUsed selects a location which has least bytes used in absolute terms. roundRobin selects a location in a round robin fashion oblivious to the bytes used or the capacity. random selects a segment cache location randomly each time among the available storage locations. mostAvailableSize selects a segment cache location that has most free space among the available storage locations. Note that if druid.segmentCache.numLoadingThreads > 1, multiple threads can download different segments at the same time. In this case, with the leastBytesUsed strategy or mostAvailableSize strategy, historicals may select a sub-optimal storage location because each decision is based on a snapshot of the storage location status of when a segment is requested to download. Historical query configs Concurrent Requests Druid uses Jetty to serve HTTP requests. Property Description Default druid.server.http.numThreads Number of threads for HTTP requests. max(10, (Number of cores * 17) / 16 + 2) + 30 druid.server.http.queueSize Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server. Unbounded druid.server.http.maxIdleTime The Jetty max idle time for a connection. PT5M druid.server.http.enableRequestLimit If enabled, no requests would be queued in jetty queue and \"HTTP 429 Too Many Requests\" error response would be sent. false druid.server.http.defaultQueryTimeout Query timeout in millis, beyond which unfinished queries will be cancelled 300000 druid.server.http.gracefulShutdownTimeout The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid). PT30S druid.server.http.unannouncePropagationDelay How long to wait for zookeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses. PT0S (do not wait) druid.server.http.maxQueryTimeout Maximum allowed value (in milliseconds) for timeout parameter. See query-context to know more about timeout. Query is rejected if the query context timeout is greater than this value. Long.MAX_VALUE druid.server.http.maxRequestHeaderSize Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks. 8 * 1024 Processing Property Description Default druid.processing.buffer.sizeBytes This specifies a buffer size (less than 2GiB), for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. Human-readable format is supported. auto (max 1GB) druid.processing.buffer.poolCacheMaxCount processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary. Integer.MAX_VALUE druid.processing.formatString Realtime and Historical processes use this format string to name their processing threads. processing-%s druid.processing.numMergeBuffers The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these. max(2, druid.processing.numThreads / 4) druid.processing.numThreads The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1. Number of cores - 1 (or 1) druid.processing.columnCache.sizeBytes Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses. 0 (disabled) druid.processing.fifo If the processing queue should treat tasks of equal priority in a FIFO manner false druid.processing.tmpDir Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path. path represented by java.io.tmpdir The amount of direct memory needed by Druid is at least druid.processing.buffer.sizeBytes * (druid.processing.numMergeBuffers + druid.processing.numThreads + 1). You can ensure at least this amount of direct memory is available by providing -XX:MaxDirectMemorySize= at the command line. Historical query configuration See general query configuration. Historical Caching You can optionally only configure caching to be enabled on the Historical by setting caching configs here. Property Possible Values Description Default druid.historical.cache.useCache true, false Enable the cache on the Historical. false druid.historical.cache.populateCache true, false Populate the cache on the Historical. false druid.historical.cache.unCacheable All druid query types All query types to not cache. [] druid.historical.cache.maxEntrySize positive integer Maximum cache entry size in bytes. 1_000_000 See cache configuration for how to configure cache settings. Query Server This section contains the configuration options for the processes that reside on Query servers (Brokers) in the suggested three-server configuration. Configuration options for the experimental Router process are also provided here. Broker For general Broker process information, see here. These Broker configurations can be defined in the broker/runtime.properties file. Broker Process Configs Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8082 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 8282 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/broker Query configuration Query routing Property Possible Values Description Default druid.broker.balancer.type random, connectionCount Determines how the broker balances connections to Historical processes. random choose randomly, connectionCount picks the process with the fewest number of active connections to random druid.broker.select.tier highestPriority, lowestPriority, custom If segments are cross-replicated across tiers in a cluster, you can tell the broker to prefer to select segments in a tier with a certain priority. highestPriority druid.broker.select.tier.custom.priorities An array of integer priorities. E.g., [-1, 0, 1, 2] Select servers in tiers with a custom priority list. The config only has effect if druid.broker.select.tier is set to custom. If druid.broker.select.tier is set to custom but this config is not specified, the effect is the same as druid.broker.select.tier set to highestPriority. Any of the integers in this config can be ignored if there's no corresponding tiers with such priorities. Tiers with priorities explicitly specified in this config always have higher priority than those not and those not specified fall back to use highestPriority strategy among themselves. Query prioritization and laning Laning strategies allow you to control capacity utilization for heterogeneous query workloads. With laning, the broker examines and classifies a query for the purpose of assigning it to a 'lane'. Lanes have capacity limits, enforced by the broker, that can be used to ensure sufficient resources are available for other lanes or for interactive queries (with no lane), or to limit overall throughput for queries within the lane. Requests in excess of the capacity are discarded with an HTTP 429 status code. Property Description Default druid.query.scheduler.numThreads Maximum number of HTTP threads to dedicate to query processing. To save HTTP thread capacity, this should be lower than druid.server.http.numThreads, but it is worth noting that like druid.server.http.enableRequestLimit is set that query requests over this limit will be denied instead of waiting in the Jetty HTTP request queue. Unbounded druid.query.scheduler.laning.strategy Query laning strategy to use to assign queries to a lane in order to control capacities for certain classes of queries. none druid.query.scheduler.prioritization.strategy Query prioritization strategy to automatically assign priorities. manual Prioritization strategies Manual prioritization strategy With this configuration, queries are never assigned a priority automatically, but will preserve a priority manually set on the query context with the priority key. This mode can be explicitly set by setting druid.query.scheduler.prioritization.strategy to none. Threshold prioritization strategy This prioritization strategy lowers the priority of queries that cross any of a configurable set of thresholds, such as how far in the past the data is, how large of an interval a query covers, or the number of segments taking part in a query. This strategy can be enabled by setting druid.query.scheduler.prioritization.strategy to threshold. Property Description Default druid.query.scheduler.prioritization.periodThreshold ISO duration threshold for how old data can be queried before automatically adjusting query priority. None druid.query.scheduler.prioritization.durationThreshold ISO duration threshold for maximum duration a queries interval can span before the priority is automatically adjusted. None druid.query.scheduler.prioritization.segmentCountThreshold Number threshold for maximum number of segments that can take part in a query before its priority is automatically adjusted. None druid.query.scheduler.prioritization.adjustment Amount to reduce the priority of queries which cross any threshold. None Laning strategies No laning strategy In this mode, queries are never assigned a lane, and the concurrent query count will only be limited by druid.server.http.numThreads or druid.query.scheduler.numThreads, if set. This is the default Druid query scheduler operating mode. Enable this strategy explicitly by setting druid.query.scheduler.laning.strategy to none. 'High/Low' laning strategy This laning strategy splits queries with a priority below zero into a low query lane, automatically. Queries with priority of zero (the default) or above are considered 'interactive'. The limit on low queries can be set to some desired percentage of the total capacity (or HTTP thread pool size), reserving capacity for interactive queries. Queries in the low lane are not guaranteed their capacity, which may be consumed by interactive queries, but may use up to this limit if total capacity is available. If the low lane is specified in the query context lane parameter, this will override the computed lane. This strategy can be enabled by setting druid.query.scheduler.laning.strategy=hilo. Property Description Default druid.query.scheduler.laning.maxLowPercent Maximum percent of the smaller number of druid.server.http.numThreads or druid.query.scheduler.numThreads, defining the number of HTTP threads that can be used by queries with a priority lower than 0. Value must be an integer in the range 1 to 100, and will be rounded up No default, must be set if using this mode 'Manual' laning strategy This laning strategy is best suited for cases where one or more external applications which query Druid are capable of manually deciding what lane a given query should belong to. Configured with a map of lane names to percent or exact max capacities, queries with a matching lane parameter in the query context will be subjected to those limits. Property Description Default druid.query.scheduler.laning.lanes.{name} Maximum percent or exact limit of queries that can concurrently run in the defined lanes. Any number of lanes may be defined like this. The lane names 'total' and 'default' are reserved for internal use. No default, must define at least one lane with a limit above 0. If druid.query.scheduler.laning.isLimitPercent is set to true, values must be integers in the range of 1 to 100. druid.query.scheduler.laning.isLimitPercent If set to true, the values set for druid.query.scheduler.laning.lanes will be treated as a percent of the smaller number of druid.server.http.numThreads or druid.query.scheduler.numThreads. Note that in this mode, these lane values across lanes are not required to add up to, and can exceed, 100%. false Server Configuration Druid uses Jetty to serve HTTP requests. Each query being processed consumes a single thread from druid.server.http.numThreads, so consider defining druid.query.scheduler.numThreads to a lower value in order to reserve HTTP threads for responding to health checks, lookup loading, and other non-query, and in most cases comparatively very short lived, HTTP requests. Property Description Default druid.server.http.numThreads Number of threads for HTTP requests. max(10, (Number of cores * 17) / 16 + 2) + 30 druid.server.http.queueSize Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server. Unbounded druid.server.http.maxIdleTime The Jetty max idle time for a connection. PT5M druid.server.http.enableRequestLimit If enabled, no requests would be queued in jetty queue and \"HTTP 429 Too Many Requests\" error response would be sent. false druid.server.http.defaultQueryTimeout Query timeout in millis, beyond which unfinished queries will be cancelled 300000 druid.server.http.maxScatterGatherBytes Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. Queries that exceed this limit will fail. This is an advance configuration that allows to protect in case Broker is under heavy load and not utilizing the data gathered in memory fast enough and leading to OOMs. This limit can be further reduced at query time using maxScatterGatherBytes in the context. Note that having large limit is not necessarily bad if broker is never under heavy concurrent load in which case data gathered is processed quickly and freeing up the memory used. Human-readable format is supported, see here. Long.MAX_VALUE druid.server.http.maxSubqueryRows Maximum number of rows from subqueries per query. These rows are stored in memory. 100000 druid.server.http.gracefulShutdownTimeout The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid). PT30S druid.server.http.unannouncePropagationDelay How long to wait for zookeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses. PT0S (do not wait) druid.server.http.maxQueryTimeout Maximum allowed value (in milliseconds) for timeout parameter. See query-context to know more about timeout. Query is rejected if the query context timeout is greater than this value. Long.MAX_VALUE druid.server.http.maxRequestHeaderSize Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks. 8 * 1024 Client Configuration Druid Brokers use an HTTP client to communicate with with data servers (Historical servers and real-time tasks). This client has the following configuration options. Property Description Default druid.broker.http.numConnections Size of connection pool for the Broker to connect to Historical and real-time processes. If there are more queries than this number that all need to speak to the same process, then they will queue up. 20 druid.broker.http.compressionCodec Compression codec the Broker uses to communicate with Historical and real-time processes. May be \"gzip\" or \"identity\". gzip druid.broker.http.readTimeout The timeout for data reads from Historical servers and real-time tasks. PT15M druid.broker.http.unusedConnectionTimeout The timeout for idle connections in connection pool. The connection in the pool will be closed after this timeout and a new one will be established. This timeout should be less than druid.broker.http.readTimeout. Set this timeout = ~90% of druid.broker.http.readTimeout PT4M druid.broker.http.maxQueuedBytes Maximum number of bytes queued per query before exerting backpressure on the channel to the data server. Similar to druid.server.http.maxScatterGatherBytes, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled. Can be overridden by the \"maxQueuedBytes\" query context parameter. Human-readable format is supported, see here. 0 (disabled) druid.broker.http.numMaxThreads `Maximum number of I/O worker threads max(10, ((number of cores * 17) / 16 + 2) + 30)` Retry Policy Druid broker can optionally retry queries internally for transient errors. Property Description Default druid.broker.retryPolicy.numTries Number of tries. 1 Processing The broker uses processing configs for nested groupBy queries. Property Description Default druid.processing.buffer.sizeBytes This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. Human-readable format is supported. auto (max 1GiB) druid.processing.buffer.poolCacheMaxCount processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary. Integer.MAX_VALUE druid.processing.formatString Realtime and Historical processes use this format string to name their processing threads. processing-%s druid.processing.numMergeBuffers The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these. max(2, druid.processing.numThreads / 4) druid.processing.numThreads The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1. Number of cores - 1 (or 1) druid.processing.columnCache.sizeBytes Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses. 0 (disabled) druid.processing.fifo If the processing queue should treat tasks of equal priority in a FIFO manner false druid.processing.tmpDir Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path. path represented by java.io.tmpdir druid.processing.merge.useParallelMergePool Enable automatic parallel merging for Brokers on a dedicated async ForkJoinPool. If false, instead merges will be done serially on the HTTP thread pool. true druid.processing.merge.pool.parallelism Size of ForkJoinPool. Note that the default configuration assumes that the value returned by Runtime.getRuntime().availableProcessors() represents 2 hyper-threads per physical core, and multiplies this value by 0.75 in attempt to size 1.5 times the number of physical cores. Runtime.getRuntime().availableProcessors() * 0.75 (rounded up) druid.processing.merge.pool.defaultMaxQueryParallelism Default maximum number of parallel merge tasks per query. Note that the default configuration assumes that the value returned by Runtime.getRuntime().availableProcessors() represents 2 hyper-threads per physical core, and multiplies this value by 0.5 in attempt to size to the number of physical cores. Runtime.getRuntime().availableProcessors() * 0.5 (rounded up) druid.processing.merge.pool.awaitShutdownMillis Time to wait for merge ForkJoinPool tasks to complete before ungracefully stopping on process shutdown in milliseconds. 60_000 druid.processing.merge.task.targetRunTimeMillis Ideal run-time of each ForkJoinPool merge task, before forking off a new task to continue merging sequences. 100 druid.processing.merge.task.initialYieldNumRows Number of rows to yield per ForkJoinPool merge task, before forking off a new task to continue merging sequences. 16384 druid.processing.merge.task.smallBatchNumRows Size of result batches to operate on in ForkJoinPool merge tasks. 4096 The amount of direct memory needed by Druid is at least druid.processing.buffer.sizeBytes * (druid.processing.numMergeBuffers + druid.processing.numThreads + 1). You can ensure at least this amount of direct memory is available by providing -XX:MaxDirectMemorySize= at the command line. Broker query configuration See general query configuration. SQL The Druid SQL server is configured through the following properties on the Broker. Property Description Default druid.sql.enable Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely. true druid.sql.avatica.enable Whether to enable JDBC querying at /druid/v2/sql/avatica/. true druid.sql.avatica.maxConnections Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections. 25 druid.sql.avatica.maxRowsPerFrame Maximum number of rows to return in a single JDBC frame. Setting this property to -1 indicates that no row limit should be applied. Clients can optionally specify a row limit in their requests; if a client specifies a row limit, the lesser value of the client-provided limit and maxRowsPerFrame will be used. 5,000 druid.sql.avatica.maxStatementsPerConnection Maximum number of simultaneous open statements per Avatica client connection. 4 druid.sql.avatica.connectionIdleTimeout Avatica client connection idle timeout. PT5M druid.sql.http.enable Whether to enable JSON over HTTP querying at /druid/v2/sql/. true druid.sql.planner.maxTopNLimit Maximum threshold for a TopN query. Higher limits will be planned as GroupBy queries instead. 100000 druid.sql.planner.metadataRefreshPeriod Throttle for metadata refreshes. PT1M druid.sql.planner.useApproximateCountDistinct Whether to use an approximate cardinality algorithm for COUNT(DISTINCT foo). true druid.sql.planner.useApproximateTopN Whether to use approximate TopN queries when a SQL query could be expressed as such. If false, exact GroupBy queries will be used instead. true druid.sql.planner.requireTimeCondition Whether to require SQL to have filter conditions on time column so that all generated native queries will have user specified intervals. If true, all queries without filter condition on time column will fail false druid.sql.planner.sqlTimeZone Sets the default time zone for the server, which will affect how time functions and timestamp literals behave. Should be a time zone name like \"America/Los_Angeles\" or offset like \"-08:00\". UTC druid.sql.planner.metadataSegmentCacheEnable Whether to keep a cache of published segments in broker. If true, broker polls coordinator in background to get segments from metadata store and maintains a local cache. If false, coordinator's REST API will be invoked when broker needs published segments info. false druid.sql.planner.metadataSegmentPollPeriod How often to poll coordinator for published segments list if druid.sql.planner.metadataSegmentCacheEnable is set to true. Poll period is in milliseconds. 60000 Previous versions of Druid had properties named druid.sql.planner.maxQueryCount and druid.sql.planner.maxSemiJoinRowsInMemory. These properties are no longer available. Since Druid 0.18.0, you can use druid.server.http.maxSubqueryRows to control the maximum number of rows permitted across all subqueries. Broker Caching You can optionally only configure caching to be enabled on the Broker by setting caching configs here. Property Possible Values Description Default druid.broker.cache.useCache true, false Enable the cache on the Broker. false druid.broker.cache.populateCache true, false Populate the cache on the Broker. false druid.broker.cache.useResultLevelCache true, false Enable result level caching on the Broker. false druid.broker.cache.populateResultLevelCache true, false Populate the result level cache on the Broker. false druid.broker.cache.resultLevelCacheLimit positive integer Maximum size of query response that can be cached. Integer.MAX_VALUE druid.broker.cache.unCacheable All druid query types All query types to not cache. [] druid.broker.cache.cacheBulkMergeLimit positive integer or 0 Queries with more segments than this number will not attempt to fetch from cache at the broker level, leaving potential caching fetches (and cache result merging) to the Historicals Integer.MAX_VALUE druid.broker.cache.maxEntrySize positive integer Maximum cache entry size in bytes. 1_000_000 See cache configuration for how to configure cache settings. Segment Discovery Property Possible Values Description Default druid.serverview.type batch or http Segment discovery method to use. \"http\" enables discovering segments using HTTP instead of zookeeper. batch druid.broker.segment.watchedTiers List of strings Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of tiers. By default, Broker would consider all tiers. This can be used to partition your dataSources in specific Historical tiers and configure brokers in partitions so that they are only queryable for specific dataSources. none druid.broker.segment.watchedDataSources List of strings Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of dataSources. By default, Broker would consider all datasources. This can be used to configure brokers in partitions so that they are only queryable for specific dataSources. none druid.broker.segment.awaitInitializationOnStart Boolean Whether the Broker will wait for its view of segments to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also druid.sql.planner.awaitInitializationOnStart, a related setting. true Cache Configuration This section describes caching configuration that is common to Broker, Historical, and MiddleManager/Peon processes. Caching could optionally be enabled on the Broker, Historical, and MiddleManager/Peon processes. See Broker, Historical, and Peon configuration options for how to enable it for different processes. Druid uses a local in-memory cache by default, unless a different type of cache is specified. Use the druid.cache.type configuration to set a different kind of cache. Cache settings are set globally, so the same configuration can be re-used for both Broker and Historical processes, when defined in the common properties file. Cache Type Property Possible Values Description Default druid.cache.type local, memcached, hybrid, caffeine The type of cache to use for queries. See below of the configuration options for each cache type caffeine Local Cache DEPRECATED: Use caffeine (default as of v0.12.0) instead The local cache is deprecated in favor of the Caffeine cache, and may be removed in a future version of Druid. The Caffeine cache affords significantly better performance and control over eviction behavior compared to local cache, and is recommended in any situation where you are using JRE 8u60 or higher. A simple in-memory LRU cache. Local cache resides in JVM heap memory, so if you enable it, make sure you increase heap size accordingly. Property Description Default druid.cache.sizeInBytes Maximum cache size in bytes. Zero disables caching. 0 druid.cache.initialSize Initial size of the hashtable backing the cache. 500000 druid.cache.logEvictionCount If non-zero, log cache eviction every logEvictionCount items. 0 Caffeine Cache A highly performant local cache implementation for Druid based on Caffeine. Requires a JRE8u60 or higher if using COMMON_FJP. Configuration Below are the configuration options known to this module: runtime.properties Description Default druid.cache.type Set this to caffeine or leave out parameter caffeine druid.cache.sizeInBytes The maximum size of the cache in bytes on heap. It can be configured as described in here. min(1GiB, Runtime.maxMemory / 10) druid.cache.expireAfter The time (in ms) after an access for which a cache entry may be expired None (no time limit) druid.cache.cacheExecutorFactory The executor factory to use for Caffeine maintenance. One of COMMON_FJP, SINGLE_THREAD, or SAME_THREAD ForkJoinPool common pool (COMMON_FJP) druid.cache.evictOnClose If a close of a namespace (ex: removing a segment from a process) should cause an eager eviction of associated cache values false druid.cache.cacheExecutorFactory Here are the possible values for druid.cache.cacheExecutorFactory, which controls how maintenance tasks are run COMMON_FJP (default) use the common ForkJoinPool. Should use with JRE 8u60 or higher. Older versions of the JRE may have worse performance than newer JRE versions. SINGLE_THREAD Use a single-threaded executor. SAME_THREAD Cache maintenance is done eagerly. Metrics In addition to the normal cache metrics, the caffeine cache implementation also reports the following in both total and delta Metric Description Normal value query/cache/caffeine/*/requests Count of hits or misses hit + miss query/cache/caffeine/*/loadTime Length of time caffeine spends loading new values (unused feature) 0 query/cache/caffeine/*/evictionBytes Size in bytes that have been evicted from the cache Varies, should tune cache sizeInBytes so that sizeInBytes/evictionBytes is approximately the rate of cache churn you desire Memcached Uses memcached as cache backend. This allows all processes to share the same cache. Property Description Default druid.cache.expiration Memcached expiration time. 2592000 (30 days) druid.cache.timeout Maximum time in milliseconds to wait for a response from Memcached. 500 druid.cache.hosts Comma separated list of Memcached hosts . none druid.cache.maxObjectSize Maximum object size in bytes for a Memcached object. 52428800 (50 MB) druid.cache.memcachedPrefix Key prefix for all keys in Memcached. druid druid.cache.numConnections Number of memcached connections to use. 1 druid.cache.protocol Memcached communication protocol. Can be binary or text. binary druid.cache.locator Memcached locator. Can be consistent or array_mod. consistent Hybrid Uses a combination of any two caches as a two-level L1 / L2 cache. This may be used to combine a local in-memory cache with a remote memcached cache. Cache requests will first check L1 cache before checking L2. If there is an L1 miss and L2 hit, it will also populate L1. Property Description Default druid.cache.l1.type type of cache to use for L1 cache. See druid.cache.type configuration for valid types. caffeine druid.cache.l2.type type of cache to use for L2 cache. See druid.cache.type configuration for valid types. caffeine druid.cache.l1.* Any property valid for the given type of L1 cache can be set using this prefix. For instance, if you are using a caffeine L1 cache, specify druid.cache.l1.sizeInBytes to set its size. defaults are the same as for the given cache type. druid.cache.l2.* Prefix for L2 cache settings, see description for L1. defaults are the same as for the given cache type. druid.cache.useL2 A boolean indicating whether to query L2 cache, if it's a miss in L1. It makes sense to configure this to false on Historical processes, if L2 is a remote cache like memcached, and this cache also used on brokers, because in this case if a query reached Historical it means that a broker didn't find corresponding results in the same remote cache, so a query to the remote cache from Historical is guaranteed to be a miss. true druid.cache.populateL2 A boolean indicating whether to put results into L2 cache. true General query configuration This section describes configurations that control behavior of Druid's query types, applicable to Broker, Historical, and MiddleManager processes. Overriding default query context values Any Query Context General Parameter default value can be overridden by setting runtime property in the format of druid.query.default.context.{query_context_key}. druid.query.default.context.{query_context_key} runtime property prefix applies to all current and future query context keys, the same as how query context parameter passed with the query works. Note that the runtime property value can be overridden if value for the same key is explicitly specify in the query contexts. The precedence chain for query context values is as follows: hard-coded default value in Druid code druid.query.default.context druid.query.default.context Note that not all query context key has a runtime property not prefixed with druid.query.default.context that can override the hard-coded default value. For example, maxQueuedBytes has druid.broker.http.maxQueuedBytes but joinFilterRewriteMaxSize does not. Hence, the only way of overriding joinFilterRewriteMaxSize hard-coded default value is with runtime property druid.query.default.context.joinFilterRewriteMaxSize. To further elaborate on the previous example: If neither druid.broker.http.maxQueuedBytes or druid.query.default.context.maxQueuedBytes is set and the query does not have maxQueuedBytes in the context, then the hard-coded value in Druid code is use. If runtime property only contains druid.broker.http.maxQueuedBytes=x and query does not have maxQueuedBytes in the context, then the value of the property, x, is use. However, if query does have maxQueuedBytes in the context, then that value is use instead. If runtime property only contains druid.query.default.context.maxQueuedBytes=y OR runtime property contains both druid.broker.http.maxQueuedBytes=x and druid.query.default.context.maxQueuedBytes=y, then the value of druid.query.default.context.maxQueuedBytes, y, is use (given that query does not have maxQueuedBytes in the context). If query does have maxQueuedBytes in the context, then that value is use instead. TopN query config Property Description Default druid.query.topN.minTopNThreshold See TopN Aliasing for details. 1000 Search query config Property Description Default druid.query.search.maxSearchLimit Maximum number of search results to return. 1000 druid.query.search.searchStrategy Default search query strategy. useIndexes SegmentMetadata query config Property Description Default druid.query.segmentMetadata.defaultHistory When no interval is specified in the query, use a default interval of defaultHistory before the end time of the most recent segment, specified in ISO8601 format. This property also controls the duration of the default interval used by GET /druid/v2/datasources/{dataSourceName} interactions for retrieving datasource dimensions/metrics. P1W druid.query.segmentMetadata.defaultAnalysisTypes This can be used to set the Default Analysis Types for all segment metadata queries, this can be overridden when making the query [\"cardinality\", \"interval\", \"minmax\"] GroupBy query config This section describes the configurations for groupBy queries. You can set the runtime properties in the runtime.properties file on Broker, Historical, and MiddleManager processes. You can set the query context parameters through the query context. Configurations for groupBy v2 Supported runtime properties: Property Description Default druid.query.groupBy.maxMergingDictionarySize Maximum amount of heap space (approximately) to use for the string dictionary during merging. When the dictionary exceeds this size, a spill to disk will be triggered. 100000000 druid.query.groupBy.maxOnDiskStorage Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling. 0 (disabled) Supported query contexts: Key Description maxMergingDictionarySize Can be used to lower the value of druid.query.groupBy.maxMergingDictionarySize for this query. maxOnDiskStorage Can be used to lower the value of druid.query.groupBy.maxOnDiskStorage for this query. Advanced configurations Common configurations for all groupBy strategies Supported runtime properties: Property Description Default druid.query.groupBy.defaultStrategy Default groupBy query strategy. v2 druid.query.groupBy.singleThreaded Merge results using a single thread. false Supported query contexts: Key Description groupByStrategy Overrides the value of druid.query.groupBy.defaultStrategy for this query. groupByIsSingleThreaded Overrides the value of druid.query.groupBy.singleThreaded for this query. GroupBy v2 configurations Supported runtime properties: Property Description Default druid.query.groupBy.bufferGrouperInitialBuckets Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024). 0 druid.query.groupBy.bufferGrouperMaxLoadFactor Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7). 0 druid.query.groupBy.forceHashAggregation Force to use hash-based aggregation. false druid.query.groupBy.intermediateCombineDegree Number of intermediate processes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful CPU cores. 8 druid.query.groupBy.numParallelCombineThreads Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(druid.query.groupBy.numParallelCombineThreads, druid.processing.numThreads). 1 (disabled) Supported query contexts: Key Description Default bufferGrouperInitialBuckets Overrides the value of druid.query.groupBy.bufferGrouperInitialBuckets for this query. None bufferGrouperMaxLoadFactor Overrides the value of druid.query.groupBy.bufferGrouperMaxLoadFactor for this query. None forceHashAggregation Overrides the value of druid.query.groupBy.forceHashAggregation None intermediateCombineDegree Overrides the value of druid.query.groupBy.intermediateCombineDegree None numParallelCombineThreads Overrides the value of druid.query.groupBy.numParallelCombineThreads None sortByDimsFirst Sort the results first by dimension values and then by timestamp. false forceLimitPushDown When all fields in the orderby are part of the grouping key, the broker will push limit application down to the Historical processes. When the sorting order uses fields that are not in the grouping key, applying this optimization can result in approximate results with unknown accuracy, so this optimization is disabled by default in that case. Enabling this context flag turns on limit push down for limit/orderbys that contain non-grouping key columns. false GroupBy v1 configurations Supported runtime properties: Property Description Default druid.query.groupBy.maxIntermediateRows Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail. 50000 druid.query.groupBy.maxResults Maximum number of results. Queries that exceed this limit will fail. 500000 Supported query contexts: Key Description Default maxIntermediateRows Can be used to lower the value of druid.query.groupBy.maxIntermediateRows for this query. None maxResults Can be used to lower the value of druid.query.groupBy.maxResults for this query. None useOffheap Set to true to store aggregations off-heap when merging results. false Router Router Process Configs Property Description Default druid.host The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process InetAddress.getLocalHost().getCanonicalHostName() druid.bindOnHost Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces. false druid.plaintextPort This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host 8888 druid.tlsPort TLS port for HTTPS connector, if druid.enableTlsPort is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer. 9088 druid.service The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services druid/router Runtime Configuration Property Description Default druid.router.defaultBrokerServiceName The default Broker to connect to in case service discovery fails. druid/broker druid.router.tierToBrokerMap Queries for a certain tier of data are routed to their appropriate Broker. This value should be an ordered JSON map of tiers to Broker names. The priority of Brokers is based on the ordering. {\"_default_tier\": \"\"} druid.router.defaultRule The default rule for all datasources. \"_default\" druid.router.pollPeriod How often to poll for new rules. PT1M druid.router.strategies Please see Router Strategies for details. [{\"type\":\"timeBoundary\"},{\"type\":\"priority\"}] druid.router.avatica.balancer.type Class to use for balancing Avatica queries across Brokers. Please see Avatica Query Balancing. rendezvousHash druid.router.managementProxy.enabled Enables the Router's management proxy functionality. false druid.router.http.numConnections Size of connection pool for the Router to connect to Broker processes. If there are more queries than this number that all need to speak to the same process, then they will queue up. 20 druid.router.http.readTimeout The timeout for data reads from Broker processes. PT15M druid.router.http.numMaxThreads Maximum number of worker threads to handle HTTP requests and responses max(10, ((number of cores * 17) / 16 + 2) + 30) druid.router.http.numRequestsQueued Maximum number of requests that may be queued to a destination 1024 druid.router.http.requestBuffersize Size of the content buffer for receiving requests. These buffers are only used for active connections that have requests with bodies that will not fit within the header buffer 8 * 1024 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"development/extensions.html":{"url":"development/extensions.html","title":"扩展组件","keywords":"","body":" Druid implements an extension system that allows for adding functionality at runtime. Extensions are commonly used to add support for deep storages (like HDFS and S3), metadata stores (like MySQL and PostgreSQL), new aggregators, new input formats, and so on. Production clusters will generally use at least two extensions; one for deep storage and one for a metadata store. Many clusters will also use additional extensions. Core extensions Core extensions are maintained by Druid committers. Name Description Docs druid-avro-extensions Support for data in Apache Avro data format. link druid-azure-extensions Microsoft Azure deep storage. link druid-basic-security Support for Basic HTTP authentication and role-based access control. link druid-bloom-filter Support for providing Bloom filters in druid queries. link druid-datasketches Support for approximate counts and set operations with Apache DataSketches. link druid-google-extensions Google Cloud Storage deep storage. link druid-hdfs-storage HDFS deep storage. link druid-histogram Approximate histograms and quantiles aggregator. Deprecated, please use the DataSketches quantiles aggregator from the druid-datasketches extension instead. link druid-kafka-extraction-namespace Apache Kafka-based namespaced lookup. Requires namespace lookup extension. link druid-kafka-indexing-service Supervised exactly-once Apache Kafka ingestion for the indexing service. link druid-kinesis-indexing-service Supervised exactly-once Kinesis ingestion for the indexing service. link druid-kerberos Kerberos authentication for druid processes. link druid-lookups-cached-global A module for lookups providing a jvm-global eager caching for lookups. It provides JDBC and URI implementations for fetching lookup data. link druid-lookups-cached-single Per lookup caching module to support the use cases where a lookup need to be isolated from the global pool of lookups link druid-orc-extensions Support for data in Apache ORC data format. link druid-parquet-extensions Support for data in Apache Parquet data format. Requires druid-avro-extensions to be loaded. link druid-protobuf-extensions Support for data in Protobuf data format. link druid-ranger-security Support for access control through Apache Ranger. link druid-s3-extensions Interfacing with data in AWS S3, and using S3 as deep storage. link druid-ec2-extensions Interfacing with AWS EC2 for autoscaling middle managers UNDOCUMENTED druid-aws-rds-extensions Support for AWS token based access to AWS RDS DB Cluster. link druid-stats Statistics related module including variance and standard deviation. link mysql-metadata-storage MySQL metadata store. link postgresql-metadata-storage PostgreSQL metadata store. link simple-client-sslcontext Simple SSLContext provider module to be used by Druid's internal HttpClient when talking to other Druid processes over HTTPS. link druid-pac4j OpenID Connect authentication for druid processes. link Community extensions Community extensions are not maintained by Druid committers, although we accept patches from community members using these extensions. They may not have been as extensively tested as the core extensions. A number of community members have contributed their own extensions to Druid that are not packaged with the default Druid tarball. If you'd like to take on maintenance for a community extension, please post on dev@druid.apache.org to let us know! All of these community extensions can be downloaded using pull-deps while specifying a -c coordinate option to pull org.apache.druid.extensions.contrib:{EXTENSION_NAME}:{DRUID_VERSION}. Name Description Docs aliyun-oss-extensions Aliyun OSS deep storage link ambari-metrics-emitter Ambari Metrics Emitter link druid-cassandra-storage Apache Cassandra deep storage. link druid-cloudfiles-extensions Rackspace Cloudfiles deep storage and firehose. link druid-distinctcount DistinctCount aggregator link druid-redis-cache A cache implementation for Druid based on Redis. link druid-time-min-max Min/Max aggregator for timestamp. link sqlserver-metadata-storage Microsoft SQLServer deep storage. link graphite-emitter Graphite metrics emitter link statsd-emitter StatsD metrics emitter link kafka-emitter Kafka metrics emitter link druid-thrift-extensions Support thrift ingestion link druid-opentsdb-emitter OpenTSDB metrics emitter link materialized-view-selection, materialized-view-maintenance Materialized View link druid-moving-average-query Support for Moving Average and other Aggregate Window Functions in Druid queries. link druid-influxdb-emitter InfluxDB metrics emitter link druid-momentsketch Support for approximate quantile queries using the momentsketch library link druid-tdigestsketch Support for approximate sketch aggregators based on T-Digest link gce-extensions GCE Extensions link Promoting community extensions to core extensions Please post on dev@druid.apache.org if you'd like an extension to be promoted to core. If we see a community extension actively supported by the community, we can promote it to core based on community feedback. For information how to create your own extension, please see here. Loading extensions Loading core extensions Apache Druid bundles all core extensions out of the box. See the list of extensions for your options. You can load bundled extensions by adding their names to your common.runtime.properties druid.extensions.loadList property. For example, to load the postgresql-metadata-storage and druid-hdfs-storage extensions, use the configuration: druid.extensions.loadList=[\"postgresql-metadata-storage\", \"druid-hdfs-storage\"] These extensions are located in the extensions directory of the distribution. Druid bundles two sets of configurations: one for the quickstart and one for a clustered configuration. Make sure you are updating the correct common.runtime.properties for your setup. Because of licensing, the mysql-metadata-storage extension does not include the required MySQL JDBC driver. For instructions on how to install this library, see the MySQL extension page. Loading community extensions You can also load community and third-party extensions not already bundled with Druid. To do this, first download the extension and then install it into your extensions directory. You can download extensions from their distributors directly, or if they are available from Maven, the included pull-deps can download them for you. To use pull-deps, specify the full Maven coordinate of the extension in the form groupId:artifactId:version. For example, for the (hypothetical) extension com.example:druid-example-extension:1.0.0, run: java \\ -cp \"lib/*\" \\ -Ddruid.extensions.directory=\"extensions\" \\ -Ddruid.extensions.hadoopDependenciesDir=\"hadoop-dependencies\" \\ org.apache.druid.cli.Main tools pull-deps \\ --no-default-hadoop \\ -c \"com.example:druid-example-extension:1.0.0\" You only have to install the extension once. Then, add \"druid-example-extension\" to druid.extensions.loadList in common.runtime.properties to instruct Druid to load the extension. Please make sure all the Extensions related configuration properties listed here are set correctly. The Maven groupId for almost every community extension is org.apache.druid.extensions.contrib. The artifactId is the name of the extension, and the version is the latest Druid stable version. Loading extensions from the classpath If you add your extension jar to the classpath at runtime, Druid will also load it into the system. This mechanism is relatively easy to reason about, but it also means that you have to ensure that all dependency jars on the classpath are compatible. That is, Druid makes no provisions while using this method to maintain class loader isolation so you must make sure that the jars on your classpath are mutually compatible. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"configuration/logging.html":{"url":"configuration/logging.html","title":"运行日志","keywords":"","body":" Apache Druid processes will emit logs that are useful for debugging to the console. Druid processes also emit periodic metrics about their state. For more about metrics, see Configuration. Metric logs are printed to the console by default, and can be disabled with -Ddruid.emitter.logging.logLevel=debug. Druid uses log4j2 for logging. Logging can be configured with a log4j2.xml file. Add the path to the directory containing the log4j2.xml file (e.g. the _common/ dir) to your classpath if you want to override default Druid log configuration. Note that this directory should be earlier in the classpath than the druid jars. The easiest way to do this is to prefix the classpath with the config dir. To enable java logging to go through log4j2, set the -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager server parameter. An example log4j2.xml ships with Druid under config/_common/log4j2.xml, and a sample file is also shown below: --> My logs are really chatty, can I set them to asynchronously write? Yes, using a log4j2.xml similar to the following causes some of the more chatty classes to write asynchronously: 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/druid-console.html":{"url":"operations/druid-console.html","title":"Web控制台","keywords":"","body":" Druid include a console for managing datasources, segments, tasks, data processes (Historicals and MiddleManagers), and coordinator dynamic configuration. Users can also run SQL and native Druid queries in the console. The Druid Console is hosted by the Router process. The following cluster settings must be enabled, as they are by default: the Router's management proxy must be enabled. the Broker processes in the cluster must have Druid SQL enabled. The Druid console can be accessed at: http://: It is important to note that any Druid console user will have, effectively, the same file permissions as the user under which Druid runs. One way these permissions are surfaced is in the file browser dialog. The dialog will show console users the files that the underlying user has permissions to. In general, avoid running Druid as root user. Consider creating a dedicated user account for running Druid. Below is a description of the high-level features and functionality of the Druid Console Home The home view provides a high level overview of the cluster. Each card is clickable and links to the appropriate view. The legacy menu allows you to go to the legacy coordinator and overlord consoles should you need them. Data loader The data loader view allows you to load data by building an ingestion spec with a step-by-step wizard. After selecting the location of your data just follow the series for steps that will show you incremental previews of the data as it will be ingested. After filling in the required details on every step you can navigate to the next step by clicking the Next button. You can also freely navigate between the steps from the top navigation. Navigating with the top navigation will leave the underlying spec unmodified while clicking the Next button will attempt to fill in the subsequent steps with appropriate defaults. Datasources The datasources view shows all the currently enabled datasources. From this view you can see the sizes and availability of the different datasources. You can edit the retention rules, configure automatic compaction, and drop data. Like any view that is powered by a DruidSQL query you can click View SQL query for table from the ... menu to run the underlying SQL query directly. You can view and edit retention rules to determine the general availability of a datasource. Segments The segment view shows all the segments in the cluster. Each segment can be has a detail view that provides more information. The Segment ID is also conveniently broken down into Datasource, Start, End, Version, and Partition columns for ease of filtering and sorting. Tasks and supervisors From this view you can check the status of existing supervisors as well as suspend, resume, and reset them. The tasks table allows you see the currently running and recently completed tasks. To make managing a lot of tasks more accessible, you can group the tasks by their Type, Datasource, or Status to make navigation easier. Click on the magnifying glass for any supervisor to see detailed reports of its progress. Click on the magnifying glass for any task to see more detail about it. Servers The servers tab lets you see the current status of the nodes making up your cluster. You can group the nodes by type or by tier to get meaningful summary statistics. Query The query view lets you issue DruidSQL queries and display the results as a table. The view will attempt to infer your query and let you modify via contextual actions such as adding filters and changing the sort order when possible. The query view can also issue queries in Druid's native query format, which is JSON over HTTP. To send a native Druid query, you must start your query with { and format it as JSON. Lookups You can create and edit query time lookups via the lookup view. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/getting-started.html":{"url":"operations/getting-started.html","title":"快速开始","keywords":"","body":" Overview If you are new to Druid, we recommend reading the Design Overview and the Ingestion Overview first for a basic understanding of Druid. Single-server Quickstart and Tutorials To get started with running Druid, the simplest and quickest way is to try the single-server quickstart and tutorials. Deploying a Druid cluster If you wish to jump straight to deploying Druid as a cluster, or if you have an existing single-server deployment that you wish to migrate to a clustered deployment, please see the Clustered Deployment Guide. Operating Druid The configuration reference describes all of Druid's configuration properties. The API reference describes the APIs available on each Druid process. The basic cluster tuning guide is an introductory guide for tuning your Druid cluster. Need help with Druid? If you have questions about using Druid, please reach out to the Druid user mailing list or other community channels! 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/security-overview.html":{"url":"operations/security-overview.html","title":"概览","keywords":"","body":" Overview By default, security features in Druid are disabled, which simplifies the initial deployment experience. However, security features must be configured in a production deployment. These features including TLS, authentication, and authorization. To implement Druid security, you configure authenticators and authorizers. Authenticators control the way user identities are verified, while authorizers map the authenticated users (via user roles) to the datasources they are permitted to access. Consequently, implementing Druid security also involves consideration of your datasource scheme, given they represent the granularity at which data access permissions are allocated. The following graphic depicts the course of request through the authentication process: This document gives you an overview of security features in Druid and how to configure them, and some best practices for securing Druid. Best practices Do not expose the Druid Console without authentication on untrusted networks. Access to the console effectively confers access the file system on the installation machine, via file browsers in the UI. You should use an API gateway that restricts who can connect from untrusted networks, whitelist the specific APIs that your users need to access, and implements account lockout and throttling features. Grant users the minimum permissions necessary to perform their functions. For instance, do not allow user who only need to query data to write to data sources or view state. Disable JavaScript, as noted in the Security section of the JavaScript guide. Run Druid as an unprivileged Unix user on the installation machine (not root). This is an important point! Administrator users on Druid have the same permission as the Unix user account it is running under. If the Druid process is running under the root user account in the OS, then Administrator users on Druid can read/write all files that the root account has access to, including sensitive files such as /etc/passwd. You can configure authentication and authorization to control access to the the Druid APIs. The first step is enabling TLS for the cluster nodes. Then configure users, roles, and permissions, as described in the following sections. The configuration settings mentioned below are primarily located in the common.runtime.properties file. Note that you need to make the configuration changes on each Druid server in the cluster. Enable TLS The first step in securing Druid is enabling TLS. You can enable TLS to secure external client connections to Druid as well as connections between cluster nodes. The configuration steps are: Enable TLS by adding druid.enableTlsPort=true to common.runtime.properties on each node in the Druid cluster. Disable the non-TLS port by setting druid.enablePlaintextPort to false. Follow the steps in Understanding Certificates and Keys to generate or import a key and certificate. Configure the keystore and truststore settings in common.runtime.properties. The file should look something like this: druid.enablePlaintextPort=false druid.enableTlsPort=true druid.server.https.keyStoreType=jks druid.server.https.keyStorePath=imply-keystore.jks druid.server.https.keyStorePassword=secret123 # replace with your own password druid.server.https.certAlias=druid druid.client.https.protocol=TLSv1.2 druid.client.https.trustStoreType=jks druid.client.https.trustStorePath=imply-truststore.jks druid.client.https.trustStorePassword=secret123 # replace with your own password Add the simple-client-sslcontext extension to druid.extensions.loadList in common.runtime.properties. This enables TLS for Druid nodes acting as clients. Restart the cluster. For more information, see TLS support and Simple SSLContext Provider Module. Druid uses Jetty as its embedded web server. Therefore you refer to Understanding Certificates and Keys for complete instructions. Enable an authenticator To authenticate requests in Druid, you configure an Authenticator. Authenticator extensions exist for HTTP basic authentication, LDAP, and Kerberos. The following takes you through sample configuration steps for enabling basic auth: Add the druid-basic-security extension to druid.extensions.loadList in common.runtime.properties. For the quickstart installation, for example, the properties file is at conf/druid/cluster/_common:druid.extensions.loadList=[\"druid-basic-security\", \"druid-histogram\", \"druid-datasketches\", \"druid-kafka-indexing-service\", \"imply-utility-belt\"] Configure the basic Authenticator, Authorizer, and Escalator settings in the same common.runtime.properties file. For example: # Druid basic security druid.auth.authenticatorChain=[\"MyBasicMetadataAuthenticator\"] druid.auth.authenticator.MyBasicMetadataAuthenticator.type=basic druid.auth.authenticator.MyBasicMetadataAuthenticator.initialAdminPassword=password1 druid.auth.authenticator.MyBasicMetadataAuthenticator.initialInternalClientPassword=password2 druid.auth.authenticator.MyBasicMetadataAuthenticator.credentialsValidator.type=metadata druid.auth.authenticator.MyBasicMetadataAuthenticator.skipOnFailure=false druid.auth.authenticator.MyBasicMetadataAuthenticator.authorizerName=MyBasicMetadataAuthorizer # Escalator druid.escalator.type=basic druid.escalator.internalClientUsername=druid_system druid.escalator.internalClientPassword=password2 druid.escalator.authorizerName=MyBasicMetadataAuthorizer druid.auth.authorizers=[\"MyBasicMetadataAuthorizer\"] druid.auth.authorizer.MyBasicMetadataAuthorizer.type=basic Restart the cluster. See Authentication and Authorization for more information about the Authenticator, Escalator, and Authorizer concepts. See Basic Security for more information about the extension used in the examples above, and Kerberos for Kerberos authentication. Enable authorizers After enabling the basic auth extension, you can add users, roles, and permissions via the Druid Coordinator user endpoint. Note that you cannot assign permissions directly to individual users. They must be assigned through roles. The following diagram depicts the authorization model, and the relationship between users, roles, permissions, and resources. The following steps walk through a sample setup procedure: The default Coordinator API port is 8081 for non-TLS connections and 8281 for secured connections. Create a user by issuing a POST request to druid-ext/basic-security/authentication/db/MyBasicMetadataAuthenticator/users/, replacing USERNAME with the new username. For example: curl -u admin:password -XPOST https://my-coordinator-ip:8281/druid-ext/basic-security/authentication/db/basic/users/myname If you have TLS enabled, be sure to adjust the curl command accordingly. For example, if your Druid servers use self-signed certificates, you may choose to include the insecure curl option to forgo certificate checking for the curl command. Add a credential for the user by issuing a POST to druid-ext/basic-security/authentication/db/MyBasicMetadataAuthenticator/users//credentials. For example: curl -u admin:password -H'Content-Type: application/json' -XPOST --data-binary @pass.json https://my-coordinator-ip:8281/druid-ext/basic-security/authentication/db/basic/users/myname/credentials The password is conveyed in the pass.json file in the following form:``` { \"password\": \"password\" } ``` For each authenticator user you create, create a corresponding authorizer user by issuing a POST request to druid-ext/basic-security/authorization/db/MyBasicMetadataAuthorizer/users/. For example: curl -u admin:password -XPOST https://my-coordinator-ip:8281/druid-ext/basic-security/authorization/db/basic/users/myname Create authorizer roles to control permissions by issuing a POST request to druid-ext/basic-security/authorization/db/MyBasicMetadataAuthorizer/roles/. For example: curl -u admin:password -XPOST https://my-coordinator-ip:8281/druid-ext/basic-security/authorization/db/basic/roles/myrole Assign roles to users by issuing a POST request to druid-ext/basic-security/authorization/db/MyBasicMetadataAuthorizer/users//roles/. For example: curl -u admin:password -XPOST https://my-coordinator-ip:8281/druid-ext/basic-security/authorization/db/basic/users/myname/roles/myrole | jq Finally, attach permissions to the roles to control how they can interact with Druid at druid-ext/basic-security/authorization/db/MyBasicMetadataAuthorizer/roles//permissions. For example: curl -u admin:password -H'Content-Type: application/json' -XPOST --data-binary @perms.json https://my-coordinator-ip:8281/druid-ext/basic-security/authorization/db/basic/roles/myrole/permissions The payload of perms.json should be in the form:``` [ { \"resource\": { \"name\": \"\", \"type\": \"DATASOURCE\" }, \"action\": \"READ\" }, { \"resource\": { \"name\": \"STATE\", \"type\": \"STATE\" }, \"action\": \"READ\" } ] ``` Configuring an LDAP authenticator As an alternative to using the basic metadata authenticator, as shown in the previous section, you can use LDAP to authenticate users. The following steps provide an overview of the setup steps. For more information on these settings, see Properties for LDAP user authentication. In common.runtime.properties, add LDAP to the authenticator chain in the order in which you want requests to be evaluated. For example: # Druid basic security druid.auth.authenticatorChain=[\"ldap\", \"MyBasicMetadataAuthenticator\"] Configure LDAP settings in common.runtime.properties as appropriate for your LDAP scheme and system. For example: druid.auth.authenticator.ldap.type=basic druid.auth.authenticator.ldap.enableCacheNotifications=true druid.auth.authenticator.ldap.credentialsValidator.type=ldap druid.auth.authenticator.ldap.credentialsValidator.url=ldap://ad_host:389 druid.auth.authenticator.ldap.credentialsValidator.bindUser=ad_admin_user druid.auth.authenticator.ldap.credentialsValidator.bindPassword=ad_admin_password druid.auth.authenticator.ldap.credentialsValidator.baseDn=dc=example,dc=com druid.auth.authenticator.ldap.credentialsValidator.userSearch=(&(sAMAccountName=%s)(objectClass=user)) druid.auth.authenticator.ldap.credentialsValidator.userAttribute=sAMAccountName druid.auth.authenticator.ldap.authorizerName=ldapauth druid.escalator.type=basic druid.escalator.internalClientUsername=ad_interal_user druid.escalator.internalClientPassword=Welcome123 druid.escalator.authorizerName=ldapauth druid.auth.authorizers=[\"ldapauth\"] druid.auth.authorizer.ldapauth.type=basic druid.auth.authorizer.ldapauth.initialAdminUser= druid.auth.authorizer.ldapauth.initialAdminRole=admin druid.auth.authorizer.ldapauth.roleProvider.type=ldap Use the Druid API to create the group mapping and allocate initial roles. For example, using curl and given a group named group1 in the directory, run: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST -d @groupmap.json http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1map The groupmap.json file contents would be something like: { \"name\": \"group1map\", \"groupPattern\": \"CN=group1,CN=Users,DC=example,DC=com\", \"roles\": [ \"readRole\" ] } Check if the group mapping is created successfully by executing the following API. This lists all group mappings. curl -i -v -H \"Content-Type: application/json\" -u internal -X GET http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings Alternatively, to check the details of a specific group mapping, use the following API: curl -i -v -H \"Content-Type: application/json\" -u internal -X GET http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1map To add additional roles to the group mapping, use the following API: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1/roles/ Add the LDAP user to Druid. To add a user, use the following authentication API: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authentication/db/ldap/users/ Use the following command to assign the role to a user: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/users//roles/ Congratulations, you have configured permissions for user-assigned roles in Druid! 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/security-user-auth.html":{"url":"operations/security-user-auth.html","title":"用户认证","keywords":"","body":" This document describes the Druid security model that extensions use to enable user authentication and authorization services to Druid. Authentication and authorization model At the center of the Druid user authentication and authorization model are resources and actions. A resource is something that authenticated users are trying to access or modify. An action is something that users are trying to do. There are three resource types: DATASOURCE – Each Druid table (i.e., tables in the druid schema in SQL) is a resource. CONFIG – Configuration resources exposed by the cluster components. STATE – Cluster-wide state resources. For specific resources associated with the types, see the endpoint list below and corresponding descriptions in API Reference. There are two actions: READ – Used for read-only operations. WRITE – Used for operations that are not read-only. In practice, most deployments will only need to define two classes of users: Admins, who have WRITE action permissions on all resource types. These users will add datasources and administer the system. Data users, who only need READ access to DATASOURCE. These users should access Query APIs only through an API gateway. Other APIs and permissions include functionality that should be limited to server admins. It is important to note that WRITE access to DATASOURCE grants a user broad access. For instance, such users will have access to the Druid file system, S3 buckets, and credentials, among other things. As such, the ability to add and manage datasources should be allocated selectively to administrators. Default user accounts Authenticator If druid.auth.authenticator..initialAdminPassword is set, a default admin user named \"admin\" will be created, with the specified initial password. If this configuration is omitted, the \"admin\" user will not be created. If druid.auth.authenticator..initialInternalClientPassword is set, a default internal system user named \"druid_system\" will be created, with the specified initial password. If this configuration is omitted, the \"druid_system\" user will not be created. Authorizer Each Authorizer will always have a default \"admin\" and \"druid_system\" user with full privileges. Defining permissions There are two action types in Druid: READ and WRITE There are three resource types in Druid: DATASOURCE, CONFIG, and STATE. DATASOURCE Resource names for this type are datasource names. Specifying a datasource permission allows the administrator to grant users access to specific datasources. CONFIG There are two possible resource names for the \"CONFIG\" resource type, \"CONFIG\" and \"security\". Granting a user access to CONFIG resources allows them to access the following endpoints. \"CONFIG\" resource name covers the following endpoints: Endpoint Process Type /druid/coordinator/v1/config coordinator /druid/indexer/v1/worker overlord /druid/indexer/v1/worker/history overlord /druid/worker/v1/disable middleManager /druid/worker/v1/enable middleManager \"security\" resource name covers the following endpoint: Endpoint Process Type /druid-ext/basic-security/authentication coordinator /druid-ext/basic-security/authorization coordinator STATE There is only one possible resource name for the \"STATE\" config resource type, \"STATE\". Granting a user access to STATE resources allows them to access the following endpoints. \"STATE\" resource name covers the following endpoints: Endpoint Process Type /druid/coordinator/v1 coordinator /druid/coordinator/v1/rules coordinator /druid/coordinator/v1/rules/history coordinator /druid/coordinator/v1/servers coordinator /druid/coordinator/v1/tiers coordinator /druid/broker/v1 broker /druid/v2/candidates broker /druid/indexer/v1/leader overlord /druid/indexer/v1/isLeader overlord /druid/indexer/v1/action overlord /druid/indexer/v1/workers overlord /druid/indexer/v1/scaling overlord /druid/worker/v1/enabled middleManager /druid/worker/v1/tasks middleManager /druid/worker/v1/task/{taskid}/shutdown middleManager /druid/worker/v1/task/{taskid}/log middleManager /druid/historical/v1 historical /druid-internal/v1/segments/ historical /druid-internal/v1/segments/ peon /druid-internal/v1/segments/ realtime /status all process types HTTP methods For information on what HTTP methods are supported on a particular request endpoint, please refer to the API documentation. GET requires READ permission, while POST and DELETE require WRITE permission. SQL Permissions Queries on Druid datasources require DATASOURCE READ permissions for the specified datasource. Queries on the INFORMATION_SCHEMA tables will return information about datasources that the caller has DATASOURCE READ access to. Other datasources will be omitted. Queries on the system schema tables require the following permissions: segments: Segments will be filtered based on DATASOURCE READ permissions. servers: The user requires STATE READ permissions. server_segments: The user requires STATE READ permissions and segments will be filtered based on DATASOURCE READ permissions. tasks: Tasks will be filtered based on DATASOURCE READ permissions. Configuration Propagation To prevent excessive load on the Coordinator, the Authenticator and Authorizer user/role Druid metadata store state is cached on each Druid process. Each process will periodically poll the Coordinator for the latest Druid metadata store state, controlled by the druid.auth.basic.common.pollingPeriod and druid.auth.basic.common.maxRandomDelay properties. When a configuration update occurs, the Coordinator can optionally notify each process with the updated Druid metadata store state. This behavior is controlled by the enableCacheNotifications and cacheNotificationTimeout properties on Authenticators and Authorizers. Note that because of the caching, changes made to the user/role Druid metadata store may not be immediately reflected at each Druid process. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/auth-ldap.html":{"url":"operations/auth-ldap.html","title":"LDAP认证","keywords":"","body":" This page describes how to set up Druid user authentication and authorization through LDAP. The first step is to enable LDAP authentication and authorization for Druid. You then map an LDAP group to roles and assign permissions and users to roles. Enable LDAP in Druid Before starting, verify that the active directory is reachable from the Druid Master servers. Command line tools such as ldapsearch and ldapwhoami, which are included with OpenLDAP, are useful for this testing. Check the connection First test that the basic connection and user credential works. For example, given a user uuser1@example.com, try: ldapwhoami -vv -H ldap://:389 -D\"uuser1@example.com\" -W Enter the password associated with the user when prompted and verify that the command succeeded. If it didn't, try the following troubleshooting steps: Verify that you've used the correct port for your LDAP instance. By default, the LDAP port is 389, but double-check with your LDAP admin if unable to connect. Check whether a network firewall is not preventing connections to the LDAP port. Check whether LDAP clients need to be specifically whitelisted at the LDAP server to be able to reach it. If so, add the Druid Coordinator server to the AD whitelist. Check the search criteria After verifying basic connectivity, check your search criteria. For example, the command for searching for user uuser1@example.com is as follows: ldapsearch -x -W -H ldap:// -D\"uuser1@example.com\" -b \"dc=example,dc=com\" \"(sAMAccountName=uuser1)\" Note the memberOf attribute in the results; it shows the groups that the user belongs to. You will use this value to map the LDAP group to the Druid roles later. The sAMAccountName attribute contains the authenticated user identity. Configure Druid user authentication with LDAP/Active Directory Enable the druid-basic-security extension in the common.runtime.properties file. See Security Overview for details. As a best practice, create a user in LDAP to be used for internal communication with Druid. In common.runtime.properties, update LDAP-related properties, as shown in the following listing: druid.auth.authenticatorChain=[\"ldap\"] druid.auth.authenticator.ldap.type=basic druid.auth.authenticator.ldap.enableCacheNotifications=true druid.auth.authenticator.ldap.credentialsValidator.type=ldap druid.auth.authenticator.ldap.credentialsValidator.url=ldap://: druid.auth.authenticator.ldap.credentialsValidator.bindUser= druid.auth.authenticator.ldap.credentialsValidator.bindPassword= druid.auth.authenticator.ldap.credentialsValidator.baseDn= druid.auth.authenticator.ldap.credentialsValidator.userSearch= druid.auth.authenticator.ldap.credentialsValidator.userAttribute=sAMAccountName druid.auth.authenticator.ldap.authorizerName=ldapauth druid.escalator.type=basic druid.escalator.internalClientUsername= druid.escalator.internalClientPassword=Welcome123 druid.escalator.authorizerName=ldapauth druid.auth.authorizers=[\"ldapauth\"] druid.auth.authorizer.ldapauth.type=basic druid.auth.authorizer.ldapauth.initialAdminUser=AD user who acts as the initial admin user, e.g.: internal@example.com> druid.auth.authorizer.ldapauth.initialAdminRole=admin druid.auth.authorizer.ldapauth.roleProvider.type=ldap Notice that the LDAP user created in the previous step, internal@example.com, serves as the internal client user and the initial admin user. Use LDAP groups to assign roles You can map LDAP groups to a role in Druid. Members in the group get access to the permissions of the corresponding role. Step 1: Create a role First create the role in Druid using the Druid REST API. Creating a role involves submitting a POST request to the Coordinator process. The following REST APIs to create the role to read access for datasource, config, state. As mentioned, the REST API calls need to address the Coordinator node. The examples used below use localhost as the Coordinator host and 8081 as the port. Adjust these settings according to your deployment. Call the following API to create role readRole . curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/roles/readRole Check that the role has been created successfully by entering the following: curl -i -v -H \"Content-Type: application/json\" -u internal -X GET http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/roles Step 2: Add permissions to a role You can now add one or more permission to the role. The following example adds read-only access to a wikipedia data source. Given the following JSON in a file named perm.json: [{ \"resource\": { \"name\": \"wikipedia\", \"type\": \"DATASOURCE\" }, \"action\": \"READ\" } ,{ \"resource\": { \"name\": \".*\", \"type\": \"STATE\" }, \"action\": \"READ\" }, { \"resource\": {\"name\": \".*\", \"type\": \"CONFIG\"}, \"action\": \"READ\"}] The following command associates the permissions in the JSON file with the role curl -i -v -H \"Content-Type: application/json\" -u internal -X POST -d@perm.json http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/roles/readRole/permissions Note that the STATE and CONFIG permissions in perm.json are needed to see the data source in the Druid console. If only querying permissions are needed, the READ action is sufficient: [{ \"resource\": { \"name\": \"wikipedia\", \"type\": \"DATASOURCE\" }, \"action\": \"READ\" }] You can also provide the name in the form of regular expression. For example, to give access to all data sources starting with wiki, specify the name as { \"name\": \"wiki.*\", ...... Step 3: Create group Mapping The following shows an example of a group to role mapping. It assumes that a group named group1 exists in the directory. Also assuming the following role mapping in a file named groupmap.json: { \"name\": \"group1map\", \"groupPattern\": \"CN=group1,CN=Users,DC=example,DC=com\", \"roles\": [ \"readRole\" ] } You can configure the mapping as follows: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST -d @groupmap.json http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1map To check whether the group mapping was created successfully, run the following command: curl -i -v -H \"Content-Type: application/json\" -u internal -X GET http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings To check the details of a specific group mapping, use the following: curl -i -v -H \"Content-Type: application/json\" -u internal -X GET http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1map To add additional roles to the group mapping, use the following API: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/groupMappings/group1/roles/ Step 4. Assign roles for individual LDAP users Once LDAP is enabled, only user passwords are verified with LDAP. You add the LDAP user to Druid as follows: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authentication/db/ldap/users/ Step 5. Assign the role to the user The following command shows how to assign a role to a user: curl -i -v -H \"Content-Type: application/json\" -u internal -X POST http://localhost:8081/druid-ext/basic-security/authorization/db/ldapauth/users//roles/ For more information about security and the basic security extension, see Security Overview. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/password-provider.html":{"url":"operations/password-provider.html","title":"密码管理","keywords":"","body":" Passwords help secure Apache Druid systems such as the metadata store and the keystore that contains server certificates, and so on. These passwords have corresponding runtime properties associated with them, for example druid.metadata.storage.connector.password corresponds to the metadata store password. By default users can directly set the passwords in plaintext for runtime properties. For example, druid.metadata.storage.connector.password=pwd sets the password to be used by Druid to connect to the metadata store to pwd. Alternatively, users can can set passwords as environment variables. Environment variable passwords allow users to avoid exposing passwords in the runtime.properties file. You can set an environment variable password as in the following example: druid.metadata.storage.connector.password={ \"type\": \"environment\", \"variable\": \"METADATA_STORAGE_PASSWORD\" } The values are described below. Field Type Description Required type String password provider type Yes: environment variable String environment variable to read password from Yes Another option that provides even greater control is to securely fetch passwords at runtime using a custom extension of the PasswordProvider interface that is registered at Druid process startup. For more information, see Adding a new Password Provider implementation. To use this implementation, simply set the relevant password runtime property similarly to how was shown for the environment variable password: druid.metadata.storage.connector.password={ \"type\": \"\", \"\": \"\", ... } 本文源自Apache Druid，阅读原文 更新于： 2020-12-08 09:31:10 "},"design/auth.html":{"url":"design/auth.html","title":"认证","keywords":"","body":" This document describes non-extension specific Apache Druid authentication and authorization configurations. Property Type Description Default Required druid.auth.authenticatorChain JSON List of Strings List of Authenticator type names [\"allowAll\"] no druid.escalator.type String Type of the Escalator that should be used for internal Druid communications. This Escalator must use an authentication scheme that is supported by an Authenticator in druid.auth.authenticatorChain. \"noop\" no druid.auth.authorizers JSON List of Strings List of Authorizer type names [\"allowAll\"] no druid.auth.unsecuredPaths List of Strings List of paths for which security checks will not be performed. All requests to these paths will be allowed. [] no druid.auth.allowUnauthenticatedHttpOptions Boolean If true, allow HTTP OPTIONS requests by unauthenticated users. This is primarily useful for supporting CORS preflight requests, which Druid does not support directly, but which can be enabled using third-party extensions.Note that you must add \"OPTIONS\" to druid.server.http.allowedHttpMethods.Also note that disabling authentication checks for OPTIONS requests will allow unauthenticated users to determine what Druid endpoints are valid (by checking if the OPTIONS request returns a 200 instead of 404). Enabling this option will reveal information about server configuration, including information about what extensions are loaded, to unauthenticated users. false no Enabling Authentication/AuthorizationLoadingLookupTest Authenticator chain Authentication decisions are handled by a chain of Authenticator instances. A request will be checked by Authenticators in the sequence defined by the druid.auth.authenticatorChain. Authenticator implementations are provided by extensions. For example, the following authenticator chain definition enables the Kerberos and HTTP Basic authenticators, from the druid-kerberos and druid-basic-security core extensions, respectively: druid.auth.authenticatorChain=[\"kerberos\", \"basic\"] A request will pass through all Authenticators in the chain, until one of the Authenticators successfully authenticates the request or sends an HTTP error response. Authenticators later in the chain will be skipped after the first successful authentication or if the request is terminated with an error response. If no Authenticator in the chain successfully authenticated a request or sent an HTTP error response, an HTTP error response will be sent at the end of the chain. Druid includes two built-in Authenticators, one of which is used for the default unsecured configuration. AllowAll authenticator This built-in Authenticator authenticates all requests, and always directs them to an Authorizer named \"allowAll\". It is not intended to be used for anything other than the default unsecured configuration. Anonymous authenticator This built-in Authenticator authenticates all requests, and directs them to an Authorizer specified in the configuration by the user. It is intended to be used for adding a default level of access so the Anonymous Authenticator should be added to the end of the authenticator chain. A request that reaches the Anonymous Authenticator at the end of the chain will succeed or fail depending on how the Authorizer linked to the Anonymous Authenticator is configured. Property Description Default Required druid.auth.authenticator..authorizerName Authorizer that requests should be directed to. N/A Yes druid.auth.authenticator..identity The identity of the requester. defaultUser No To use the Anonymous Authenticator, add an authenticator with type anonymous to the authenticatorChain. For example, the following enables the Anonymous Authenticator with the druid-basic-security extension: druid.auth.authenticatorChain=[\"basic\", \"anonymous\"] druid.auth.authenticator.anonymous.type=anonymous druid.auth.authenticator.anonymous.identity=defaultUser druid.auth.authenticator.anonymous.authorizerName=myBasicAuthorizer # ... usual configs for basic authentication would go here ... Trusted domain Authenticator This built-in Trusted Domain Authenticator authenticates requests originating from the configured trusted domain, and directs them to an Authorizer specified in the configuration by the user. It is intended to be used for adding a default level of trust and allow access for hosts within same domain. Property Description Default Required druid.auth.authenticator..name authenticator name. N/A Yes druid.auth.authenticator..domain Trusted Domain from which requests should be authenticated. If authentication is allowed for connections from only a given host, fully qualified hostname of that host needs to be specified. N/A Yes druid.auth.authenticator..useForwardedHeaders Clients connecting to druid could pass through many layers of proxy. Some proxies also append its own IP address to 'X-Forwarded-For' header before passing on the request to another proxy. Some proxies also connect on behalf of client. If this config is set to true and if 'X-Forwarded-For' is present, trusted domain authenticator will use left most host name from X-Forwarded-For header. Note: It is possible to spoof X-Forwarded-For headers in HTTP requests, enable this with caution. false No druid.auth.authenticator..authorizerName Authorizer that requests should be directed to. N/A Yes druid.auth.authenticator..identity The identity of the requester. defaultUser No To use the Trusted Domain Authenticator, add an authenticator with type trustedDomain to the authenticatorChain. For example, the following enables the Trusted Domain Authenticator : druid.auth.authenticatorChain=[\"trustedDomain\"] druid.auth.authenticator.trustedDomain.type=trustedDomain druid.auth.authenticator.trustedDomain.domain=trustedhost.mycompany.com druid.auth.authenticator.trustedDomain.identity=defaultUser druid.auth.authenticator.trustedDomain.authorizerName=myBasicAuthorizer druid.auth.authenticator.trustedDomain.name=myTrustedAutenticator # ... usual configs for druid would go here ... Escalator The druid.escalator.type property determines what authentication scheme should be used for internal Druid cluster communications (such as when a Broker process communicates with Historical processes for query processing). The Escalator chosen for this property must use an authentication scheme that is supported by an Authenticator in druid.auth.authenticatorChain. Authenticator extension implementers must also provide a corresponding Escalator implementation if they intend to use a particular authentication scheme for internal Druid communications. Noop escalator This built-in default Escalator is intended for use only with the default AllowAll Authenticator and Authorizer. Authorizers Authorization decisions are handled by an Authorizer. The druid.auth.authorizers property determines what Authorizer implementations will be active. There are two built-in Authorizers, \"default\" and \"noop\". Other implementations are provided by extensions. For example, the following authorizers definition enables the \"basic\" implementation from druid-basic-security: druid.auth.authorizers=[\"basic\"] Only a single Authorizer will authorize any given request. Druid includes one built in authorizer: AllowAll authorizer The Authorizer with type name \"allowAll\" accepts all requests. Default Unsecured Configuration When druid.auth.authenticatorChain is left empty or unspecified, Druid will create an authenticator chain with a single AllowAll Authenticator named \"allowAll\". When druid.auth.authorizers is left empty or unspecified, Druid will create a single AllowAll Authorizer named \"allowAll\". The default value of druid.escalator.type is \"noop\" to match the default unsecured Authenticator/Authorizer configurations. Authenticator to Authorizer Routing When an Authenticator successfully authenticates a request, it must attach a AuthenticationResult to the request, containing an information about the identity of the requester, as well as the name of the Authorizer that should authorize the authenticated request. An Authenticator implementation should provide some means through configuration to allow users to select what Authorizer(s) the Authenticator should route requests to. Internal system user Internal requests between Druid processes (non-user initiated communications) need to have authentication credentials attached. These requests should be run as an \"internal system user\", an identity that represents the Druid cluster itself, with full access permissions. The details of how the internal system user is defined is left to extension implementations. Authorizer Internal System User Handling Authorizers implementations must recognize and authorize an identity for the \"internal system user\", with full access permissions. Authenticator and Escalator Internal System User Handling An Authenticator implementation that is intended to support internal Druid communications must recognize credentials for the \"internal system user\", as provided by a corresponding Escalator implementation. An Escalator must implement three methods related to the internal system user: public HttpClient createEscalatedClient(HttpClient baseClient); public org.eclipse.jetty.client.HttpClient createEscalatedJettyClient(org.eclipse.jetty.client.HttpClient baseClient); public AuthenticationResult createEscalatedAuthenticationResult(); createEscalatedClient returns an wrapped HttpClient that attaches the credentials of the \"internal system user\" to requests. createEscalatedJettyClient is similar to createEscalatedClient, except that it operates on a Jetty HttpClient. createEscalatedAuthenticationResult returns an AuthenticationResult containing the identity of the \"internal system user\". Reserved Name Configuration Property For extension implementers, please note that the following configuration properties are reserved for the names of Authenticators and Authorizers: druid.auth.authenticator..name= druid.auth.authorizer..name= These properties provide the authenticator and authorizer names to the implementations as @JsonProperty parameters, potentially useful when multiple authenticators or authorizers of the same type are configured. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/tls-support.html":{"url":"operations/tls-support.html","title":"TLS加密支持","keywords":"","body":" General configuration Property Description Default druid.enablePlaintextPort Enable/Disable HTTP connector. true druid.enableTlsPort Enable/Disable HTTPS connector. false Although not recommended, the HTTP and HTTPS connectors can both be enabled at a time. The respective ports are configurable using druid.plaintextPort and druid.tlsPort properties on each process. Please see Configuration section of individual processes to check the valid and default values for these ports. Jetty server configuration Apache Druid uses Jetty as its embedded web server. To get familiar with TLS/SSL, along with related concepts like keys and certificates, read Configuring SSL/TLS in the Jetty documentation. To get more in-depth knowledge of TLS/SSL support in Java in general, refer to the Java Secure Socket Extension (JSSE) Reference Guide. The Configuring the Jetty SslContextFactory section can help in understanding TLS/SSL configurations listed below. Finally, Java Cryptography Architecture Standard Algorithm Name Documentation for JDK 8 lists all possible values for the configs belong, among others provided by Java implementation. Property Description Default Required druid.server.https.keyStorePath The file path or URL of the TLS/SSL Key store. none yes druid.server.https.keyStoreType The type of the key store. none yes druid.server.https.certAlias Alias of TLS/SSL certificate for the connector. none yes druid.server.https.keyStorePassword The Password Provider or String password for the Key Store. none yes The following table contains configuration options related to client certificate authentication. Property Description Default Required druid.server.https.requireClientCertificate If set to true, clients must identify themselves by providing a TLS certificate, without which connections will fail. false no druid.server.https.requestClientCertificate If set to true, clients may optionally identify themselves by providing a TLS certificate. Connections will not fail if TLS certificate is not provided. This property is ignored if requireClientCertificate is set to true. If requireClientCertificate and requestClientCertificate are false, the rest of the options in this table are ignored. false no druid.server.https.trustStoreType The type of the trust store containing certificates used to validate client certificates. Not needed if requireClientCertificate and requestClientCertificate are false. java.security.KeyStore.getDefaultType() no druid.server.https.trustStorePath The file path or URL of the trust store containing certificates used to validate client certificates. Not needed if requireClientCertificate and requestClientCertificate are false. none yes, only if requireClientCertificate is true druid.server.https.trustStoreAlgorithm Algorithm to be used by TrustManager to validate client certificate chains. Not needed if requireClientCertificate and requestClientCertificate are false. javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm() no druid.server.https.trustStorePassword The password provider or String password for the Trust Store. Not needed if requireClientCertificate and requestClientCertificate are false. none no druid.server.https.validateHostnames If set to true, check that the client's hostname matches the CN/subjectAltNames in the client certificate. Not used if requireClientCertificate and requestClientCertificate are false. true no druid.server.https.crlPath Specifies a path to a file containing static Certificate Revocation Lists, used to check if a client certificate has been revoked. Not used if requireClientCertificate and requestClientCertificate are false. null no The following table contains non-mandatory advanced configuration options, use caution. Property Description Default Required druid.server.https.keyManagerFactoryAlgorithm Algorithm to use for creating KeyManager, more details here. javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm() no druid.server.https.keyManagerPassword The Password Provider or String password for the Key Manager. none no druid.server.https.includeCipherSuites List of cipher suite names to include. You can either use the exact cipher suite name or a regular expression. Jetty's default include cipher list no druid.server.https.excludeCipherSuites List of cipher suite names to exclude. You can either use the exact cipher suite name or a regular expression. Jetty's default exclude cipher list no druid.server.https.includeProtocols List of exact protocols names to include. Jetty's default include protocol list no druid.server.https.excludeProtocols List of exact protocols names to exclude. Jetty's default exclude protocol list no Internal communication over TLS Whenever possible Druid processes will use HTTPS to talk to each other. To enable this communication Druid's HttpClient needs to be configured with a proper SSLContext that is able to validate the Server Certificates, otherwise communication will fail. Since, there are various ways to configure SSLContext, by default, Druid looks for an instance of SSLContext Guice binding while creating the HttpClient. This binding can be achieved writing a Druid extension which can provide an instance of SSLContext. Druid comes with a simple extension present here which should be useful enough for most simple cases, see this for how to include extensions. If this extension does not satisfy the requirements then please follow the extension implementation to create your own extension. When Druid Coordinator/Overlord have both HTTP and HTTPS enabled and Client sends request to non-leader process, then Client is always redirected to the HTTPS endpoint on leader process. So, Clients should be first upgraded to be able to handle redirect to HTTPS. Then Druid Overlord/Coordinator should be upgraded and configured to run both HTTP and HTTPS ports. Then Client configuration should be changed to refer to Druid Coordinator/Overlord via the HTTPS endpoint and then HTTP port on Druid Coordinator/Overlord should be disabled. Custom certificate checks Druid supports custom certificate check extensions. Please refer to the org.apache.druid.server.security.TLSCertificateChecker interface for details on the methods to be implemented. To use a custom TLS certificate checker, specify the following property: Property Description Default Required druid.tls.certificateChecker Type name of custom TLS certificate checker, provided by extensions. Please refer to extension documentation for the type name that should be specified. \"default\" no The default checker delegates to the standard trust manager and performs no additional actions or checks. If using a non-default certificate checker, please refer to the extension documentation for additional configuration properties needed. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/basic-cluster-tuning.html":{"url":"operations/basic-cluster-tuning.html","title":"集群调优","keywords":"","body":" This document provides basic guidelines for configuration properties and cluster architecture considerations related to performance tuning of an Apache Druid deployment. Please note that this document provides general guidelines and rules-of-thumb: these are not absolute, universal rules for cluster tuning, and this introductory guide is not an exhaustive description of all Druid tuning properties, which are described in the configuration reference. If you have questions on tuning Druid for specific use cases, or questions on configuration properties not covered in this guide, please ask the Druid user mailing list or other community channels. Process-specific guidelines Historical Heap sizing The biggest contributions to heap usage on Historicals are: Partial unmerged query results from segments The stored maps for lookups. A general rule-of-thumb for sizing the Historical heap is (0.5GB * number of CPU cores), with an upper limit of ~24GB. This rule-of-thumb scales using the number of CPU cores as a convenient proxy for hardware size and level of concurrency (note: this formula is not a hard rule for sizing Historical heaps). Having a heap that is too large can result in excessively long GC collection pauses, the ~24GB upper limit is imposed to avoid this. If caching is enabled on Historicals, the cache is stored on heap, sized by druid.cache.sizeInBytes. Running out of heap on the Historicals can indicate misconfiguration or usage patterns that are overloading the cluster. Lookups If you are using lookups, calculate the total size of the lookup maps being loaded. Druid performs an atomic swap when updating lookup maps (both the old map and the new map will exist in heap during the swap), so the maximum potential heap usage from lookup maps will be (2 * total size of all loaded lookups). Be sure to add (2 * total size of all loaded lookups) to your heap size in addition to the (0.5GB * number of CPU cores) guideline. Processing Threads and Buffers Please see the General Guidelines for Processing Threads and Buffers section for an overview of processing thread/buffer configuration. On Historicals: druid.processing.numThreads should generally be set to (number of cores - 1): a smaller value can result in CPU underutilization, while going over the number of cores can result in unnecessary CPU contention. druid.processing.buffer.sizeBytes can be set to 500MB. druid.processing.numMergeBuffers, a 1:4 ratio of merge buffers to processing threads is a reasonable choice for general use. Direct Memory Sizing The processing and merge buffers described above are direct memory buffers. When a historical processes a query, it must open a set of segments for reading. This also requires some direct memory space, described in segment decompression buffers. A formula for estimating direct memory usage follows: (druid.processing.numThreads + druid.processing.numMergeBuffers + 1) * druid.processing.buffer.sizeBytes The + 1 factor is a fuzzy estimate meant to account for the segment decompression buffers. Connection pool sizing Please see the General Connection Pool Guidelines section for an overview of connection pool configuration. For Historicals, druid.server.http.numThreads should be set to a value slightly higher than the sum of druid.broker.http.numConnections across all the Brokers in the cluster. Tuning the cluster so that each Historical can accept 50 queries and 10 non-queries is a reasonable starting point. Segment Cache Size druid.segmentCache.locations specifies locations where segment data can be stored on the Historical. The sum of available disk space across these locations is set as the default value for property: druid.server.maxSize, which controls the total size of segment data that can be assigned by the Coordinator to a Historical. Segments are memory-mapped by Historical processes using any available free system memory (i.e., memory not used by the Historical JVM and heap/direct memory buffers or other processes on the system). Segments that are not currently in memory will be paged from disk when queried. Therefore, the size of cache locations set within druid.segmentCache.locations should be such that a Historical is not allocated an excessive amount of segment data. As the value of (free system memory / total size of all druid.segmentCache.locations) increases, a greater proportion of segments can be kept in memory, allowing for better query performance. The total segment data size assigned to a Historical can be overridden with druid.server.maxSize, but this is not required for most of the use cases. Number of Historicals The number of Historicals needed in a cluster depends on how much data the cluster has. For good performance, you will want enough Historicals such that each Historical has a good (free system memory / total size of all druid.segmentCache.locations) ratio, as described in the segment cache size section above. Having a smaller number of big servers is generally better than having a large number of small servers, as long as you have enough fault tolerance for your use case. SSD storage We recommend using SSDs for storage on the Historicals, as they handle segment data stored on disk. Total memory usage To estimate total memory usage of the Historical under these guidelines: Heap: (0.5GB * number of CPU cores) + (2 * total size of lookup maps) + druid.cache.sizeInBytes Direct Memory: (druid.processing.numThreads + druid.processing.numMergeBuffers + 1) * druid.processing.buffer.sizeBytes The Historical will use any available free system memory (i.e., memory not used by the Historical JVM and heap/direct memory buffers or other processes on the system) for memory-mapping of segments on disk. For better query performance, you will want to ensure a good (free system memory / total size of all druid.segmentCache.locations) ratio so that a greater proportion of segments can be kept in memory. Segment sizes matter Be sure to check out segment size optimization to help tune your Historical processes for maximum performance. Broker Heap sizing The biggest contributions to heap usage on Brokers are: Partial unmerged query results from Historicals and Tasks The segment timeline: this consists of location information (which Historical/Task is serving a segment) for all currently available segments. Cached segment metadata: this consists of metadata, such as per-segment schemas, for all currently available segments. The Broker heap requirements scale based on the number of segments in the cluster, and the total data size of the segments. The heap size will vary based on data size and usage patterns, but 4G to 8G is a good starting point for a small or medium cluster (~15 servers or less). For a rough estimate of memory requirements on the high end, very large clusters with a node count on the order of ~100 nodes may need Broker heaps of 30GB-60GB. If caching is enabled on the Broker, the cache is stored on heap, sized by druid.cache.sizeInBytes. Direct memory sizing On the Broker, the amount of direct memory needed depends on how many merge buffers (used for merging GroupBys) are configured. The Broker does not generally need processing threads or processing buffers, as query results are merged on-heap in the HTTP connection threads instead. druid.processing.buffer.sizeBytes can be set to 500MB. druid.processing.numThreads: set this to 1 (the minimum allowed) druid.processing.numMergeBuffers: set this to the same value as on Historicals or a bit higher Connection pool sizing Please see the General Connection Pool Guidelines section for an overview of connection pool configuration. On the Brokers, please ensure that the sum of druid.broker.http.numConnections across all the Brokers is slightly lower than the value of druid.server.http.numThreads on your Historicals and Tasks. druid.server.http.numThreads on the Broker should be set to a value slightly higher than druid.broker.http.numConnections on the same Broker. Tuning the cluster so that each Historical can accept 50 queries and 10 non-queries, adjusting the Brokers accordingly, is a reasonable starting point. Broker backpressure When retrieving query results from Historical processes or Tasks, the Broker can optionally specify a maximum buffer size for queued, unread data, and exert backpressure on the channel to the Historical or Tasks when limit is reached (causing writes to the channel to block on the Historical/Task side until the Broker is able to drain some data from the channel). This buffer size is controlled by the druid.broker.http.maxQueuedBytes setting. The limit is divided across the number of Historicals/Tasks that a query would hit: suppose I have druid.broker.http.maxQueuedBytes set to 5MB, and the Broker receives a query that needs to be fanned out to 2 Historicals. Each per-historical channel would get a 2.5MB buffer in this case. You can generally set this to a value of approximately 2MB * number of Historicals. As your cluster scales up with more Historicals and Tasks, consider increasing this buffer size and increasing the Broker heap accordingly. If the buffer is too small, this can lead to inefficient queries due to the buffer filling up rapidly and stalling the channel If the buffer is too large, this puts more memory pressure on the Broker due to more queued result data in the HTTP channels. Number of brokers A 1:15 ratio of Brokers to Historicals is a reasonable starting point (this is not a hard rule). If you need Broker HA, you can deploy 2 initially and then use the 1:15 ratio guideline for additional Brokers. Total memory usage To estimate total memory usage of the Broker under these guidelines: Heap: allocated heap size Direct Memory: (druid.processing.numThreads + druid.processing.numMergeBuffers + 1) * druid.processing.buffer.sizeBytes MiddleManager The MiddleManager is a lightweight task controller/manager that launches Task processes, which perform ingestion work. MiddleManager heap sizing The MiddleManager itself does not require much resources, you can set the heap to ~128MB generally. SSD storage We recommend using SSDs for storage on the MiddleManagers, as the Tasks launched by MiddleManagers handle segment data stored on disk. Task Count The number of tasks a MiddleManager can launch is controlled by the druid.worker.capacity setting. The number of workers needed in your cluster depends on how many concurrent ingestion tasks you need to run for your use cases. The number of workers that can be launched on a given machine depends on the size of resources allocated per worker and available system resources. You can allocate more MiddleManager machines to your cluster to add task capacity. Task configurations The following section below describes configuration for Tasks launched by the MiddleManager. The Tasks can be queried and perform ingestion workloads, so they require more resources than the MM. Task heap sizing A 1GB heap is usually enough for Tasks. Lookups If you are using lookups, calculate the total size of the lookup maps being loaded. Druid performs an atomic swap when updating lookup maps (both the old map and the new map will exist in heap during the swap), so the maximum potential heap usage from lookup maps will be (2 * total size of all loaded lookups). Be sure to add (2 * total size of all loaded lookups) to your Task heap size if you are using lookups. Task processing threads and buffers For Tasks, 1 or 2 processing threads are often enough, as the Tasks tend to hold much less queryable data than Historical processes. druid.indexer.fork.property.druid.processing.numThreads: set this to 1 or 2 druid.indexer.fork.property.druid.processing.numMergeBuffers: set this to 2 druid.indexer.fork.property.druid.processing.buffer.sizeBytes: can be set to 100MB Direct memory sizing The processing and merge buffers described above are direct memory buffers. When a Task processes a query, it must open a set of segments for reading. This also requires some direct memory space, described in segment decompression buffers. An ingestion Task also needs to merge partial ingestion results, which requires direct memory space, described in segment merging. A formula for estimating direct memory usage follows: (druid.processing.numThreads + druid.processing.numMergeBuffers + 1) * druid.processing.buffer.sizeBytes The + 1 factor is a fuzzy estimate meant to account for the segment decompression buffers and dictionary merging buffers. Connection pool sizing Please see the General Connection Pool Guidelines section for an overview of connection pool configuration. For Tasks, druid.server.http.numThreads should be set to a value slightly higher than the sum of druid.broker.http.numConnections across all the Brokers in the cluster. Tuning the cluster so that each Task can accept 50 queries and 10 non-queries is a reasonable starting point. Total memory usage To estimate total memory usage of a Task under these guidelines: Heap: 1GB + (2 * total size of lookup maps) Direct Memory: (druid.processing.numThreads + druid.processing.numMergeBuffers + 1) * druid.processing.buffer.sizeBytes The total memory usage of the MiddleManager + Tasks: MM heap size + druid.worker.capacity * (single task memory usage) Configuration guidelines for specific ingestion types Kafka/Kinesis ingestion If you use the Kafka Indexing Service or Kinesis Indexing Service, the number of tasks required will depend on the number of partitions and your taskCount/replica settings. On top of those requirements, allocating more task slots in your cluster is a good idea, so that you have free task slots available for other tasks, such as compaction tasks. Hadoop ingestion If you are only using Hadoop-based batch ingestion with no other ingestion types, you can lower the amount of resources allocated per Task. Batch ingestion tasks do not need to answer queries, and the bulk of the ingestion workload will be executed on the Hadoop cluster, so the Tasks do not require much resources. Parallel native ingestion If you are using parallel native batch ingestion, allocating more available task slots is a good idea and will allow greater ingestion concurrency. Coordinator The main performance-related setting on the Coordinator is the heap size. The heap requirements of the Coordinator scale with the number of servers, segments, and tasks in the cluster. You can set the Coordinator heap to the same size as your Broker heap, or slightly smaller: both services have to process cluster-wide state and answer API requests about this state. Dynamic Configuration percentOfSegmentsToConsiderPerMove The default value is 100. This means that the Coordinator will consider all segments when it is looking for a segment to move. The Coordinator makes a weighted choice, with segments on Servers with the least capacity being the most likely segments to be moved. This weighted selection strategy means that the segments on the servers who have the most available capacity are the least likely to be chosen. As the number of segments in the cluster increases, the probability of choosing the Nth segment to move decreases; where N is the last segment considered for moving. An admin can use this config to skip consideration of that Nth segment. Instead of skipping a precise amount of segments, we skip a percentage of segments in the cluster. For example, with the value set to 25, only the first 25% of segments will be considered as a segment that can be moved. This 25% of segments will come from the servers that have the least available capacity. In this example, each time the Coordinator looks for a segment to move, it will consider 75% less segments than it did when the configuration was 100. On clusters with hundreds of thousands of segments, this can add up to meaningful coordination time savings. General recommendations for this configuration: If you are not worried about the amount of time it takes your Coordinator to complete a full coordination cycle, you likely do not need to modify this config. If you are frustrated with how long the Coordinator takes to run a full coordination cycle, and you have set the Coordinator dynamic config maxSegmentsToMove to a value above 0 (the default is 5), setting this config to a non-default value can help shorten coordination time. The recommended starting point value is 66. It represents a meaningful decrease in the percentage of segments considered while also not being too aggressive (You will consider 1/3 fewer segments per move operation with this value). The impact that modifying this config will have on your coordination time will be a function of how low you set the config value, the value for maxSegmentsToMove and the total number of segments in your cluster. If your cluster has a relatively small number of segments, or you choose to move few segments per coordination cycle, there may not be much savings to be had here. Overlord The main performance-related setting on the Overlord is the heap size. The heap requirements of the Overlord scale primarily with the number of running Tasks. The Overlord tends to require less resources than the Coordinator or Broker. You can generally set the Overlord heap to a value that's 25-50% of your Coordinator heap. Router The Router has light resource requirements, as it proxies requests to Brokers without performing much computational work itself. You can assign it 256MB heap as a starting point, growing it if needed. Guidelines for processing threads and buffers Processing threads The druid.processing.numThreads configuration controls the size of the processing thread pool used for computing query results. The size of this pool limits how many queries can be concurrently processed. Processing buffers druid.processing.buffer.sizeBytes is a closely related property that controls the size of the off-heap buffers allocated to the processing threads. One buffer is allocated for each processing thread. A size between 500MB and 1GB is a reasonable choice for general use. The TopN and GroupBy queries use these buffers to store intermediate computed results. As the buffer size increases, more data can be processed in a single pass. GroupBy merging buffers If you plan to issue GroupBy V2 queries, druid.processing.numMergeBuffers is an important configuration property. GroupBy V2 queries use an additional pool of off-heap buffers for merging query results. These buffers have the same size as the processing buffers described above, set by the druid.processing.buffer.sizeBytes property. Non-nested GroupBy V2 queries require 1 merge buffer per query, while a nested GroupBy V2 query requires 2 merge buffers (regardless of the depth of nesting). The number of merge buffers determines the number of GroupBy V2 queries that can be processed concurrently. Connection pool guidelines Each Druid process has a configuration property for the number of HTTP connection handling threads, druid.server.http.numThreads. The number of HTTP server threads limits how many concurrent HTTP API requests a given process can handle. Sizing the connection pool for queries The Broker has a setting druid.broker.http.numConnections that controls how many outgoing connections it can make to a given Historical or Task process. These connections are used to send queries to the Historicals or Tasks, with one connection per query; the value of druid.broker.http.numConnections is effectively a limit on the number of concurrent queries that a given broker can process. Suppose we have a cluster with 3 Brokers and druid.broker.http.numConnections is set to 10. This means that each Broker in the cluster will open up to 10 connections to each individual Historical or Task (for a total of 30 incoming query connections per Historical/Task). On the Historical/Task side, this means that druid.server.http.numThreads must be set to a value at least as high as the sum of druid.broker.http.numConnections across all the Brokers in the cluster. In practice, you will want to allocate additional server threads for non-query API requests such as status checks; adding 10 threads for those is a good general guideline. Using the example with 3 Brokers in the cluster and druid.broker.http.numConnections set to 10, a value of 40 would be appropriate for druid.server.http.numThreads on Historicals and Tasks. As a starting point, allowing for 50 concurrent queries (requests that read segment data from datasources) + 10 non-query requests (other requests like status checks) on Historicals and Tasks is reasonable (i.e., set druid.server.http.numThreads to 60 there), while sizing druid.broker.http.numConnections based on the number of Brokers in the cluster to fit within the 50 query connection limit per Historical/Task. If the connection pool across Brokers and Historicals/Tasks is too small, the cluster will be underutilized as there are too few concurrent query slots. If the connection pool is too large, you may get out-of-memory errors due to excessive concurrent load, and increased resource contention. The connection pool sizing matters most when you require QoS-type guarantees and use query priorities; otherwise, these settings can be more loosely configured. If your cluster usage patterns are heavily biased towards a high number of small concurrent queries (where each query takes less than ~15ms), enlarging the connection pool can be a good idea. The 50/10 general guideline here is a rough starting point, since different queries impose different amounts of load on the system. To size the connection pool more exactly for your cluster, you would need to know the execution times for your queries and ensure that the rate of incoming queries does not exceed your \"drain\" rate. Per-segment direct memory buffers Segment decompression When opening a segment for reading during segment merging or query processing, Druid allocates a 64KB off-heap decompression buffer for each column being read. Thus, there is additional direct memory overhead of (64KB number of columns read per segment number of segments read) when reading segments. Segment merging In addition to the segment decompression overhead described above, when a set of segments are merged during ingestion, a direct buffer is allocated for every String typed column, for every segment in the set to be merged. The size of these buffers are equal to the cardinality of the String column within its segment, times 4 bytes (the buffers store integers). For example, if two segments are being merged, the first segment having a single String column with cardinality 1000, and the second segment having a String column with cardinality 500, the merge step would allocate (1000 + 500) * 4 = 6000 bytes of direct memory. These buffers are used for merging the value dictionaries of the String column across segments. These \"dictionary merging buffers\" are independent of the \"merge buffers\" configured by druid.processing.numMergeBuffers. General recommendations JVM tuning Garbage Collection We recommend using the G1GC garbage collector: -XX:+UseG1GC Enabling process termination on out-of-memory errors is useful as well, since the process generally will not recover from such a state, and it's better to restart the process: -XX:+ExitOnOutOfMemoryError Other useful JVM flags -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.io.tmpdir= -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Dorg.jboss.logging.provider=slf4j -Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.SLF4JLogger -Dlog4j.shutdownCallbackRegistry=org.apache.druid.common.config.Log4jShutdown -Dlog4j.shutdownHookEnabled=true -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -Xloggc:/var/logs/druid/historical.gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=50 -XX:GCLogFileSize=10m -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/logs/druid/historical.hprof -XX:MaxDirectMemorySize=1g Please note that the flag settings above represent sample, general guidelines only. Be careful to use values appropriate for your specific scenario and be sure to test any changes in staging environments. ExitOnOutOfMemoryError flag is only supported starting JDK 8u92 . For older versions, -XX:OnOutOfMemoryError='kill -9 %p' can be used. MaxDirectMemorySize restricts JVM from allocating more than specified limit, by setting it to unlimited JVM restriction is lifted and OS level memory limits would still be effective. It's still important to make sure that Druid is not configured to allocate more off-heap memory than your machine has available. Important settings here include druid.processing.numThreads, druid.processing.numMergeBuffers, and druid.processing.buffer.sizeBytes. Additionally, for large JVM heaps, here are a few Garbage Collection efficiency guidelines that have been known to help in some cases. Mount /tmp on tmpfs ( See http://www.evanjones.ca/jvm-mmap-pause.html ) On Disk-IO intensive processes (e.g. Historical and MiddleManager), GC and Druid logs should be written to a different disk than where data is written. Disable Transparent Huge Pages ( See https://blogs.oracle.com/linux/performance-issues-with-transparent-huge-pages-thp ) Try disabling biased locking by using -XX:-UseBiasedLocking JVM flag. ( See https://dzone.com/articles/logging-stop-world-pauses-jvm ) Use UTC timezone We recommend using UTC timezone for all your events and across your hosts, not just for Druid, but for all data infrastructure. This can greatly mitigate potential query problems with inconsistent timezones. To query in a non-UTC timezone see query granularities System configuration SSDs SSDs are highly recommended for Historical, MiddleManager, and Indexer processes if you are not running a cluster that is entirely in memory. SSDs can greatly mitigate the time required to page data in and out of memory. JBOD vs RAID Historical processes store large number of segments on Disk and support specifying multiple paths for storing those. Typically, hosts have multiple disks configured with RAID which makes them look like a single disk to OS. RAID might have overheads specially if its not hardware controller based but software based. So, Historicals might get improved disk throughput with JBOD. Swap space We recommend not using swap space for Historical, MiddleManager, and Indexer processes since due to the large number of memory mapped segment files can lead to poor and unpredictable performance. Linux limits For Historical, MiddleManager, and Indexer processes (and for really large clusters, Broker processes), you might need to adjust some Linux system limits to account for a large number of open files, a large number of network connections, or a large number of memory mapped files. ulimit The limit on the number of open files can be set permanently by editing /etc/security/limits.conf. This value should be substantially greater than the number of segment files that will exist on the server. max_map_count Historical processes and to a lesser extent, MiddleManager and Indexer processes memory map segment files, so depending on the number of segments per server, /proc/sys/vm/max_map_count might also need to be adjusted. Depending on the variant of Linux, this might be done via sysctl by placing a file in /etc/sysctl.d/ that sets vm.max_map_count. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/segment-optimization.html":{"url":"operations/segment-optimization.html","title":"数据段文件优化","keywords":"","body":" In Apache Druid, it's important to optimize the segment size because Druid stores data in segments. If you're using the best-effort roll-up mode, increasing the segment size might introduce further aggregation which reduces the dataSource size. When a query is submitted, that query is distributed to all Historicals and realtime tasks which hold the input segments of the query. Each process and task picks a thread from its own processing thread pool to process a single segment. If segment sizes are too large, data might not be well distributed between data servers, decreasing the degree of parallelism possible during query processing. At the other extreme where segment sizes are too small, the scheduling overhead of processing a larger number of segments per query can reduce performance, as the threads that process each segment compete for the fixed slots of the processing pool. It would be best if you can optimize the segment size at ingestion time, but sometimes it's not easy especially when it comes to stream ingestion because the amount of data ingested might vary over time. In this case, you can create segments with a sub-optimized size first and optimize them later. You may need to consider the followings to optimize your segments. Number of rows per segment: it's generally recommended for each segment to have around 5 million rows. This setting is usually more important than the below \"segment byte size\". This is because Druid uses a single thread to process each segment, and thus this setting can directly control how many rows each thread processes, which in turn means how well the query execution is parallelized. Segment byte size: it's recommended to set 300 ~ 700MB. If this value doesn't match with the \"number of rows per segment\", please consider optimizing number of rows per segment rather than this value. The above recommendation works in general, but the optimal setting can vary based on your workload. For example, if most of your queries are heavy and take a long time to process each row, you may want to make segments smaller so that the query processing can be more parallelized. If you still see some performance issue after optimizing segment size, you may need to find the optimal settings for your workload. There might be several ways to check if the compaction is necessary. One way is using the System Schema. The system schema provides several tables about the current system status including the segments table. By running the below query, you can get the average number of rows and average size for published segments. SELECT \"start\", \"end\", version, COUNT(*) AS num_segments, AVG(\"num_rows\") AS avg_num_rows, SUM(\"num_rows\") AS total_num_rows, AVG(\"size\") AS avg_size, SUM(\"size\") AS total_size FROM sys.segments A WHERE datasource = 'your_dataSource' AND is_published = 1 GROUP BY 1, 2, 3 ORDER BY 1, 2, 3 DESC; Please note that the query result might include overshadowed segments. In this case, you may want to see only rows of the max version per interval (pair of start and end). Once you find your segments need compaction, you can consider the below two options: Turning on the automatic compaction of Coordinators. The Coordinator periodically submits compaction tasks to re-index small segments. To enable the automatic compaction, you need to configure it for each dataSource via Coordinator's dynamic configuration. See Compaction Configuration API and Compaction Configuration for details. Running periodic Hadoop batch ingestion jobs and using a dataSource inputSpec to read from the segments generated by the Kafka indexing tasks. This might be helpful if you want to compact a lot of segments in parallel. Details on how to do this can be found on the Updating existing data section of the data management page. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/http-compression.html":{"url":"operations/http-compression.html","title":"HTTP压缩","keywords":"","body":" Apache Druid supports http request decompression and response compression, to use this, http request header Content-Encoding:gzip and Accept-Encoding:gzip is needed to be set. Property Description Default druid.server.http.compressionLevel The compression level. Value should be between [-1,9], -1 for default level, 0 for no compression. -1 (default compression level) druid.server.http.inflateBufferSize The buffer size used by gzip decoder. Set to 0 to disable request decompression. 4096 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/api-reference.html":{"url":"operations/api-reference.html","title":"API手册","keywords":"","body":" This page documents all of the API endpoints for each Druid service type. Common The following endpoints are supported by all processes. Process information GET /status Returns the Druid version, loaded extensions, memory used, total memory and other useful information about the process. /status/health An endpoint that always returns a boolean \"true\" value with a 200 OK response, useful for automated health checks. /status/properties Returns the current configuration properties of the process. /status/selfDiscovered/status Returns a JSON map of the form {\"selfDiscovered\": true/false}, indicating whether the node has received a confirmation from the central node discovery mechanism (currently ZooKeeper) of the Druid cluster that the node has been added to the cluster. It is recommended to not consider a Druid node \"healthy\" or \"ready\" in automated deployment/container management systems until it returns {\"selfDiscovered\": true} from this endpoint. This is because a node may be isolated from the rest of the cluster due to network issues and it doesn't make sense to consider nodes \"healthy\" in this case. Also, when nodes such as Brokers use ZooKeeper segment discovery for building their view of the Druid cluster (as opposed to HTTP segment discovery), they may be unusable until the ZooKeeper client is fully initialized and starts to receive data from the ZooKeeper cluster. {\"selfDiscovered\": true} is a proxy event indicating that the ZooKeeper client on the node has started to receive data from the ZooKeeper cluster and it's expected that all segments and other nodes will be discovered by this node timely from this point. /status/selfDiscovered Similar to /status/selfDiscovered/status, but returns 200 OK response with empty body if the node has discovered itself and 503 SERVICE UNAVAILABLE if the node hasn't discovered itself yet. This endpoint might be useful because some monitoring checks such as AWS load balancer health checks are not able to look at the response body. Master Server This section documents the API endpoints for the processes that reside on Master servers (Coordinators and Overlords) in the suggested three-server configuration. Coordinator Leadership GET /druid/coordinator/v1/leader Returns the current leader Coordinator of the cluster. /druid/coordinator/v1/isLeader Returns a JSON object with field \"leader\", either true or false, indicating if this server is the current leader Coordinator of the cluster. In addition, returns HTTP 200 if the server is the current leader and HTTP 404 if not. This is suitable for use as a load balancer status check if you only want the active leader to be considered in-service at the load balancer. Segment Loading GET /druid/coordinator/v1/loadstatus Returns the percentage of segments actually loaded in the cluster versus segments that should be loaded in the cluster. /druid/coordinator/v1/loadstatus?simple Returns the number of segments left to load until segments that should be loaded in the cluster are available for queries. This does not include segment replication counts. /druid/coordinator/v1/loadstatus?full Returns the number of segments left to load in each tier until segments that should be loaded in the cluster are all available. This includes segment replication counts. /druid/coordinator/v1/loadqueue Returns the ids of segments to load and drop for each Historical process. /druid/coordinator/v1/loadqueue?simple Returns the number of segments to load and drop, as well as the total segment load and drop size in bytes for each Historical process. /druid/coordinator/v1/loadqueue?full Returns the serialized JSON of segments to load and drop for each Historical process. Segment Loading by Datasource Note that all interval query parameters are ISO 8601 strings (e.g., 2016-06-27/2016-06-28). Also note that these APIs only guarantees that the segments are available at the time of the call. Segments can still become missing because of historical process failures or any other reasons afterward. GET /druid/coordinator/v1/datasources/{dataSourceName}/loadstatus?forceMetadataRefresh={boolean}&interval={myInterval} Returns the percentage of segments actually loaded in the cluster versus segments that should be loaded in the cluster for the given datasource over the given interval (or last 2 weeks if interval is not given). forceMetadataRefresh is required to be set. Setting forceMetadataRefresh to true will force the coordinator to poll latest segment metadata from the metadata store (Note: forceMetadataRefresh=true refreshes Coordinator's metadata cache of all datasources. This can be a heavy operation in terms of the load on the metadata store but can be necessary to make sure that we verify all the latest segments' load status) Setting forceMetadataRefresh to false will use the metadata cached on the coordinator from the last force/periodic refresh. If no used segments are found for the given inputs, this API returns 204 No Content /druid/coordinator/v1/datasources/{dataSourceName}/loadstatus?simple&forceMetadataRefresh={boolean}&interval={myInterval} Returns the number of segments left to load until segments that should be loaded in the cluster are available for the given datasource over the given interval (or last 2 weeks if interval is not given). This does not include segment replication counts. forceMetadataRefresh is required to be set. Setting forceMetadataRefresh to true will force the coordinator to poll latest segment metadata from the metadata store (Note: forceMetadataRefresh=true refreshes Coordinator's metadata cache of all datasources. This can be a heavy operation in terms of the load on the metadata store but can be necessary to make sure that we verify all the latest segments' load status) Setting forceMetadataRefresh to false will use the metadata cached on the coordinator from the last force/periodic refresh. If no used segments are found for the given inputs, this API returns 204 No Content /druid/coordinator/v1/datasources/{dataSourceName}/loadstatus?full&forceMetadataRefresh={boolean}&interval={myInterval} Returns the number of segments left to load in each tier until segments that should be loaded in the cluster are all available for the given datasource over the given interval (or last 2 weeks if interval is not given). This includes segment replication counts. forceMetadataRefresh is required to be set. Setting forceMetadataRefresh to true will force the coordinator to poll latest segment metadata from the metadata store (Note: forceMetadataRefresh=true refreshes Coordinator's metadata cache of all datasources. This can be a heavy operation in terms of the load on the metadata store but can be necessary to make sure that we verify all the latest segments' load status) Setting forceMetadataRefresh to false will use the metadata cached on the coordinator from the last force/periodic refresh. If no used segments are found for the given inputs, this API returns 204 No Content Metadata store information GET /druid/coordinator/v1/metadata/datasources Returns a list of the names of data sources with at least one used segment in the cluster. /druid/coordinator/v1/metadata/datasources?includeUnused Returns a list of the names of data sources, regardless of whether there are used segments belonging to those data sources in the cluster or not. /druid/coordinator/v1/metadata/datasources?full Returns a list of all data sources with at least one used segment in the cluster. Returns all metadata about those data sources as stored in the metadata store. /druid/coordinator/v1/metadata/datasources/{dataSourceName} Returns full metadata for a datasource as stored in the metadata store. /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments Returns a list of all segments for a datasource as stored in the metadata store. /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments?full Returns a list of all segments for a datasource with the full segment metadata as stored in the metadata store. /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments/{segmentId} Returns full segment metadata for a specific segment as stored in the metadata store, if the segment is used. If the segment is unused, or is unknown, a 404 response is returned. POST /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments Returns a list of all segments, overlapping with any of given intervals, for a datasource as stored in the metadata store. Request body is array of string IS0 8601 intervals like [interval1, interval2,...] for example [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"] /druid/coordinator/v1/metadata/datasources/{dataSourceName}/segments?full Returns a list of all segments, overlapping with any of given intervals, for a datasource with the full segment metadata as stored in the metadata store. Request body is array of string ISO 8601 intervals like [interval1, interval2,...] for example [\"2012-01-01T00:00:00.000/2012-01-03T00:00:00.000\", \"2012-01-05T00:00:00.000/2012-01-07T00:00:00.000\"] Datasources Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/coordinator/v1/datasources Returns a list of datasource names found in the cluster. /druid/coordinator/v1/datasources?simple Returns a list of JSON objects containing the name and properties of datasources found in the cluster. Properties include segment count, total segment byte size, replicated total segment byte size, minTime, and maxTime. /druid/coordinator/v1/datasources?full Returns a list of datasource names found in the cluster with all metadata about those datasources. /druid/coordinator/v1/datasources/{dataSourceName} Returns a JSON object containing the name and properties of a datasource. Properties include segment count, total segment byte size, replicated total segment byte size, minTime, and maxTime. /druid/coordinator/v1/datasources/{dataSourceName}?full Returns full metadata for a datasource . /druid/coordinator/v1/datasources/{dataSourceName}/intervals Returns a set of segment intervals. /druid/coordinator/v1/datasources/{dataSourceName}/intervals?simple Returns a map of an interval to a JSON object containing the total byte size of segments and number of segments for that interval. /druid/coordinator/v1/datasources/{dataSourceName}/intervals?full Returns a map of an interval to a map of segment metadata to a set of server names that contain the segment for that interval. /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval} Returns a set of segment ids for an interval. /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}?simple Returns a map of segment intervals contained within the specified interval to a JSON object containing the total byte size of segments and number of segments for an interval. /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}?full Returns a map of segment intervals contained within the specified interval to a map of segment metadata to a set of server names that contain the segment for an interval. /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval}/serverview Returns a map of segment intervals contained within the specified interval to information about the servers that contain the segment for an interval. /druid/coordinator/v1/datasources/{dataSourceName}/segments Returns a list of all segments for a datasource in the cluster. /druid/coordinator/v1/datasources/{dataSourceName}/segments?full Returns a list of all segments for a datasource in the cluster with the full segment metadata. /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId} Returns full segment metadata for a specific segment in the cluster. /druid/coordinator/v1/datasources/{dataSourceName}/tiers Return the tiers that a datasource exists in. Note for coordinator's POST and DELETE API's The segments would be enabled when these API's are called, but then can be disabled again by the coordinator if any dropRule matches. Segments enabled by these API's might not be loaded by historical processes if no loadRule matches. If an indexing or kill task runs at the same time as these API's are invoked, the behavior is undefined. Some segments might be killed and others might be enabled. It's also possible that all segments might be disabled but at the same time, the indexing task is able to read data from those segments and succeed. Caution : Avoid using indexing or kill tasks and these API's at the same time for the same datasource and time chunk. (It's fine if the time chunks or datasource don't overlap) POST /druid/coordinator/v1/datasources/{dataSourceName} Marks as used all segments belonging to a data source. Returns a JSON object of the form {\"numChangedSegments\": } with the number of segments in the database whose state has been changed (that is, the segments were marked as used) as the result of this API call. /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId} Marks as used a segment of a data source. Returns a JSON object of the form {\"segmentStateChanged\": } with the boolean indicating if the state of the segment has been changed (that is, the segment was marked as used) as the result of this API call. /druid/coordinator/v1/datasources/{dataSourceName}/markUsed /druid/coordinator/v1/datasources/{dataSourceName}/markUnused Marks segments (un)used for a datasource by interval or set of segment Ids. When marking used only segments that are not overshadowed will be updated. The request payload contains the interval or set of segment Ids to be marked unused. Either interval or segment ids should be provided, if both or none are provided in the payload, the API would throw an error (400 BAD REQUEST). Interval specifies the start and end times as IS0 8601 strings. interval=(start/end) where start and end both are inclusive and only the segments completely contained within the specified interval will be disabled, partially overlapping segments will not be affected. JSON Request Payload: Key Description Example interval The interval for which to mark segments unused \"2015-09-12T03:00:00.000Z/2015-09-12T05:00:00.000Z\" segmentIds Set of segment Ids to be marked unused [\"segmentId1\", \"segmentId2\"] DELETE /druid/coordinator/v1/datasources/{dataSourceName} Marks as unused all segments belonging to a data source. Returns a JSON object of the form {\"numChangedSegments\": } with the number of segments in the database whose state has been changed (that is, the segments were marked as unused) as the result of this API call. /druid/coordinator/v1/datasources/{dataSourceName}/intervals/{interval} @Deprecated. /druid/coordinator/v1/datasources/{dataSourceName}?kill=true&interval={myInterval} Runs a Kill task for a given interval and datasource. /druid/coordinator/v1/datasources/{dataSourceName}/segments/{segmentId} Marks as unused a segment of a data source. Returns a JSON object of the form {\"segmentStateChanged\": } with the boolean indicating if the state of the segment has been changed (that is, the segment was marked as unused) as the result of this API call. Retention Rules Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/coordinator/v1/rules Returns all rules as JSON objects for all datasources in the cluster including the default datasource. /druid/coordinator/v1/rules/{dataSourceName} Returns all rules for a specified datasource. /druid/coordinator/v1/rules/{dataSourceName}?full Returns all rules for a specified datasource and includes default datasource. /druid/coordinator/v1/rules/history?interval= Returns audit history of rules for all datasources. default value of interval can be specified by setting druid.audit.manager.auditHistoryMillis (1 week if not configured) in Coordinator runtime.properties /druid/coordinator/v1/rules/history?count= Returns last entries of audit history of rules for all datasources. /druid/coordinator/v1/rules/{dataSourceName}/history?interval= Returns audit history of rules for a specified datasource. default value of interval can be specified by setting druid.audit.manager.auditHistoryMillis (1 week if not configured) in Coordinator runtime.properties /druid/coordinator/v1/rules/{dataSourceName}/history?count= Returns last entries of audit history of rules for a specified datasource. POST /druid/coordinator/v1/rules/{dataSourceName} POST with a list of rules in JSON form to update rules. Optional Header Parameters for auditing the config change can also be specified. Header Param Name Description Default X-Druid-Author author making the config change \"\" X-Druid-Comment comment describing the change being done \"\" Intervals Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/coordinator/v1/intervals Returns all intervals for all datasources with total size and count. /druid/coordinator/v1/intervals/{interval} Returns aggregated total size and count for all intervals that intersect given isointerval. /druid/coordinator/v1/intervals/{interval}?simple Returns total size and count for each interval within given isointerval. /druid/coordinator/v1/intervals/{interval}?full Returns total size and count for each datasource for each interval within given isointerval. Compaction Status GET /druid/coordinator/v1/compaction/progress?dataSource={dataSource} Returns the total size of segments awaiting compaction for the given dataSource. This is only valid for dataSource which has compaction enabled. GET /druid/coordinator/v1/compaction/status Returns the status and statistics from the latest auto compaction run of all dataSources which have/had auto compaction enabled. The response payload includes a list of latestStatus objects. Each latestStatus represents the status for a dataSource (which has/had auto compaction enabled). The latestStatus object has the following keys: dataSource: name of the datasource for this status information scheduleStatus: auto compaction scheduling status. Possible values are NOT_ENABLED and RUNNING. Returns RUNNING if the dataSource has an active auto compaction config submitted otherwise, NOT_ENABLED bytesAwaitingCompaction: total bytes of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction) bytesCompacted: total bytes of this datasource that are already compacted with the spec set in the auto compaction config. bytesSkipped: total bytes of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. segmentCountAwaitingCompaction: total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction) segmentCountCompacted: total number of segments of this datasource that are already compacted with the spec set in the auto compaction config. segmentCountSkipped: total number of segments of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. intervalCountAwaitingCompaction: total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction) intervalCountCompacted: total number of intervals of this datasource that are already compacted with the spec set in the auto compaction config. intervalCountSkipped: total number of intervals of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. GET /druid/coordinator/v1/compaction/status?dataSource={dataSource} Similar to the API /druid/coordinator/v1/compaction/status above but filters response to only return information for the {dataSource} given. Note that {dataSource} given must have/had auto compaction enabled. Compaction Configuration GET /druid/coordinator/v1/config/compaction Returns all compaction configs. /druid/coordinator/v1/config/compaction/{dataSource} Returns a compaction config of a dataSource. POST /druid/coordinator/v1/config/compaction/taskslots?ratio={someRatio}&max={someMaxSlots} Update the capacity for compaction tasks. ratio and max are used to limit the max number of compaction tasks. They mean the ratio of the total task slots to the compaction task slots and the maximum number of task slots for compaction tasks, respectively. The actual max number of compaction tasks is min(max, ratio * total task slots). Note that ratio and max are optional and can be omitted. If they are omitted, default values (0.1 and unbounded) will be set for them. /druid/coordinator/v1/config/compaction Creates or updates the compaction config for a dataSource. See Compaction Configuration for configuration details. DELETE /druid/coordinator/v1/config/compaction/{dataSource} Removes the compaction config for a dataSource. Server information GET /druid/coordinator/v1/servers Returns a list of servers URLs using the format {hostname}:{port}. Note that processes that run with different types will appear multiple times with different ports. /druid/coordinator/v1/servers?simple Returns a list of server data objects in which each object has the following keys: host: host URL include ({hostname}:{port}) type: process type (indexer-executor, historical) currSize: storage size currently used maxSize: maximum storage size priority tier Overlord Leadership GET /druid/indexer/v1/leader Returns the current leader Overlord of the cluster. If you have multiple Overlords, just one is leading at any given time. The others are on standby. /druid/indexer/v1/isLeader This returns a JSON object with field \"leader\", either true or false. In addition, this call returns HTTP 200 if the server is the current leader and HTTP 404 if not. This is suitable for use as a load balancer status check if you only want the active leader to be considered in-service at the load balancer. Tasks Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/indexer/v1/tasks Retrieve list of tasks. Accepts query string parameters state, datasource, createdTimeInterval, max, and type. Query Parameter Description state filter list of tasks by task state, valid options are running, complete, waiting, and pending. datasource return tasks filtered by Druid datasource. createdTimeInterval return tasks created within the specified interval. max maximum number of \"complete\" tasks to return. Only applies when state is set to \"complete\". type filter tasks by task type. See task documentation for more details. /druid/indexer/v1/completeTasks Retrieve list of complete tasks. Equivalent to /druid/indexer/v1/tasks?state=complete. /druid/indexer/v1/runningTasks Retrieve list of running tasks. Equivalent to /druid/indexer/v1/tasks?state=running. /druid/indexer/v1/waitingTasks Retrieve list of waiting tasks. Equivalent to /druid/indexer/v1/tasks?state=waiting. /druid/indexer/v1/pendingTasks Retrieve list of pending tasks. Equivalent to /druid/indexer/v1/tasks?state=pending. /druid/indexer/v1/task/{taskId} Retrieve the 'payload' of a task. /druid/indexer/v1/task/{taskId}/status Retrieve the status of a task. /druid/indexer/v1/task/{taskId}/segments Retrieve information about the segments of a task. This API is deprecated and will be removed in future releases. /druid/indexer/v1/task/{taskId}/reports Retrieve a task completion report for a task. Only works for completed tasks. POST /druid/indexer/v1/task Endpoint for submitting tasks and supervisor specs to the Overlord. Returns the taskId of the submitted task. /druid/indexer/v1/task/{taskId}/shutdown Shuts down a task. /druid/indexer/v1/datasources/{dataSource}/shutdownAllTasks Shuts down all tasks for a dataSource. /druid/indexer/v1/taskStatus Retrieve list of task status objects for list of task id strings in request body. DELETE /druid/indexer/v1/pendingSegments/{dataSource} Manually clean up pending segments table in metadata storage for datasource. Returns a JSON object response with numDeleted and count of rows deleted from the pending segments table. This API is used by the druid.coordinator.kill.pendingSegments.on coordinator setting which automates this operation to perform periodically. Supervisors GET /druid/indexer/v1/supervisor Returns a list of strings of the currently active supervisor ids. /druid/indexer/v1/supervisor?full Returns a list of objects of the currently active supervisors. Field Type Description id String supervisor unique identifier state String basic state of the supervisor. Available states:UNHEALTHY_SUPERVISOR, UNHEALTHY_TASKS, PENDING, RUNNING, SUSPENDED, STOPPING. Check Kafka Docs for details. detailedState String supervisor specific state. (See documentation of specific supervisor for details), e.g. Kafka or Kinesis) healthy Boolean true or false indicator of overall supervisor health spec SupervisorSpec json specification of supervisor (See Supervisor Configuration for details) /druid/indexer/v1/supervisor?state=true Returns a list of objects of the currently active supervisors and their current state. Field Type Description id String supervisor unique identifier state String basic state of the supervisor. Available states: UNHEALTHY_SUPERVISOR, UNHEALTHY_TASKS, PENDING, RUNNING, SUSPENDED, STOPPING. Check Kafka Docs for details. detailedState String supervisor specific state. (See documentation of the specific supervisor for details, e.g. Kafka or Kinesis) healthy Boolean true or false indicator of overall supervisor health suspended Boolean true or false indicator of whether the supervisor is in suspended state /druid/indexer/v1/supervisor/ Returns the current spec for the supervisor with the provided ID. /druid/indexer/v1/supervisor//status Returns the current status of the supervisor with the provided ID. /druid/indexer/v1/supervisor/history Returns an audit history of specs for all supervisors (current and past). /druid/indexer/v1/supervisor//history Returns an audit history of specs for the supervisor with the provided ID. POST /druid/indexer/v1/supervisor Create a new supervisor or update an existing one. /druid/indexer/v1/supervisor//suspend Suspend the current running supervisor of the provided ID. Responds with updated SupervisorSpec. /druid/indexer/v1/supervisor/suspendAll Suspend all supervisors at once. /druid/indexer/v1/supervisor//resume Resume indexing tasks for a supervisor. Responds with updated SupervisorSpec. /druid/indexer/v1/supervisor/resumeAll Resume all supervisors at once. /druid/indexer/v1/supervisor//reset Reset the specified supervisor. /druid/indexer/v1/supervisor//terminate Terminate a supervisor of the provided ID. /druid/indexer/v1/supervisor/terminateAll Terminate all supervisors at once. /druid/indexer/v1/supervisor//shutdown Shutdown a supervisor. This API is deprecated and will be removed in future releases. Please use the equivalent 'terminate' instead. Dynamic configuration See Overlord Dynamic Configuration for details. Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/indexer/v1/worker Retrieves current overlord dynamic configuration. /druid/indexer/v1/worker/history?interval={interval}&counter={count} Retrieves history of changes to overlord dynamic configuration. Accepts interval and count query string parameters to filter by interval and limit the number of results respectively. /druid/indexer/v1/workers Retrieves a list of all the worker nodes in the cluster along with its metadata. /druid/indexer/v1/scaling Retrieves overlord scaling events if auto-scaling runners are in use. POST /druid/indexer/v1/worker Update overlord dynamic worker configuration. Data Server This section documents the API endpoints for the processes that reside on Data servers (MiddleManagers/Peons and Historicals) in the suggested three-server configuration. MiddleManager GET /druid/worker/v1/enabled Check whether a MiddleManager is in an enabled or disabled state. Returns JSON object keyed by the combined druid.host and druid.port with the boolean state as the value. {\"localhost:8091\":true} /druid/worker/v1/tasks Retrieve a list of active tasks being run on MiddleManager. Returns JSON list of taskid strings. Normal usage should prefer to use the /druid/indexer/v1/tasks Overlord API or one of it's task state specific variants instead. [\"index_wikiticker_2019-02-11T02:20:15.316Z\"] /druid/worker/v1/task/{taskid}/log Retrieve task log output stream by task id. Normal usage should prefer to use the /druid/indexer/v1/task/{taskId}/log Overlord API instead. POST /druid/worker/v1/disable 'Disable' a MiddleManager, causing it to stop accepting new tasks but complete all existing tasks. Returns JSON object keyed by the combined druid.host and druid.port: {\"localhost:8091\":\"disabled\"} /druid/worker/v1/enable 'Enable' a MiddleManager, allowing it to accept new tasks again if it was previously disabled. Returns JSON object keyed by the combined druid.host and druid.port: {\"localhost:8091\":\"enabled\"} /druid/worker/v1/task/{taskid}/shutdown Shutdown a running task by taskid. Normal usage should prefer to use the /druid/indexer/v1/task/{taskId}/shutdown Overlord API instead. Returns JSON: {\"task\":\"index_kafka_wikiticker_f7011f8ffba384b_fpeclode\"} Peon GET /druid/worker/v1/chat/{taskId}/rowStats Retrieve a live row stats report from a Peon. See task reports for more details. /druid/worker/v1/chat/{taskId}/unparseableEvents Retrieve an unparseable events report from a Peon. See task reports for more details. Historical Segment Loading GET /druid/historical/v1/loadstatus Returns JSON of the form {\"cacheInitialized\":}, where value is either true or false indicating if all segments in the local cache have been loaded. This can be used to know when a Historical process is ready to be queried after a restart. /druid/historical/v1/readiness Similar to /druid/historical/v1/loadstatus, but instead of returning JSON with a flag, responses 200 OK if segments in the local cache have been loaded, and 503 SERVICE UNAVAILABLE, if they haven't. Query Server This section documents the API endpoints for the processes that reside on Query servers (Brokers) in the suggested three-server configuration. Broker Datasource Information Note that all interval URL parameters are ISO 8601 strings delimited by a _ instead of a / (e.g., 2016-06-27_2016-06-28). GET /druid/v2/datasources Returns a list of queryable datasources. /druid/v2/datasources/{dataSourceName} Returns the dimensions and metrics of the datasource. Optionally, you can provide request parameter \"full\" to get list of served intervals with dimensions and metrics being served for those intervals. You can also provide request param \"interval\" explicitly to refer to a particular interval. If no interval is specified, a default interval spanning a configurable period before the current time will be used. The default duration of this interval is specified in ISO 8601 duration format via: druid.query.segmentMetadata.defaultHistory /druid/v2/datasources/{dataSourceName}/dimensions Returns the dimensions of the datasource. This API is deprecated and will be removed in future releases. Please use SegmentMetadataQuery instead which provides more comprehensive information and supports all dataSource types including streaming dataSources. It's also encouraged to use INFORMATION_SCHEMA tables if you're using SQL. /druid/v2/datasources/{dataSourceName}/metrics Returns the metrics of the datasource. This API is deprecated and will be removed in future releases. Please use SegmentMetadataQuery instead which provides more comprehensive information and supports all dataSource types including streaming dataSources. It's also encouraged to use INFORMATION_SCHEMA tables if you're using SQL. /druid/v2/datasources/{dataSourceName}/candidates?intervals={comma-separated-intervals}&numCandidates={numCandidates} Returns segment information lists including server locations for the given datasource and intervals. If \"numCandidates\" is not specified, it will return all servers for each interval. Load Status GET /druid/broker/v1/loadstatus Returns a flag indicating if the Broker knows about all segments in the cluster. This can be used to know when a Broker process is ready to be queried after a restart. /druid/broker/v1/readiness Similar to /druid/broker/v1/loadstatus, but instead of returning a JSON, responses 200 OK if its ready and otherwise 503 SERVICE UNAVAILABLE. Queries POST /druid/v2/ The endpoint for submitting queries. Accepts an option ?pretty that pretty prints the results. /druid/v2/candidates/ Returns segment information lists including server locations for the given query.. Router GET /druid/v2/datasources Returns a list of queryable datasources. /druid/v2/datasources/{dataSourceName} Returns the dimensions and metrics of the datasource. /druid/v2/datasources/{dataSourceName}/dimensions Returns the dimensions of the datasource. /druid/v2/datasources/{dataSourceName}/metrics Returns the metrics of the datasource. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/high-availability.html":{"url":"operations/high-availability.html","title":"高可用","keywords":"","body":" Apache ZooKeeper, metadata store, the coordinator, the overlord, and brokers are recommended to set up a high availability environment. For highly-available ZooKeeper, you will need a cluster of 3 or 5 ZooKeeper nodes. We recommend either installing ZooKeeper on its own hardware, or running 3 or 5 Master servers (where overlords or coordinators are running) and configuring ZooKeeper on them appropriately. See the ZooKeeper admin guide for more details. For highly-available metadata storage, we recommend MySQL or PostgreSQL with replication and failover enabled. See MySQL HA/Scalability Guide and PostgreSQL's High Availability, Load Balancing, and Replication for MySQL and PostgreSQL, respectively. For highly-available Apache Druid Coordinators and Overlords, we recommend to run multiple servers. If they are all configured to use the same ZooKeeper cluster and metadata storage, then they will automatically failover between each other as necessary. Only one will be active at a time, but inactive servers will redirect to the currently active server. Druid Brokers can be scaled out and all running servers will be active and queryable. We recommend placing them behind a load balancer. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/rolling-updates.html":{"url":"operations/rolling-updates.html","title":"版本滚动升级","keywords":"","body":" For rolling Apache Druid cluster updates with no downtime, we recommend updating Druid processes in the following order: Historical *Overlord (if any) *Middle Manager/Indexers (if any) Standalone Real-time (if any) Broker Coordinator ( or merged Coordinator+Overlord ) * In 0.12.0, there are protocol changes between the Kafka supervisor and Kafka Indexing task and also some changes to the metadata formats persisted on disk. Therefore, to support rolling upgrade, all the Middle Managers will need to be upgraded first before the Overlord. Note that this ordering is different from the standard order of upgrade, also note that this ordering is only necessary when using the Kafka Indexing Service. If one is not using Kafka Indexing Service or can handle down time for Kafka Supervisor then one can upgrade in any order. Historical Historical processes can be updated one at a time. Each Historical process has a startup time to memory map all the segments it was serving before the update. The startup time typically takes a few seconds to a few minutes, depending on the hardware of the host. As long as each Historical process is updated with a sufficient delay (greater than the time required to start a single process), you can rolling update the entire Historical cluster. Overlord Overlord processes can be updated one at a time in a rolling fashion. Middle Managers/Indexers Middle Managers or Indexer nodes run both batch and real-time indexing tasks. Generally you want to update Middle Managers in such a way that real-time indexing tasks do not fail. There are three strategies for doing that. Rolling restart (restore-based) Middle Managers can be updated one at a time in a rolling fashion when you set druid.indexer.task.restoreTasksOnRestart=true. In this case, indexing tasks that support restoring will restore their state on Middle Manager restart, and will not fail. Currently, only realtime tasks support restoring, so non-realtime indexing tasks will fail and will need to be resubmitted. Rolling restart (graceful-termination-based) Middle Managers can be gracefully terminated using the \"disable\" API. This works for all task types, even tasks that are not restorable. To prepare a Middle Manager for update, send a POST request to /druid/worker/v1/disable. The Overlord will now no longer send tasks to this Middle Manager. Tasks that have already started will run to completion. Current state can be checked using /druid/worker/v1/enabled . To view all existing tasks, send a GET request to /druid/worker/v1/tasks. When this list is empty, you can safely update the Middle Manager. After the Middle Manager starts back up, it is automatically enabled again. You can also manually enable Middle Managers by POSTing to /druid/worker/v1/enable. Autoscaling-based replacement If autoscaling is enabled on your Overlord, then Overlord processes can launch new Middle Manager processes en masse and then gracefully terminate old ones as their tasks finish. This process is configured by setting druid.indexer.runner.minWorkerVersion=#{VERSION}. Each time you update your Overlord process, the VERSION value should be increased, which will trigger a mass launch of new Middle Managers. The config druid.indexer.autoscale.workerVersion=#{VERSION} also needs to be set. Standalone Real-time Standalone real-time processes can be updated one at a time in a rolling fashion. Broker Broker processes can be updated one at a time in a rolling fashion. There needs to be some delay between updating each process as Brokers must load the entire state of the cluster before they return valid results. Coordinator Coordinator processes can be updated one at a time in a rolling fashion. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/rule-configuration.html":{"url":"operations/rule-configuration.html","title":"数据留存规则","keywords":"","body":" In Apache Druid, Coordinator processes use rules to determine what data should be loaded to or dropped from the cluster. Rules are used for data retention and query execution, and are set on the Coordinator console (http://coordinator_ip:port). There are three types of rules, i.e., load rules, drop rules, and broadcast rules. Load rules indicate how segments should be assigned to different historical process tiers and how many replicas of a segment should exist in each tier. Drop rules indicate when segments should be dropped entirely from the cluster. Finally, broadcast rules indicate how segments of different datasources should be co-located in Historical processes. The Coordinator loads a set of rules from the metadata storage. Rules may be specific to a certain datasource and/or a default set of rules can be configured. Rules are read in order and hence the ordering of rules is important. The Coordinator will cycle through all used segments and match each segment with the first rule that applies. Each segment may only match a single rule. Note: It is recommended that the Coordinator console is used to configure rules. However, the Coordinator process does have HTTP endpoints to programmatically configure rules. Load rules Load rules indicate how many replicas of a segment should exist in a server tier. Please note: If a Load rule is used to retain only data from a certain interval or period, it must be accompanied by a Drop rule. If a Drop rule is not included, data not within the specified interval or period will be retained by the default rule (loadForever). Forever Load Rule Forever load rules are of the form: { \"type\" : \"loadForever\", \"tieredReplicants\": { \"hot\": 1, \"_default_tier\" : 1 } } type - this should always be \"loadForever\" tieredReplicants - A JSON Object where the keys are the tier names and values are the number of replicas for that tier. Interval Load Rule Interval load rules are of the form: { \"type\" : \"loadByInterval\", \"interval\": \"2012-01-01/2013-01-01\", \"tieredReplicants\": { \"hot\": 1, \"_default_tier\" : 1 } } type - this should always be \"loadByInterval\" interval - A JSON Object representing ISO-8601 Intervals tieredReplicants - A JSON Object where the keys are the tier names and values are the number of replicas for that tier. Period Load Rule Period load rules are of the form: { \"type\" : \"loadByPeriod\", \"period\" : \"P1M\", \"includeFuture\" : true, \"tieredReplicants\": { \"hot\": 1, \"_default_tier\" : 1 } } type - this should always be \"loadByPeriod\" period - A JSON Object representing ISO-8601 Periods includeFuture - A JSON Boolean indicating whether the load period should include the future. This property is optional, Default is true. tieredReplicants - A JSON Object where the keys are the tier names and values are the number of replicas for that tier. The interval of a segment will be compared against the specified period. The period is from some time in the past to the future or to the current time, which depends on includeFuture is true or false. The rule matches if the period overlaps the interval. Drop Rules Drop rules indicate when segments should be dropped from the cluster. Forever Drop Rule Forever drop rules are of the form: { \"type\" : \"dropForever\" } type - this should always be \"dropForever\" All segments that match this rule are dropped from the cluster. Interval Drop Rule Interval drop rules are of the form: { \"type\" : \"dropByInterval\", \"interval\" : \"2012-01-01/2013-01-01\" } type - this should always be \"dropByInterval\" interval - A JSON Object representing ISO-8601 Periods A segment is dropped if the interval contains the interval of the segment. Period Drop Rule Period drop rules are of the form: { \"type\" : \"dropByPeriod\", \"period\" : \"P1M\", \"includeFuture\" : true } type - this should always be \"dropByPeriod\" period - A JSON Object representing ISO-8601 Periods includeFuture - A JSON Boolean indicating whether the load period should include the future. This property is optional, Default is true. The interval of a segment will be compared against the specified period. The period is from some time in the past to the future or to the current time, which depends on includeFuture is true or false. The rule matches if the period contains the interval. This drop rule always dropping recent data. Period Drop Before Rule Period drop before rules are of the form: { \"type\" : \"dropBeforeByPeriod\", \"period\" : \"P1M\" } type - this should always be \"dropBeforeByPeriod\" period - A JSON Object representing ISO-8601 Periods The interval of a segment will be compared against the specified period. The period is from some time in the past to the current time. The rule matches if the interval before the period. If you just want to retain recent data, you can use this rule to drop the old data that before a specified period and add a loadForever rule to follow it. Notes, dropBeforeByPeriod + loadForever is equivalent to loadByPeriod(includeFuture = true) + dropForever. Broadcast Rules Broadcast rules indicate that segments of a data source should be loaded by all servers of a cluster of the following types: historicals, brokers, tasks, and indexers. Note that the broadcast segments are only directly queryable through the historicals, but they are currently loaded on other server types to support join queries. Forever Broadcast Rule Forever broadcast rules are of the form: { \"type\" : \"broadcastForever\" } type - this should always be \"broadcastForever\" This rule applies to all segments of a datasource, covering all intervals. Interval Broadcast Rule Interval broadcast rules are of the form: { \"type\" : \"broadcastByInterval\", \"interval\" : \"2012-01-01/2013-01-01\" } type - this should always be \"broadcastByInterval\" interval - A JSON Object representing ISO-8601 Periods. Only the segments of the interval will be broadcasted. Period Broadcast Rule Period broadcast rules are of the form: { \"type\" : \"broadcastByPeriod\", \"period\" : \"P1M\", \"includeFuture\" : true } type - this should always be \"broadcastByPeriod\" period - A JSON Object representing ISO-8601 Periods includeFuture - A JSON Boolean indicating whether the load period should include the future. This property is optional, Default is true. The interval of a segment will be compared against the specified period. The period is from some time in the past to the future or to the current time, which depends on includeFuture is true or false. The rule matches if the period overlaps the interval. Permanently deleting data Druid can fully drop data from the cluster, wipe the metadata store entry, and remove the data from deep storage for any segments that are marked as unused (segments dropped from the cluster via rules are always marked as unused). You can submit a kill task to the Overlord to do this. Reloading dropped data Data that has been dropped from a Druid cluster cannot be reloaded using only rules. To reload dropped data in Druid, you must first set your retention period (i.e. changing the retention period from 1 month to 2 months), and then mark as used all segments belonging to the datasource in the Druid Coordinator console, or through the Druid Coordinator endpoints. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/metrics.html":{"url":"operations/metrics.html","title":"应用指标","keywords":"","body":" Druid generates metrics related to queries, ingestion, and coordination. Metrics are emitted as JSON objects to a runtime log file or over HTTP (to a service such as Apache Kafka). Metric emission is disabled by default. All Druid metrics share a common set of fields: timestamp - the time the metric was created metric - the name of the metric service - the service name that emitted the metric host - the host name that emitted the metric value - some numeric value associated with the metric Metrics may have additional dimensions beyond those listed above. Most metric values reset each emission period. By default druid emission period is 1 minute, this can be changed by setting the property druid.monitoring.emissionPeriod. Available Metrics Query metrics Broker Metric Description Dimensions Normal Value query/time Milliseconds taken to complete a query. Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension. query/bytes number of bytes returned in query response. Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension. query/node/time Milliseconds taken to query individual historical/realtime processes. id, status, server. query/node/bytes number of bytes returned from querying individual historical/realtime processes. id, status, server. query/node/ttfb Time to first byte. Milliseconds elapsed until Broker starts receiving the response from individual historical/realtime processes. id, status, server. query/node/backpressure Milliseconds that the channel to this process has spent suspended due to backpressure. id, status, server. query/count number of total queries This metric is only available if the QueryCountStatsMonitor module is included. query/success/count number of queries successfully processed This metric is only available if the QueryCountStatsMonitor module is included. query/failed/count number of failed queries This metric is only available if the QueryCountStatsMonitor module is included. query/interrupted/count number of queries interrupted due to cancellation. This metric is only available if the QueryCountStatsMonitor module is included. query/timeout/count number of timed out queries. This metric is only available if the QueryCountStatsMonitor module is included. sqlQuery/time Milliseconds taken to complete a SQL query. id, nativeQueryIds, dataSource, remoteAddress, success. sqlQuery/bytes number of bytes returned in SQL query response. id, nativeQueryIds, dataSource, remoteAddress, success. Historical Metric Description Dimensions Normal Value query/time Milliseconds taken to complete a query. Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension. query/segment/time Milliseconds taken to query individual segment. Includes time to page in the segment from disk. id, status, segment. several hundred milliseconds query/wait/time Milliseconds spent waiting for a segment to be scanned. id, segment. segment/scan/pending Number of segments in queue waiting to be scanned. Close to 0 query/segmentAndCache/time Milliseconds taken to query individual segment or hit the cache (if it is enabled on the Historical process). id, segment. several hundred milliseconds query/cpu/time Microseconds of CPU time taken to complete a query Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension. Varies query/count number of total queries This metric is only available if the QueryCountStatsMonitor module is included. query/success/count number of queries successfully processed This metric is only available if the QueryCountStatsMonitor module is included. query/failed/count number of failed queries This metric is only available if the QueryCountStatsMonitor module is included. query/interrupted/count number of queries interrupted due to cancellation. This metric is only available if the QueryCountStatsMonitor module is included. query/timeout/count number of timed out queries. This metric is only available if the QueryCountStatsMonitor module is included. Real-time Metric Description Dimensions Normal Value query/time Milliseconds taken to complete a query. Common: dataSource, type, interval, hasFilters, duration, context, remoteAddress, id. Aggregation Queries: numMetrics, numComplexMetrics. GroupBy: numDimensions. TopN: threshold, dimension. query/wait/time Milliseconds spent waiting for a segment to be scanned. id, segment. several hundred milliseconds segment/scan/pending Number of segments in queue waiting to be scanned. Close to 0 query/count number of total queries This metric is only available if the QueryCountStatsMonitor module is included. query/success/count number of queries successfully processed This metric is only available if the QueryCountStatsMonitor module is included. query/failed/count number of failed queries This metric is only available if the QueryCountStatsMonitor module is included. query/interrupted/count number of queries interrupted due to cancellation. This metric is only available if the QueryCountStatsMonitor module is included. query/timeout/count number of timed out queries. This metric is only available if the QueryCountStatsMonitor module is included. Jetty Metric Description Normal Value jetty/numOpenConnections Number of open jetty connections. Not much higher than number of jetty threads. Cache Metric Description Normal Value query/cache/delta/* Cache metrics since the last emission. N/A query/cache/total/* Total cache metrics. N/A Metric Description Dimensions Normal Value */numEntries Number of cache entries. Varies. */sizeBytes Size in bytes of cache entries. Varies. */hits Number of cache hits. Varies. */misses Number of cache misses. Varies. */evictions Number of cache evictions. Varies. */hitRate Cache hit rate. ~40% */averageByte Average cache entry byte size. Varies. */timeouts Number of cache timeouts. 0 */errors Number of cache errors. 0 */put/ok Number of new cache entries successfully cached. Varies, but more than zero. */put/error Number of new cache entries that could not be cached due to errors. Varies, but more than zero. */put/oversized Number of potential new cache entries that were skipped due to being too large (based on druid.{broker,historical,realtime}.cache.maxEntrySize properties). Varies. Memcached only metrics Memcached client metrics are reported as per the following. These metrics come directly from the client as opposed to from the cache retrieval layer. Metric Description Dimensions Normal Value query/cache/memcached/total Cache metrics unique to memcached (only if druid.cache.type=memcached) as their actual values Variable N/A query/cache/memcached/delta Cache metrics unique to memcached (only if druid.cache.type=memcached) as their delta from the prior event emission Variable N/A SQL Metrics If SQL is enabled, the Broker will emit the following metrics for SQL. Metric Description Dimensions Normal Value sqlQuery/time Milliseconds taken to complete a SQL. id, nativeQueryIds, dataSource, remoteAddress, success. sqlQuery/bytes number of bytes returned in SQL response. id, nativeQueryIds, dataSource, remoteAddress, success. Ingestion Metrics (Kafka Indexing Service) These metrics are applicable for the Kafka Indexing Service. Metric Description Dimensions Normal Value ingest/kafka/lag Total lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute. dataSource. Greater than 0, should not be a very high number ingest/kafka/maxLag Max lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute. dataSource. Greater than 0, should not be a very high number ingest/kafka/avgLag Average lag between the offsets consumed by the Kafka indexing tasks and latest offsets in Kafka brokers across all partitions. Minimum emission period for this metric is a minute. dataSource. Greater than 0, should not be a very high number Ingestion Metrics (Kinesis Indexing Service) These metrics are applicable for the Kinesis Indexing Service. Metric Description Dimensions Normal Value ingest/kinesis/lag/time Total lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute. dataSource. Greater than 0, up to max Kinesis retention period in milliseconds ingest/kinesis/maxLag/time Max lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute. dataSource. Greater than 0, up to max Kinesis retention period in milliseconds ingest/kinesis/avgLag/time Average lag time in milliseconds between the current message sequence number consumed by the Kinesis indexing tasks and latest sequence number in Kinesis across all shards. Minimum emission period for this metric is a minute. dataSource. Greater than 0, up to max Kinesis retention period in milliseconds Ingestion metrics (Realtime process) These metrics are only available if the RealtimeMetricsMonitor is included in the monitors list for the Realtime process. These metrics are deltas for each emission period. Metric Description Dimensions Normal Value ingest/events/thrownAway Number of events rejected because they are outside the windowPeriod. dataSource, taskId, taskType. 0 ingest/events/unparseable Number of events rejected because the events are unparseable. dataSource, taskId, taskType. 0 ingest/events/duplicate Number of events rejected because the events are duplicated. dataSource, taskId, taskType. 0 ingest/events/processed Number of events successfully processed per emission period. dataSource, taskId, taskType. Equal to your # of events per emission period. ingest/rows/output Number of Druid rows persisted. dataSource, taskId, taskType. Your # of events with rollup. ingest/persists/count Number of times persist occurred. dataSource, taskId, taskType. Depends on configuration. ingest/persists/time Milliseconds spent doing intermediate persist. dataSource, taskId, taskType. Depends on configuration. Generally a few minutes at most. ingest/persists/cpu Cpu time in Nanoseconds spent on doing intermediate persist. dataSource, taskId, taskType. Depends on configuration. Generally a few minutes at most. ingest/persists/backPressure Milliseconds spent creating persist tasks and blocking waiting for them to finish. dataSource, taskId, taskType. 0 or very low ingest/persists/failed Number of persists that failed. dataSource, taskId, taskType. 0 ingest/handoff/failed Number of handoffs that failed. dataSource, taskId, taskType. 0 ingest/merge/time Milliseconds spent merging intermediate segments dataSource, taskId, taskType. Depends on configuration. Generally a few minutes at most. ingest/merge/cpu Cpu time in Nanoseconds spent on merging intermediate segments. dataSource, taskId, taskType. Depends on configuration. Generally a few minutes at most. ingest/handoff/count Number of handoffs that happened. dataSource, taskId, taskType. Varies. Generally greater than 0 once every segment granular period if cluster operating normally ingest/sink/count Number of sinks not handoffed. dataSource, taskId, taskType. 1~3 ingest/events/messageGap Time gap between the data time in event and current system time. dataSource, taskId, taskType. Greater than 0, depends on the time carried in event Note: If the JVM does not support CPU time measurement for the current thread, ingest/merge/cpu and ingest/persists/cpu will be 0. Indexing service Metric Description Dimensions Normal Value task/run/time Milliseconds taken to run a task. dataSource, taskId, taskType, taskStatus. Varies. task/action/log/time Milliseconds taken to log a task action to the audit log. dataSource, taskId, taskType task/action/run/time Milliseconds taken to execute a task action. dataSource, taskId, taskType Varies from subsecond to a few seconds, based on action type. segment/added/bytes Size in bytes of new segments created. dataSource, taskId, taskType, interval. Varies. segment/moved/bytes Size in bytes of segments moved/archived via the Move Task. dataSource, taskId, taskType, interval. Varies. segment/nuked/bytes Size in bytes of segments deleted via the Kill Task. dataSource, taskId, taskType, interval. Varies. task/success/count Number of successful tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included. dataSource. Varies. task/failed/count Number of failed tasks per emission period. This metric is only available if the TaskCountStatsMonitor module is included. dataSource. Varies. task/running/count Number of current running tasks. This metric is only available if the TaskCountStatsMonitor module is included. dataSource. Varies. task/pending/count Number of current pending tasks. This metric is only available if the TaskCountStatsMonitor module is included. dataSource. Varies. task/waiting/count Number of current waiting tasks. This metric is only available if the TaskCountStatsMonitor module is included. dataSource. Varies. taskSlot/total/count Number of total task slots per emission period. This metric is only available if the TaskSlotCountStatsMonitor module is included. Varies. taskSlot/idle/count Number of idle task slots per emission period. This metric is only available if the TaskSlotCountStatsMonitor module is included. Varies. taskSlot/used/count Number of busy task slots per emission period. This metric is only available if the TaskSlotCountStatsMonitor module is included. Varies. taskSlot/lazy/count Number of total task slots in lazy marked MiddleManagers and Indexers per emission period. This metric is only available if the TaskSlotCountStatsMonitor module is included. Varies. taskSlot/blacklisted/count Number of total task slots in blacklisted MiddleManagers and Indexers per emission period. This metric is only available if the TaskSlotCountStatsMonitor module is included. Varies. Shuffle metrics (Native parallel task) The shuffle metrics can be enabled by adding org.apache.druid.indexing.worker.shuffle.ShuffleMonitor in druid.monitoring.monitors See Enabling Metrics for more details. Metric Description Dimensions Normal Value ingest/shuffle/bytes Number of bytes shuffled per emission period. supervisorTaskId Varies ingest/shuffle/requests Number of shuffle requests per emission period. supervisorTaskId Varies Coordination These metrics are for the Druid Coordinator and are reset each time the Coordinator runs the coordination logic. Metric Description Dimensions Normal Value segment/assigned/count Number of segments assigned to be loaded in the cluster. tier. Varies. segment/moved/count Number of segments moved in the cluster. tier. Varies. segment/dropped/count Number of segments dropped due to being overshadowed. tier. Varies. segment/deleted/count Number of segments dropped due to rules. tier. Varies. segment/unneeded/count Number of segments dropped due to being marked as unused. tier. Varies. segment/cost/raw Used in cost balancing. The raw cost of hosting segments. tier. Varies. segment/cost/normalization Used in cost balancing. The normalization of hosting segments. tier. Varies. segment/cost/normalized Used in cost balancing. The normalized cost of hosting segments. tier. Varies. segment/loadQueue/size Size in bytes of segments to load. server. Varies. segment/loadQueue/failed Number of segments that failed to load. server. 0 segment/loadQueue/count Number of segments to load. server. Varies. segment/dropQueue/count Number of segments to drop. server. Varies. segment/size Total size of used segments in a data source. Emitted only for data sources to which at least one used segment belongs. dataSource. Varies. segment/count Number of used segments belonging to a data source. Emitted only for data sources to which at least one used segment belongs. dataSource. segment/overShadowed/count Number of overshadowed segments. Varies. segment/unavailable/count Number of segments (not including replicas) left to load until segments that should be loaded in the cluster are available for queries. dataSource. 0 segment/underReplicated/count Number of segments (including replicas) left to load until segments that should be loaded in the cluster are available for queries. tier, dataSource. 0 tier/historical/count Number of available historical nodes in each tier. tier. Varies. tier/replication/factor Configured maximum replication factor in each tier. tier. Varies. tier/required/capacity Total capacity in bytes required in each tier. tier. Varies. tier/total/capacity Total capacity in bytes available in each tier. tier. Varies. compact/task/count Number of tasks issued in the auto compaction run. Varies. compactTask/maxSlot/count Max number of task slots that can be used for auto compaction tasks in the auto compaction run. Varies. compactTask/availableSlot/count Number of available task slots that can be used for auto compaction tasks in the auto compaction run. (this is max slot minus any currently running compaction task) Varies. segment/waitCompact/bytes Total bytes of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction). datasource. Varies. segment/waitCompact/count Total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction). datasource. Varies. interval/waitCompact/count Total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that are eligible for auto compaction). datasource. Varies. segment/compacted/bytes Total bytes of this datasource that are already compacted with the spec set in the auto compaction config. datasource. Varies. segment/compacted/count Total number of segments of this datasource that are already compacted with the spec set in the auto compaction config. datasource. Varies. interval/compacted/count Total number of intervals of this datasource that are already compacted with the spec set in the auto compaction config. datasource. Varies. segment/skipCompact/bytes Total bytes of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. datasource. Varies. segment/skipCompact/count Total number of segments of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. datasource. Varies. interval/skipCompact/count Total number of intervals of this datasource that are skipped (not eligible for auto compaction) by the auto compaction. datasource. Varies. coordinator/time Approximate Coordinator duty runtime in milliseconds. The duty dimension is the string alias of the Duty that is being run. duty. Varies. coordinator/global/time Approximate runtime of a full coordination cycle in milliseconds. The dutyGroup dimension indicates what type of coordination this run was. i.e. Historical Management vs Indexing dutyGroup Varies. If emitBalancingStats is set to true in the Coordinator dynamic configuration, then log entries for class org.apache.druid.server.coordinator.duty.EmitClusterStatsAndMetrics will have extra information on balancing decisions. General Health Historical Metric Description Dimensions Normal Value segment/max Maximum byte limit available for segments. Varies. segment/used Bytes used for served segments. dataSource, tier, priority. segment/usedPercent Percentage of space used by served segments. dataSource, tier, priority. segment/count Number of served segments. dataSource, tier, priority. Varies. segment/pendingDelete On-disk size in bytes of segments that are waiting to be cleared out Varies. JVM These metrics are only available if the JVMMonitor module is included. Metric Description Dimensions Normal Value jvm/pool/committed Committed pool. poolKind, poolName. close to max pool jvm/pool/init Initial pool. poolKind, poolName. Varies. jvm/pool/max Max pool. poolKind, poolName. Varies. jvm/pool/used Pool used. poolKind, poolName. jvm/bufferpool/count Bufferpool count. bufferpoolName. Varies. jvm/bufferpool/used Bufferpool used. bufferpoolName. close to capacity jvm/bufferpool/capacity Bufferpool capacity. bufferpoolName. Varies. jvm/mem/init Initial memory. memKind. Varies. jvm/mem/max Max memory. memKind. Varies. jvm/mem/used Used memory. memKind. jvm/mem/committed Committed memory. memKind. close to max memory jvm/gc/count Garbage collection count. gcName (cms/g1/parallel/etc.), gcGen (old/young) Varies. jvm/gc/cpu Count of CPU time in Nanoseconds spent on garbage collection. Note: jvm/gc/cpu represents the total time over multiple GC cycles; divide by jvm/gc/count to get the mean GC time per cycle gcName, gcGen Sum of jvm/gc/cpu should be within 10-30% of sum of jvm/cpu/total, depending on the GC algorithm used (reported by JvmCpuMonitor) EventReceiverFirehose The following metric is only available if the EventReceiverFirehoseMonitor module is included. Metric Description Dimensions Normal Value ingest/events/buffered Number of events queued in the EventReceiverFirehose's buffer serviceName, dataSource, taskId, taskType, bufferCapacity. Equal to current # of events in the buffer queue. ingest/bytes/received Number of bytes received by the EventReceiverFirehose. serviceName, dataSource, taskId, taskType. Varies. Sys These metrics are only available if the SysMonitor module is included. Metric Description Dimensions Normal Value sys/swap/free Free swap. Varies. sys/swap/max Max swap. Varies. sys/swap/pageIn Paged in swap. Varies. sys/swap/pageOut Paged out swap. Varies. sys/disk/write/count Writes to disk. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. Varies. sys/disk/read/count Reads from disk. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. Varies. sys/disk/write/size Bytes written to disk. Can we used to determine how much paging is occurring with regards to segments. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. Varies. sys/disk/read/size Bytes read from disk. Can we used to determine how much paging is occurring with regards to segments. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. Varies. sys/net/write/size Bytes written to the network. netName, netAddress, netHwaddr Varies. sys/net/read/size Bytes read from the network. netName, netAddress, netHwaddr Varies. sys/fs/used Filesystem bytes used. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. sys/fs/max Filesystesm bytes max. fsDevName, fsDirName, fsTypeName, fsSysTypeName, fsOptions. Varies. sys/mem/used Memory used. sys/mem/max Memory max. Varies. sys/storage/used Disk space used. fsDirName. Varies. sys/cpu CPU used. cpuName, cpuTime. Varies. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/alerts.html":{"url":"operations/alerts.html","title":"告警","keywords":"","body":" Druid generates alerts on getting into unexpected situations. Alerts are emitted as JSON objects to a runtime log file or over HTTP (to a service such as Apache Kafka). Alert emission is disabled by default. All Druid alerts share a common set of fields: timestamp - the time the alert was created service - the service name that emitted the alert host - the host name that emitted the alert severity - severity of the alert e.g. anomaly, component-failure, service-failure etc. description - a description of the alert data - if there was an exception then a JSON object with fields exceptionType, exceptionMessage and exceptionStackTrace 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/other-hadoop.html":{"url":"operations/other-hadoop.html","title":"Hadoop相关","keywords":"","body":" Apache Druid can interact with Hadoop in two ways: Use HDFS for deep storage using the druid-hdfs-storage extension. Batch-load data from Hadoop using Map/Reduce jobs. These are not necessarily linked together; you can load data with Hadoop jobs into a non-HDFS deep storage (like S3), and you can use HDFS for deep storage even if you're loading data from streams rather than using Hadoop jobs. For best results, use these tips when configuring Druid to interact with your favorite Hadoop distribution. Tip #1: Place Hadoop XMLs on Druid classpath Place your Hadoop configuration XMLs (core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml) on the classpath of your Druid processes. You can do this by copying them into conf/druid/_common/core-site.xml, conf/druid/_common/hdfs-site.xml, and so on. This allows Druid to find your Hadoop cluster and properly submit jobs. Tip #2: Classloader modification on Hadoop (Map/Reduce jobs only) Druid uses a number of libraries that are also likely present on your Hadoop cluster, and if these libraries conflict, your Map/Reduce jobs can fail. This problem can be avoided by enabling classloader isolation using the Hadoop job property mapreduce.job.classloader = true. This instructs Hadoop to use a separate classloader for Druid dependencies and for Hadoop's own dependencies. If your version of Hadoop does not support this functionality, you can also try setting the property mapreduce.job.user.classpath.first = true. This instructs Hadoop to prefer loading Druid's version of a library when there is a conflict. Generally, you should only set one of these parameters, not both. These properties can be set in either one of the following ways: Using the task definition, e.g. add \"mapreduce.job.classloader\": \"true\" to the jobProperties of the tuningConfig of your indexing task (see the Hadoop batch ingestion documentation). Using system properties, e.g. on the MiddleManager set druid.indexer.runner.javaOpts=... -Dhadoop.mapreduce.job.classloader=true in Middle Manager configuration. Overriding specific classes When mapreduce.job.classloader = true, it is also possible to specifically define which classes should be loaded from the hadoop system classpath and which should be loaded from job-supplied JARs. This is controlled by defining class inclusion/exclusion patterns in the mapreduce.job.classloader.system.classes property in the jobProperties of tuningConfig. For example, some community members have reported version incompatibility errors with the Validator class: Error: java.lang.ClassNotFoundException: javax.validation.Validator The following jobProperties excludes javax.validation. classes from being loaded from the system classpath, while including those from java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.. \"jobProperties\": { \"mapreduce.job.classloader\": \"true\", \"mapreduce.job.classloader.system.classes\": \"-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.\" } mapred-default.xml documentation contains more information about this property. Tip #3: Use specific versions of Hadoop libraries Druid loads Hadoop client libraries from two different locations. Each set of libraries is loaded in an isolated classloader. HDFS deep storage uses jars from extensions/druid-hdfs-storage/ to read and write Druid data on HDFS. Batch ingestion uses jars from hadoop-dependencies/ to submit Map/Reduce jobs (location customizable via the druid.extensions.hadoopDependenciesDir runtime property; see Configuration). hadoop-client:2.8.5 is the default version of the Hadoop client bundled with Druid for both purposes. This works with many Hadoop distributions (the version does not necessarily need to match), but if you run into issues, you can instead have Druid load libraries that exactly match your distribution. To do this, either copy the jars from your Hadoop cluster, or use the pull-deps tool to download the jars from a Maven repository. Preferred: Load using Druid's standard mechanism If you have issues with HDFS deep storage, you can switch your Hadoop client libraries by recompiling the druid-hdfs-storage extension using an alternate version of the Hadoop client libraries. You can do this by editing the main Druid pom.xml and rebuilding the distribution by running mvn package. If you have issues with Map/Reduce jobs, you can switch your Hadoop client libraries without rebuilding Druid. You can do this by adding a new set of libraries to the hadoop-dependencies/ directory (or another directory specified by druid.extensions.hadoopDependenciesDir) and then using hadoopDependencyCoordinates in the Hadoop Index Task to specify the Hadoop dependencies you want Druid to load. Example: Suppose you specify druid.extensions.hadoopDependenciesDir=/usr/local/druid_tarball/hadoop-dependencies, and you have downloaded hadoop-client 2.3.0 and 2.4.0, either by copying them from your Hadoop cluster or by using pull-deps to download the jars from a Maven repository. Then underneath hadoop-dependencies, your jars should look like this: hadoop-dependencies/ └── hadoop-client ├── 2.3.0 │ ├── activation-1.1.jar │ ├── avro-1.7.4.jar │ ├── commons-beanutils-1.7.0.jar │ ├── commons-beanutils-core-1.8.0.jar │ ├── commons-cli-1.2.jar │ ├── commons-codec-1.4.jar ..... lots of jars └── 2.4.0 ├── activation-1.1.jar ├── avro-1.7.4.jar ├── commons-beanutils-1.7.0.jar ├── commons-beanutils-core-1.8.0.jar ├── commons-cli-1.2.jar ├── commons-codec-1.4.jar ..... lots of jars As you can see, under hadoop-client, there are two sub-directories, each denotes a version of hadoop-client. Next, use hadoopDependencyCoordinates in Hadoop Index Task to specify the Hadoop dependencies you want Druid to load. For example, in your Hadoop Index Task spec file, you can write: \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.4.0\"] This instructs Druid to load hadoop-client 2.4.0 when processing the task. What happens behind the scene is that Druid first looks for a folder called hadoop-client underneath druid.extensions.hadoopDependenciesDir, then looks for a folder called 2.4.0 underneath hadoop-client, and upon successfully locating these folders, hadoop-client 2.4.0 is loaded. Alternative: Append your Hadoop jars to the Druid classpath You can also load Hadoop client libraries in Druid's main classloader, rather than an isolated classloader. This mechanism is relatively easy to reason about, but it also means that you have to ensure that all dependency jars on the classpath are compatible. That is, Druid makes no provisions while using this method to maintain class loader isolation so you must make sure that the jars on your classpath are mutually compatible. Set druid.indexer.task.defaultHadoopCoordinates=[]. By setting this to an empty list, Druid will not load any other Hadoop dependencies except the ones specified in the classpath. Append your Hadoop jars to Druid's classpath. Druid will load them into the system. Notes on specific Hadoop distributions If the tips above do not solve any issues you are having with HDFS deep storage or Hadoop batch indexing, you may have luck with one of the following suggestions contributed by the Druid community. CDH Members of the community have reported dependency conflicts between the version of Jackson used in CDH and Druid when running a Mapreduce job like: java.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object; Preferred workaround First, try the tip under \"Classloader modification on Hadoop\" above. More recent versions of CDH have been reported to work with the classloader isolation option (mapreduce.job.classloader = true). Alternate workaround - 1 You can try editing Druid's pom.xml dependencies to match the version of Jackson in your Hadoop version and recompile Druid. For more about building Druid, please see Building Druid. Alternate workaround - 2 Another workaround solution is to build a custom fat jar of Druid using sbt, which manually excludes all the conflicting Jackson dependencies, and then put this fat jar in the classpath of the command that starts Overlord indexing service. To do this, please follow the following steps. (1) Download and install sbt. (2) Make a new directory named 'druid_build'. (3) Cd to 'druid_build' and create the build.sbt file with the content here. You can always add more building targets or remove the ones you don't need. (4) In the same directory create a new directory named 'project'. (5) Put the druid source code into 'druid_build/project'. (6) Create a file 'druid_build/project/assembly.sbt' with content as follows. addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\") (7) In the 'druid_build' directory, run 'sbt assembly'. (8) In the 'druid_build/target/scala-2.10' folder, you will find the fat jar you just build. (9) Make sure the jars you've uploaded has been completely removed. The HDFS directory is by default '/tmp/druid-indexing/classpath'. (10) Include the fat jar in the classpath when you start the indexing service. Make sure you've removed 'lib/*' from your classpath because now the fat jar includes all you need. Alternate workaround - 3 If sbt is not your choice, you can also use maven-shade-plugin to make a fat jar: relocation all Jackson packages will resolve it too. In this way, druid will not be affected by Jackson library embedded in hadoop. Please follow the steps below: (1) Add all extensions you needed to services/pom.xml like org.apache.druid.extensions druid-avro-extensions ${project.parent.version} org.apache.druid.extensions druid-parquet-extensions ${project.parent.version} org.apache.druid.extensions druid-hdfs-storage ${project.parent.version} org.apache.druid.extensions mysql-metadata-storage ${project.parent.version} (2) Shade Jackson packages and assemble a fat jar. org.apache.maven.plugins maven-shade-plugin package shade ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar com.fasterxml.jackson shade.com.fasterxml.jackson *:* *:* META-INF/*.SF META-INF/*.DSA META-INF/*.RSA Copy out services/target/xxxxx-selfcontained.jar after mvn install in project root for further usage. (3) run hadoop indexer (post an indexing task is not possible now) as below. lib is not needed anymore. As hadoop indexer is a standalone tool, you don't have to replace the jars of your running services: java -Xmx32m \\ -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\ -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\ -Djava.security.krb5.conf=$KRB5 \\ org.apache.druid.cli.Main index hadoop \\ $config_path 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/management-uis.html":{"url":"operations/management-uis.html","title":"管理界面","keywords":"","body":" Legacy consoles Druid provides a console for managing datasources, segments, tasks, data processes (Historicals and MiddleManagers), and coordinator dynamic configuration. The user can also run SQL and native Druid queries within the console. For more information on the Druid Console, have a look at the Druid Console overview The Druid Console contains all of the functionality provided by the older consoles described below, which are still available if needed. The legacy consoles may be replaced by the Druid Console in the future. These older consoles provide a subset of the functionality of the Druid Console. We recommend using the Druid Console if possible. Coordinator consoles Version 2 The Druid Coordinator exposes a web console for displaying cluster information and rule configuration. After the Coordinator starts, the console can be accessed at: http://: There exists a full cluster view (which shows indexing tasks and Historical processes), as well as views for individual Historical processes, datasources and segments themselves. Segment information can be displayed in raw JSON form or as part of a sortable and filterable table. The Coordinator console also exposes an interface to creating and editing rules. All valid datasources configured in the segment database, along with a default datasource, are available for configuration. Rules of different types can be added, deleted or edited. Version 1 The oldest version of Druid's Coordinator console is still available for backwards compatibility at: http://:/old-console Overlord console The Overlord console can be used to view pending tasks, running tasks, available workers, and recent worker creation and termination. The console can be accessed at: http://:/console.html 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/dump-segment.html":{"url":"operations/dump-segment.html","title":"导出数据段文件","keywords":"","body":" The DumpSegment tool can be used to dump the metadata or contents of an Apache Druid segment for debugging purposes. Note that the dump is not necessarily a full-fidelity translation of the segment. In particular, not all metadata is included, and complex metric values may not be complete. To run the tool, point it at a segment directory and provide a file for writing output: java -classpath \"/my/druid/lib/*\" -Ddruid.extensions.loadList=\"[]\" org.apache.druid.cli.Main \\ tools dump-segment \\ --directory /home/druid/path/to/segment/ \\ --out /home/druid/output.txt Output format Data dumps By default, or with --dump rows, this tool dumps rows of the segment as newline-separate JSON objects, with one object per line, using the default serialization for each column. Normally all columns are included, but if you like, you can limit the dump to specific columns with --column name. For example, one line might look like this when pretty-printed: { \"__time\": 1442018818771, \"added\": 36, \"channel\": \"#en.wikipedia\", \"cityName\": null, \"comment\": \"added project\", \"count\": 1, \"countryIsoCode\": null, \"countryName\": null, \"deleted\": 0, \"delta\": 36, \"isAnonymous\": \"false\", \"isMinor\": \"false\", \"isNew\": \"false\", \"isRobot\": \"false\", \"isUnpatrolled\": \"false\", \"iuser\": \"00001553\", \"metroCode\": null, \"namespace\": \"Talk\", \"page\": \"Talk:Oswald Tilghman\", \"regionIsoCode\": null, \"regionName\": null, \"user\": \"GELongstreet\" } Metadata dumps With --dump metadata, this tool dumps metadata instead of rows. Metadata dumps generated by this tool are in the same format as returned by the SegmentMetadata query. Bitmap dumps With --dump bitmaps, this tool dump bitmap indexes instead of rows. Bitmap dumps generated by this tool include dictionary-encoded string columns only. The output contains a field \"bitmapSerdeFactory\" describing the type of bitmaps used in the segment, and a field \"bitmaps\" containing the bitmaps for each value of each column. These are base64 encoded by default, but you can also dump them as lists of row numbers with --decompress-bitmaps. Normally all columns are included, but if you like, you can limit the dump to specific columns with --column name. Sample output: { \"bitmapSerdeFactory\": { \"type\": \"roaring\", \"compressRunOnSerialization\": true }, \"bitmaps\": { \"isRobot\": { \"false\": \"//aExfu+Nv3X...\", \"true\": \"gAl7OoRByQ...\" } } } Command line arguments argument description required? --directory file Directory containing segment data. This could be generated by unzipping an \"index.zip\" from deep storage. yes --output file File to write to, or omit to write to stdout. yes --dump TYPE Dump either 'rows' (default), 'metadata', or 'bitmaps' no --column columnName Column to include. Specify multiple times for multiple columns, or omit to include all columns. no --filter json JSON-encoded query filter. Omit to include all rows. Only used if dumping rows. no --time-iso8601 Format __time column in ISO8601 format rather than long. Only used if dumping rows. no --decompress-bitmaps Dump bitmaps as arrays rather than base64-encoded compressed bitmaps. Only used if dumping bitmaps. no 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/reset-cluster.html":{"url":"operations/reset-cluster.html","title":"重置集群","keywords":"","body":" The reset-cluster tool can be used to completely wipe out Apache Druid cluster state stored on Metadata and Deep storage. This is intended to be used in dev/test environments where you typically want to reset the cluster before running the test suite. reset-cluster automatically figures out necessary information from Druid cluster configuration. So the java classpath used in the command must have all the necessary druid configuration files. It can be run in one of the following ways. java org.apache.druid.cli.Main tools reset-cluster [--metadataStore] [--segmentFiles] [--taskLogs] [--hadoopWorkingPath] or java org.apache.druid.cli.Main tools reset-cluster --all Usage documentation can be printed by running following command. $ java org.apache.druid.cli.Main help tools reset-cluster NAME druid tools reset-cluster - Cleanup all persisted state from metadata and deep storage. SYNOPSIS druid tools reset-cluster [--all] [--hadoopWorkingPath] [--metadataStore] [--segmentFiles] [--taskLogs] OPTIONS --all delete all state stored in metadata and deep storage --hadoopWorkingPath delete hadoopWorkingPath --metadataStore delete all records in metadata storage --segmentFiles delete all segment files from deep storage --taskLogs delete all tasklogs 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/insert-segment-to-db.html":{"url":"operations/insert-segment-to-db.html","title":"插入数据段文件","keywords":"","body":" In older versions of Apache Druid, insert-segment-to-db was a tool that could scan deep storage and insert data from there into Druid metadata storage. It was intended to be used to update the segment table in the metadata storage after manually migrating segments from one place to another, or even to recover lost metadata storage by telling it where the segments are stored. In Druid 0.14.x and earlier, Druid wrote segment metadata to two places: the metadata store's druid_segments table, and descriptor.json files in deep storage. This practice was stopped in Druid 0.15.0 as part of consolidated metadata management, for the following reasons: If any segments are manually dropped or re-enabled by cluster operators, this information is not reflected in deep storage. Restoring metadata from deep storage would undo any such drops or re-enables. Ingestion methods that allocate segments optimistically (such as native Kafka or Kinesis stream ingestion, or native batch ingestion in 'append' mode) can write segments to deep storage that are not meant to actually be used by the Druid cluster. There is no way, while purely looking at deep storage, to differentiate the segments that made it into the metadata store originally (and therefore should be used) from the segments that did not (and therefore should not be used). Nothing in Druid other than the insert-segment-to-db tool read the descriptor.json files. After this change, Druid stopped writing descriptor.json files to deep storage, and now only writes segment metadata to the metadata store. This meant the insert-segment-to-db tool is no longer useful, so it was removed in Druid 0.15.0. It is highly recommended that you take regular backups of your metadata store, since it is difficult to recover Druid clusters properly without it. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/pull-deps.html":{"url":"operations/pull-deps.html","title":"pull-deps工具","keywords":"","body":" pull-deps is an Apache Druid tool that can pull down dependencies to the local repository and lay dependencies out into the extension directory as needed. pull-deps has several command line options, they are as follows: -c or --coordinate (Can be specified multiple times) Extension coordinate to pull down, followed by a maven coordinate, e.g. org.apache.druid.extensions:mysql-metadata-storage -h or --hadoop-coordinate (Can be specified multiply times) Apache Hadoop dependency to pull down, followed by a maven coordinate, e.g. org.apache.hadoop:hadoop-client:2.4.0 --no-default-hadoop Don't pull down the default hadoop coordinate, i.e., org.apache.hadoop:hadoop-client:2.3.0. If -h option is supplied, then default hadoop coordinate will not be downloaded. --clean Remove existing extension and hadoop dependencies directories before pulling down dependencies. -l or --localRepository A local repository that Maven will use to put downloaded files. Then pull-deps will lay these files out into the extensions directory as needed. -r or --remoteRepository Add a remote repository. Unless --no-default-remote-repositories is provided, these will be used after https://repo1.maven.org/maven2/ and https://metamx.artifactoryonline.com/metamx/pub-libs-releases-local --no-default-remote-repositories Don't use the default remote repositories, only use the repositories provided directly via --remoteRepository. -d or --defaultVersion Version to use for extension coordinate that doesn't have a version information. For example, if extension coordinate is org.apache.druid.extensions:mysql-metadata-storage, and default version is {{DRUIDVERSION}}, then this coordinate will be treated as org.apache.druid.extensions:mysql-metadata-storage:{{DRUIDVERSION}} --use-proxy Use http/https proxy to send request to the remote repository servers. --proxy-host and --proxy-port must be set explicitly if this option is enabled. --proxy-type Set the proxy type, Should be either http or https, default value is https. --proxy-host Set the proxy host. e.g. proxy.com. --proxy-port Set the proxy port number. e.g. 8080. --proxy-username Set a username to connect to the proxy, this option is only required if the proxy server uses authentication. --proxy-password Set a password to connect to the proxy, this option is only required if the proxy server uses authentication. To run pull-deps, you should 1) Specify druid.extensions.directory and druid.extensions.hadoopDependenciesDir, these two properties tell pull-deps where to put extensions. If you don't specify them, default values will be used, see Configuration. 2) Tell pull-deps what to download using -c or -h option, which are followed by a maven coordinate. Example: Suppose you want to download mysql-metadata-storage and hadoop-client(both 2.3.0 and 2.4.0) with a specific version, you can run pull-deps command with -c org.apache.druid.extensions:mysql-metadata-storage:{{DRUIDVERSION}}, -h org.apache.hadoop:hadoop-client:2.3.0 and -h org.apache.hadoop:hadoop-client:2.4.0, an example command would be: java -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --clean -c org.apache.druid.extensions:mysql-metadata-storage:{{DRUIDVERSION}} -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0 Because --clean is supplied, this command will first remove the directories specified at druid.extensions.directory and druid.extensions.hadoopDependenciesDir, then recreate them and start downloading the extensions there. After finishing downloading, if you go to the extension directories you specified, you will see tree extensions extensions └── mysql-metadata-storage └── mysql-metadata-storage-{{DRUIDVERSION}}.jar tree hadoop-dependencies hadoop-dependencies/ └── hadoop-client ├── 2.3.0 │ ├── activation-1.1.jar │ ├── avro-1.7.4.jar │ ├── commons-beanutils-1.7.0.jar │ ├── commons-beanutils-core-1.8.0.jar │ ├── commons-cli-1.2.jar │ ├── commons-codec-1.4.jar ..... lots of jars └── 2.4.0 ├── activation-1.1.jar ├── avro-1.7.4.jar ├── commons-beanutils-1.7.0.jar ├── commons-beanutils-core-1.8.0.jar ├── commons-cli-1.2.jar ├── commons-codec-1.4.jar ..... lots of jars Note that if you specify --defaultVersion, you don't have to put version information in the coordinate. For example, if you want mysql-metadata-storage to use version {{DRUIDVERSION}}, you can change the command above to java -classpath \"/my/druid/lib/*\" org.apache.druid.cli.Main tools pull-deps --defaultVersion {{DRUIDVERSION}} --clean -c org.apache.druid.extensions:mysql-metadata-storage -h org.apache.hadoop:hadoop-client:2.3.0 -h org.apache.hadoop:hadoop-client:2.4.0 Please note to use the pull-deps tool you must know the Maven groupId, artifactId, and version of your extension. For Druid community extensions listed here, the groupId is \"org.apache.druid.extensions.contrib\" and the artifactId is the name of the extension. 本文源自Apache Druid，阅读原文 更新于： 2021-05-05 22:23:45 "},"operations/deep-storage-migration.html":{"url":"operations/deep-storage-migration.html","title":"迁移深度存储","keywords":"","body":" If you have been running an evaluation Druid cluster using local deep storage and wish to migrate to a more production-capable deep storage system such as S3 or HDFS, this document describes the necessary steps. Migration of deep storage involves the following steps at a high level: Copying segments from local deep storage to the new deep storage Exporting Druid's segments table from metadata Rewriting the load specs in the exported segment data to reflect the new deep storage location Reimporting the edited segments into metadata Shut down cluster services To ensure a clean migration, shut down the non-coordinator services to ensure that metadata state will not change as you do the migration. When migrating from Derby, the coordinator processes will still need to be up initially, as they host the Derby database. Copy segments from old deep storage to new deep storage. Before migrating, you will need to copy your old segments to the new deep storage. For information on what path structure to use in the new deep storage, please see deep storage migration options. Export segments with rewritten load specs Druid provides an Export Metadata Tool for exporting metadata from Derby into CSV files which can then be reimported. By setting deep storage migration options, the export-metadata tool will export CSV files where the segment load specs have been rewritten to load from your new deep storage location. Run the export-metadata tool on your existing cluster, using the migration options appropriate for your new deep storage location, and save the CSV files it generates. After a successful export, you can shut down the coordinator. Import metadata After generating the CSV exports with the modified segment data, you can reimport the contents of the Druid segments table from the generated CSVs. Please refer to import commands for examples. Only the druid_segments table needs to be imported. Restart cluster After importing the segment table successfully, you can now restart your cluster. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/export-metadata.html":{"url":"operations/export-metadata.html","title":"导出元数据","keywords":"","body":" Druid includes an export-metadata tool for assisting with migration of cluster metadata and deep storage. This tool exports the contents of the following Druid metadata tables: segments rules config datasource supervisors Additionally, the tool can rewrite the local deep storage location descriptors in the rows of the segments table to point to new deep storage locations (S3, HDFS, and local rewrite paths are supported). The tool has the following limitations: Only exporting from Derby metadata is currently supported If rewriting load specs for deep storage migration, only migrating from local deep storage is currently supported. export-metadata Options The export-metadata tool provides the following options: Connection Properties --connectURI: The URI of the Derby database, e.g. jdbc:derby://localhost:1527/var/druid/metadata.db;create=true --user: Username --password: Password --base: corresponds to the value of druid.metadata.storage.tables.base in the configuration, druid by default. Output Path --output-path, -o: The output directory of the tool. CSV files for the Druid segments, rules, config, datasource, and supervisors tables will be written to this directory. Export Format Options --use-hex-blobs, -x: If set, export BLOB payload columns as hexadecimal strings. This needs to be set if importing back into Derby. Default is false. --booleans-as-strings, -t: If set, write boolean values as \"true\" or \"false\" instead of \"1\" and \"0\". This needs to be set if importing back into Derby. Default is false. Deep Storage Migration Migration to S3 Deep Storage By setting the options below, the tool will rewrite the segment load specs to point to a new S3 deep storage location. This helps users migrate segments stored in local deep storage to S3. --s3bucket, -b: The S3 bucket that will hold the migrated segments --s3baseKey, -k: The base S3 key where the migrated segments will be stored When copying the local deep storage segments to S3, the rewrite performed by this tool requires that the directory structure of the segments be unchanged. For example, if the cluster had the following local deep storage configuration: druid.storage.type=local druid.storage.storageDirectory=/druid/segments If the target S3 bucket was migration, with a base key of example, the contents of s3://migration/example/ must be identical to that of /druid/segments on the old local filesystem. Migration to HDFS Deep Storage By setting the options below, the tool will rewrite the segment load specs to point to a new HDFS deep storage location. This helps users migrate segments stored in local deep storage to HDFS. --hadoopStorageDirectory, -h: The HDFS path that will hold the migrated segments When copying the local deep storage segments to HDFS, the rewrite performed by this tool requires that the directory structure of the segments be unchanged, with the exception of directory names containing colons (:). For example, if the cluster had the following local deep storage configuration: druid.storage.type=local druid.storage.storageDirectory=/druid/segments If the target hadoopStorageDirectory was /migration/example, the contents of hdfs:///migration/example/ must be identical to that of /druid/segments on the old local filesystem. Additionally, the segments paths in local deep storage contain colons(:) in their names, e.g.: wikipedia/2016-06-27T02:00:00.000Z_2016-06-27T03:00:00.000Z/2019-05-03T21:57:15.950Z/1/index.zip HDFS cannot store files containing colons, and this tool expects the colons to be replaced with underscores (_) in HDFS. In this example, the wikipedia segment above under /druid/segments in local deep storage would need to be migrated to HDFS under hdfs:///migration/example/ with the following path: wikipedia/2016-06-27T02_00_00.000Z_2016-06-27T03_00_00.000Z/2019-05-03T21_57_15.950Z/1/index.zip Migration to New Local Deep Storage Path By setting the options below, the tool will rewrite the segment load specs to point to a new local deep storage location. This helps users migrate segments stored in local deep storage to a new path (e.g., a new NFS mount). --newLocalPath, -n: The new path on the local filesystem that will hold the migrated segments When copying the local deep storage segments to a new path, the rewrite performed by this tool requires that the directory structure of the segments be unchanged. For example, if the cluster had the following local deep storage configuration: druid.storage.type=local druid.storage.storageDirectory=/druid/segments If the new path was /migration/example, the contents of /migration/example/ must be identical to that of /druid/segments on the local filesystem. Running the tool To use the tool, you can run the following from the root of the Druid package: cd ${DRUID_ROOT} mkdir -p /tmp/csv java -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[] org.apache.druid.cli.Main tools export-metadata --connectURI \"jdbc:derby://localhost:1527/var/druid/metadata.db;\" -o /tmp/csv In the example command above: lib is the Druid lib directory extensions is the Druid extensions directory /tmp/csv is the output directory. Please make sure that this directory exists. Importing Metadata After running the tool, the output directory will contain _raw.csv and .csv files. The _raw.csv files are intermediate files used by the tool, containing the table data as exported by Derby without modification. The .csv files are used for import into another database such as MySQL and PostgreSQL and have any configured deep storage location rewrites applied. Example import commands for Derby, MySQL, and PostgreSQL are shown below. These example import commands expect /tmp/csv and its contents to be accessible from the server. For other options, such as importing from the client filesystem, please refer to the database's documentation. Derby CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','\"',null,0); CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','\"',null,0); CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','\"',null,0); CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','\"',null,0); CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','\"',null,0); MySQL LOAD DATA INFILE '/tmp/csv/druid_segments.csv' INTO TABLE druid_segments FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,created_date,start,end,partitioned,version,used,payload); SHOW WARNINGS; LOAD DATA INFILE '/tmp/csv/druid_rules.csv' INTO TABLE druid_rules FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,version,payload); SHOW WARNINGS; LOAD DATA INFILE '/tmp/csv/druid_config.csv' INTO TABLE druid_config FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (name,payload); SHOW WARNINGS; LOAD DATA INFILE '/tmp/csv/druid_dataSource.csv' INTO TABLE druid_dataSource FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (dataSource,created_date,commit_metadata_payload,commit_metadata_sha1); SHOW WARNINGS; LOAD DATA INFILE '/tmp/csv/druid_supervisors.csv' INTO TABLE druid_supervisors FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,spec_id,created_date,payload); SHOW WARNINGS; PostgreSQL COPY druid_segments(id,dataSource,created_date,start,\"end\",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV; COPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV; COPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV; COPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV; COPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV; 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"operations/metadata-migration.html":{"url":"operations/metadata-migration.html","title":"元数据迁移","keywords":"","body":" If you have been running an evaluation Druid cluster using the built-in Derby metadata storage and wish to migrate to a more production-capable metadata store such as MySQL or PostgreSQL, this document describes the necessary steps. Shut down cluster services To ensure a clean migration, shut down the non-coordinator services to ensure that metadata state will not change as you do the migration. When migrating from Derby, the coordinator processes will still need to be up initially, as they host the Derby database. Exporting metadata Druid provides an Export Metadata Tool for exporting metadata from Derby into CSV files which can then be imported into your new metadata store. The tool also provides options for rewriting the deep storage locations of segments; this is useful for deep storage migration. Run the export-metadata tool on your existing cluster, and save the CSV files it generates. After a successful export, you can shut down the coordinator. Initializing the new metadata store Create database Before importing the existing cluster metadata, you will need to set up the new metadata store. The MySQL extension and PostgreSQL extension docs have instructions for initial database setup. Update configuration Update your Druid runtime properties with the new metadata configuration. Create Druid tables Druid provides a metadata-init tool for creating Druid's metadata tables. After initializing the Druid database, you can run the commands shown below from the root of the Druid package to initialize the tables. In the example commands below: lib is the Druid lib directory extensions is the Druid extensions directory base corresponds to the value of druid.metadata.storage.tables.base in the configuration, druid by default. The --connectURI parameter corresponds to the value of druid.metadata.storage.connector.connectURI. The --user parameter corresponds to the value of druid.metadata.storage.connector.user. The --password parameter corresponds to the value of druid.metadata.storage.connector.password. MySQL cd ${DRUID_ROOT} java -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[\\\"mysql-metadata-storage\\\"] -Ddruid.metadata.storage.type=mysql org.apache.druid.cli.Main tools metadata-init --connectURI=\"\" --user --password --base druid PostgreSQL cd ${DRUID_ROOT} java -classpath \"lib/*\" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory=\"extensions\" -Ddruid.extensions.loadList=[\\\"postgresql-metadata-storage\\\"] -Ddruid.metadata.storage.type=postgresql org.apache.druid.cli.Main tools metadata-init --connectURI=\"\" --user --password --base druid Import metadata After initializing the tables, please refer to the import commands for your target database. Restart cluster After importing the metadata successfully, you can now restart your cluster. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"operations/use_sbt_to_build_fat_jar.html":{"url":"operations/use_sbt_to_build_fat_jar.html","title":"打包fat jar","keywords":"","body":" libraryDependencies ++= Seq( \"com.amazonaws\" % \"aws-java-sdk\" % \"1.9.23\" exclude(\"common-logging\", \"common-logging\"), \"org.joda\" % \"joda-convert\" % \"1.7\", \"joda-time\" % \"joda-time\" % \"2.7\", \"org.apache.druid\" % \"druid\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid\" % \"druid-services\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid\" % \"druid-indexing-service\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid\" % \"druid-indexing-hadoop\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid.extensions\" % \"mysql-metadata-storage\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid.extensions\" % \"druid-s3-extensions\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid.extensions\" % \"druid-histogram\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"org.apache.druid.extensions\" % \"druid-hdfs-storage\" % \"0.8.1\" excludeAll ( ExclusionRule(\"org.ow2.asm\"), ExclusionRule(\"com.fasterxml.jackson.core\"), ExclusionRule(\"com.fasterxml.jackson.datatype\"), ExclusionRule(\"com.fasterxml.jackson.dataformat\"), ExclusionRule(\"com.fasterxml.jackson.jaxrs\"), ExclusionRule(\"com.fasterxml.jackson.module\") ), \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.3.0\", \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.3.0\", \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.3.0\", \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-guava\" % \"2.3.0\", \"com.fasterxml.jackson.datatype\" % \"jackson-datatype-joda\" % \"2.3.0\", \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-base\" % \"2.3.0\", \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-json-provider\" % \"2.3.0\", \"com.fasterxml.jackson.jaxrs\" % \"jackson-jaxrs-smile-provider\" % \"2.3.0\", \"com.fasterxml.jackson.module\" % \"jackson-module-jaxb-annotations\" % \"2.3.0\", \"com.sun.jersey\" % \"jersey-servlet\" % \"1.17.1\", \"mysql\" % \"mysql-connector-java\" % \"5.1.34\", \"org.scalatest\" %% \"scalatest\" % \"2.2.3\" % \"test\", \"org.mockito\" % \"mockito-core\" % \"1.10.19\" % \"test\" ) assemblyMergeStrategy in assembly := { case path if path contains \"pom.\" => MergeStrategy.first case path if path contains \"javax.inject.Named\" => MergeStrategy.first case path if path contains \"mime.types\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/impl/SimpleLog.class\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/impl/SimpleLog$1.class\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/impl/NoOpLog.class\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/LogFactory.class\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/LogConfigurationException.class\" => MergeStrategy.first case path if path contains \"org/apache/commons/logging/Log.class\" => MergeStrategy.first case path if path contains \"META-INF/jersey-module-version\" => MergeStrategy.first case path if path contains \".properties\" => MergeStrategy.first case path if path contains \".class\" => MergeStrategy.first case x => val oldStrategy = (assemblyMergeStrategy in assembly).value oldStrategy(x) } 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"development/overview.html":{"url":"development/overview.html","title":"概览","keywords":"","body":" Druid's codebase consists of several major components. For developers interested in learning the code, this document provides a high level overview of the main components that make up Druid and the relevant classes to start from to learn the code. Storage format Data in Druid is stored in a custom column format known as a segment. Segments are composed of different types of columns. Column.java and the classes that extend it is a great place to looking into the storage format. Segment creation Raw data is ingested in IncrementalIndex.java, and segments are created in IndexMerger.java. Storage engine Druid segments are memory mapped in IndexIO.java to be exposed for querying. Query engine Most of the logic related to Druid queries can be found in the Query* classes. Druid leverages query runners to run queries. Query runners often embed other query runners and each query runner adds on a layer of logic. A good starting point to trace the query logic is to start from QueryResource.java. Coordination Most of the coordination logic for Historical processes is on the Druid Coordinator. The starting point here is DruidCoordinator.java. Most of the coordination logic for (real-time) ingestion is in the Druid indexing service. The starting point here is OverlordResource.java. Real-time Ingestion Druid loads data through FirehoseFactory.java classes. Firehoses often wrap other firehoses, where, similar to the design of the query runners, each firehose adds a layer of logic, and the persist and hand-off logic is in RealtimePlumber.java. Hadoop-based Batch Ingestion The two main Hadoop indexing classes are HadoopDruidDetermineConfigurationJob.java for the job to determine how many Druid segments to create, and HadoopDruidIndexerJob.java, which creates Druid segments. At some point in the future, we may move the Hadoop ingestion code out of core Druid. Internal UIs Druid currently has two internal UIs. One is for the Coordinator and one is for the Overlord. At some point in the future, we will likely move the internal UI code out of core Druid. Client libraries We welcome contributions for new client libraries to interact with Druid. See the Community and third-party libraries page for links to existing client libraries. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"development/modules.html":{"url":"development/modules.html","title":"模块","keywords":"","body":" Druid uses a module system that allows for the addition of extensions at runtime. Writing your own extensions Druid's extensions leverage Guice in order to add things at runtime. Basically, Guice is a framework for Dependency Injection, but we use it to hold the expected object graph of the Druid process. Extensions can make any changes they want/need to the object graph via adding Guice bindings. While the extensions actually give you the capability to change almost anything however you want, in general, we expect people to want to extend one of the things listed below. This means that we honor our versioning strategy for changes that affect the interfaces called out on this page, but other interfaces are deemed \"internal\" and can be changed in an incompatible manner even between patch releases. Add a new deep storage implementation by extending the org.apache.druid.segment.loading.DataSegment* and org.apache.druid.tasklogs.TaskLog* classes. Add a new input source by extending org.apache.druid.data.input.InputSource. Add a new input entity by extending org.apache.druid.data.input.InputEntity. Add a new input source reader if necessary by extending org.apache.druid.data.input.InputSourceReader. You can use org.apache.druid.data.input.impl.InputEntityIteratingReader in most cases. Add a new input format by extending org.apache.druid.data.input.InputFormat. Add a new input entity reader by extending org.apache.druid.data.input.TextReader for text formats or org.apache.druid.data.input.IntermediateRowParsingReader for binary formats. Add Aggregators by extending org.apache.druid.query.aggregation.AggregatorFactory, org.apache.druid.query.aggregation.Aggregator, and org.apache.druid.query.aggregation.BufferAggregator. Add PostAggregators by extending org.apache.druid.query.aggregation.PostAggregator. Add ExtractionFns by extending org.apache.druid.query.extraction.ExtractionFn. Add Complex metrics by extending org.apache.druid.segment.serde.ComplexMetricSerde. Add new Query types by extending org.apache.druid.query.QueryRunnerFactory, org.apache.druid.query.QueryToolChest, and org.apache.druid.query.Query. Add new Jersey resources by calling Jerseys.addResource(binder, clazz). Add new Jetty filters by extending org.apache.druid.server.initialization.jetty.ServletFilterHolder. Add new secret providers by extending org.apache.druid.metadata.PasswordProvider. Add new dynamic configuration providers by extending org.apache.druid.metadata.DynamicConfigProvider. Add new ingest transform by implementing the org.apache.druid.segment.transform.Transform interface from the druid-processing package. Bundle your extension with all the other Druid extensions Extensions are added to the system via an implementation of org.apache.druid.initialization.DruidModule. Creating a Druid Module The DruidModule class is has two methods A configure(Binder) method A getJacksonModules() method The configure(Binder) method is the same method that a normal Guice module would have. The getJacksonModules() method provides a list of Jackson modules that are used to help initialize the Jackson ObjectMapper instances used by Druid. This is how you add extensions that are instantiated via Jackson (like AggregatorFactory and InputSource objects) to Druid. Registering your Druid Module Once you have your DruidModule created, you will need to package an extra file in the META-INF/services directory of your jar. This is easiest to accomplish with a maven project by creating files in the src/main/resources directory. There are examples of this in the Druid code under the cassandra-storage, hdfs-storage and s3-extensions modules, for examples. The file that should exist in your jar is META-INF/services/org.apache.druid.initialization.DruidModule It should be a text file with a new-line delimited list of package-qualified classes that implement DruidModule like org.apache.druid.storage.cassandra.CassandraDruidModule If your jar has this file, then when it is added to the classpath or as an extension, Druid will notice the file and will instantiate instances of the Module. Your Module should have a default constructor, but if you need access to runtime configuration properties, it can have a method with @Inject on it to get a Properties object injected into it from Guice. Adding a new deep storage implementation Check the azure-storage, google-storage, cassandra-storage, hdfs-storage and s3-extensions modules for examples of how to do this. The basic idea behind the extension is that you need to add bindings for your DataSegmentPusher and DataSegmentPuller objects. The way to add them is something like (taken from HdfsStorageDruidModule) Binders.dataSegmentPullerBinder(binder) .addBinding(\"hdfs\") .to(HdfsDataSegmentPuller.class).in(LazySingleton.class); Binders.dataSegmentPusherBinder(binder) .addBinding(\"hdfs\") .to(HdfsDataSegmentPusher.class).in(LazySingleton.class); Binders.dataSegment*Binder() is a call provided by the druid-core jar which sets up a Guice multibind \"MapBinder\". If that doesn't make sense, don't worry about it, just think of it as a magical incantation. addBinding(\"hdfs\") for the Puller binder creates a new handler for loadSpec objects of type \"hdfs\". For the Pusher binder it creates a new type value that you can specify for the druid.storage.type parameter. to(...).in(...); is normal Guice stuff. In addition to DataSegmentPusher and DataSegmentPuller, you can also bind: DataSegmentKiller: Removes segments, used as part of the Kill Task to delete unused segments, i.e. perform garbage collection of segments that are either superseded by newer versions or that have been dropped from the cluster. DataSegmentMover: Allow migrating segments from one place to another, currently this is only used as part of the MoveTask to move unused segments to a different S3 bucket or prefix, typically to reduce storage costs of unused data (e.g. move to glacier or cheaper storage) DataSegmentArchiver: Just a wrapper around Mover, but comes with a pre-configured target bucket/path, so it doesn't have to be specified at runtime as part of the ArchiveTask. Validating your deep storage implementation WARNING! This is not a formal procedure, but a collection of hints to validate if your new deep storage implementation is able do push, pull and kill segments. It's recommended to use batch ingestion tasks to validate your implementation. The segment will be automatically rolled up to Historical note after ~20 seconds. In this way, you can validate both push (at realtime process) and pull (at Historical process) segments. DataSegmentPusher Wherever your data storage (cloud storage service, distributed file system, etc.) is, you should be able to see one new file: index.zip (partitionNum_index.zip for HDFS data storage) after your ingestion task ends. DataSegmentPuller After ~20 secs your ingestion task ends, you should be able to see your Historical process trying to load the new segment. The following example was retrieved from a Historical process configured to use Azure for deep storage: 2015-04-14T02:42:33,450 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - New request[LOAD: dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00 .000Z_2015-04-14T02:41:09.484Z] with zNode[/druid/dev/loadQueue/192.168.33.104:8081/dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09. 484Z]. 2015-04-14T02:42:33,451 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - Loading segment dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.0 00Z_2015-04-14T02:41:09.484Z 2015-04-14T02:42:33,463 INFO [ZkCoordinator-0] org.apache.druid.guice.JsonConfigurator - Loaded class[class org.apache.druid.storage.azure.AzureAccountConfig] from props[drui d.azure.] as [org.apache.druid.storage.azure.AzureAccountConfig@759c9ad9] 2015-04-14T02:49:08,275 INFO [ZkCoordinator-0] org.apache.druid.utils.CompressionUtils - Unzipping file[/opt/druid/tmp/compressionUtilZipCache1263964429587449785.z ip] to [/opt/druid/zk_druid/dde/2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z/2015-04-14T02:41:09.484Z/0] 2015-04-14T02:49:08,276 INFO [ZkCoordinator-0] org.apache.druid.storage.azure.AzureDataSegmentPuller - Loaded 1196 bytes from [dde/2015-01-02T00:00:00.000Z_2015-01-03 T00:00:00.000Z/2015-04-14T02:41:09.484Z/0/index.zip] to [/opt/druid/zk_druid/dde/2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z/2015-04-14T02:41:09.484Z/0] 2015-04-14T02:49:08,277 WARN [ZkCoordinator-0] org.apache.druid.segment.loading.SegmentLoaderLocalCacheManager - Segment [dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z] is different than expected size. Expected [0] found [1196] 2015-04-14T02:49:08,282 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.BatchDataSegmentAnnouncer - Announcing segment[dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z] at path[/druid/dev/segments/192.168.33.104:8081/192.168.33.104:8081_historical__default_tier_2015-04-14T02:49:08.282Z_7bb87230ebf940188511dd4a53ffd7351] 2015-04-14T02:49:08,292 INFO [ZkCoordinator-0] org.apache.druid.server.coordination.ZkCoordinator - Completed request [LOAD: dde_2015-01-02T00:00:00.000Z_2015-01-03T00:00:00.000Z_2015-04-14T02:41:09.484Z] DataSegmentKiller The easiest way of testing the segment killing is marking a segment as not used and then starting a killing task through the old Coordinator console. To mark a segment as not used, you need to connect to your metadata storage and update the used column to false on the segment table rows. To start a segment killing task, you need to access the old Coordinator console http://: then select the appropriate datasource and then input a time range (e.g. 2000/3000). After the killing task ends, index.zip (partitionNum_index.zip for HDFS data storage) file should be deleted from the data storage. Adding support for a new input source Adding support for a new input source requires to implement three interfaces, i.e., InputSource, InputEntity, and InputSourceReader. InputSource is to define where the input data is stored. InputEntity is to define how data can be read in parallel in native parallel indexing. InputSourceReader defines how to read your new input source and you can simply use the provided InputEntityIteratingReader in most cases. There is an example of this in the druid-s3-extensions module with the S3InputSource and S3Entity. Adding an InputSource is done almost entirely through the Jackson Modules instead of Guice. Specifically, note the implementation @Override public List getJacksonModules() { return ImmutableList.of( new SimpleModule().registerSubtypes(new NamedType(S3InputSource.class, \"s3\")) ); } This is registering the InputSource with Jackson's polymorphic serialization/deserialization layer. More concretely, having this will mean that if you specify a \"inputSource\": { \"type\": \"s3\", ... } in your IO config, then the system will load this InputSource for your InputSource implementation. Note that inside of Druid, we have made the @JacksonInject annotation for Jackson deserialized objects actually use the base Guice injector to resolve the object to be injected. So, if your InputSource needs access to some object, you can add a @JacksonInject annotation on a setter and it will get set on instantiation. Adding support for a new data format Adding support for a new data format requires implementing two interfaces, i.e., InputFormat and InputEntityReader. InputFormat is to define how your data is formatted. InputEntityReader is to define how to parse your data and convert into Druid InputRow. There is an example in the druid-orc-extensions module with the OrcInputFormat and OrcReader. Adding an InputFormat is very similar to adding an InputSource. They operate purely through Jackson and thus should just be additions to the Jackson modules returned by your DruidModule. Adding Aggregators Adding AggregatorFactory objects is very similar to InputSource objects. They operate purely through Jackson and thus should just be additions to the Jackson modules returned by your DruidModule. Adding Complex Metrics Adding ComplexMetrics is a little ugly in the current version. The method of getting at complex metrics is through registration with the ComplexMetrics.registerSerde() method. There is no special Guice stuff to get this working, just in your configure(Binder) method register the serialization/deserialization. Adding new Query types Adding a new Query type requires the implementation of three interfaces. org.apache.druid.query.Query org.apache.druid.query.QueryToolChest org.apache.druid.query.QueryRunnerFactory Registering these uses the same general strategy as a deep storage mechanism does. You do something like DruidBinders.queryToolChestBinder(binder) .addBinding(SegmentMetadataQuery.class) .to(SegmentMetadataQueryQueryToolChest.class); DruidBinders.queryRunnerFactoryBinder(binder) .addBinding(SegmentMetadataQuery.class) .to(SegmentMetadataQueryRunnerFactory.class); The first one binds the SegmentMetadataQueryQueryToolChest for usage when a SegmentMetadataQuery is used. The second one does the same thing but for the QueryRunnerFactory instead. Adding new Jersey resources Adding new Jersey resources to a module requires calling the following code to bind the resource in the module: Jerseys.addResource(binder, NewResource.class); Adding a new Password Provider implementation You will need to implement org.apache.druid.metadata.PasswordProvider interface. For every place where Druid uses PasswordProvider, a new instance of the implementation will be created, thus make sure all the necessary information required for fetching each password is supplied during object instantiation. In your implementation of org.apache.druid.initialization.DruidModule, getJacksonModules should look something like this - return ImmutableList.of( new SimpleModule(\"SomePasswordProviderModule\") .registerSubtypes( new NamedType(SomePasswordProvider.class, \"some\") ) ); where SomePasswordProvider is the implementation of PasswordProvider interface, you can have a look at org.apache.druid.metadata.EnvironmentVariablePasswordProvider for example. Adding a new DynamicConfigProvider implementation You will need to implement org.apache.druid.metadata.DynamicConfigProvider interface. For every place where Druid uses DynamicConfigProvider, a new instance of the implementation will be created, thus make sure all the necessary information required for fetching all information is supplied during object instantiation. In your implementation of org.apache.druid.initialization.DruidModule, getJacksonModules should look something like this - return ImmutableList.of( new SimpleModule(\"SomeDynamicConfigProviderModule\") .registerSubtypes( new NamedType(SomeDynamicConfigProvider.class, \"some\") ) ); where SomeDynamicConfigProvider is the implementation of DynamicConfigProvider interface, you can have a look at org.apache.druid.metadata.MapStringDynamicConfigProvider for example. Adding a Transform Extension To create a transform extension implement the org.apache.druid.segment.transform.Transform interface. You'll need to install the druid-processing package to import org.apache.druid.segment.transform. import com.fasterxml.jackson.annotation.JsonCreator; import com.fasterxml.jackson.annotation.JsonProperty; import org.apache.druid.segment.transform.RowFunction; import org.apache.druid.segment.transform.Transform; public class MyTransform implements Transform { private final String name; @JsonCreator public MyTransform( @JsonProperty(\"name\") final String name ) { this.name = name; } @JsonProperty @Override public String getName() { return name; } @Override public RowFunction getRowFunction() { return new MyRowFunction(); } static class MyRowFunction implements RowFunction { @Override public Object eval(Row row) { return \"transformed-value\"; } } } Then register your transform as a Jackson module. import com.fasterxml.jackson.databind.Module; import com.fasterxml.jackson.databind.jsontype.NamedModule; import com.fasterxml.jackson.databind.module.SimpleModule; import com.google.inject.Binder; import com.google.common.collect.ImmutableList; import org.apache.druid.initialization.DruidModule; public class MyTransformModule implements DruidModule { @Override public List getJacksonModules() { return return ImmutableList.of( new SimpleModule(\"MyTransformModule\").registerSubtypes( new NamedType(MyTransform.class, \"my-transform\") ) ): } @Override public void configure(Binder binder) { } } Bundle your extension with all the other Druid extensions When you do mvn install, Druid extensions will be packaged within the Druid tarball and extensions directory, which are both underneath distribution/target/. If you want your extension to be included, you can add your extension's maven coordinate as an argument at distribution/pom.xml During mvn install, maven will install your extension to the local maven repository, and then call pull-deps to pull your extension from there. In the end, you should see your extension underneath distribution/target/extensions and within Druid tarball. Managing dependencies Managing library collisions can be daunting for extensions which draw in commonly used libraries. Here is a list of group IDs for libraries that are suggested to be specified with a provided scope to prevent collision with versions used in druid: \"org.apache.druid\", \"com.metamx.druid\", \"asm\", \"org.ow2.asm\", \"org.jboss.netty\", \"com.google.guava\", \"com.google.code.findbugs\", \"com.google.protobuf\", \"com.esotericsoftware.minlog\", \"log4j\", \"org.slf4j\", \"commons-logging\", \"org.eclipse.jetty\", \"org.mortbay.jetty\", \"com.sun.jersey\", \"com.sun.jersey.contribs\", \"common-beanutils\", \"commons-codec\", \"commons-lang\", \"commons-cli\", \"commons-io\", \"javax.activation\", \"org.apache.httpcomponents\", \"org.apache.zookeeper\", \"org.codehaus.jackson\", \"com.fasterxml.jackson\", \"com.fasterxml.jackson.core\", \"com.fasterxml.jackson.dataformat\", \"com.fasterxml.jackson.datatype\", \"org.roaringbitmap\", \"net.java.dev.jets3t\" See the documentation in org.apache.druid.cli.PullDependencies for more information. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"development/javascript.html":{"url":"development/javascript.html","title":"javascript","keywords":"","body":" This page discusses how to use JavaScript to extend Apache Druid. Examples JavaScript can be used to extend Druid in a variety of ways: Aggregators Extraction functions Filters Post-aggregators Input parsers Router strategy Worker select strategy JavaScript can be injected dynamically at runtime, making it convenient to rapidly prototype new functionality without needing to write and deploy Druid extensions. Druid uses the Mozilla Rhino engine at optimization level 9 to compile and execute JavaScript. Security Druid does not execute JavaScript functions in a sandbox, so they have full access to the machine. So JavaScript functions allow users to execute arbitrary code inside druid process. So, by default, JavaScript is disabled. However, on dev/staging environments or secured production environments you can enable those by setting the configuration property druid.javascript.enabled = true. Global variables Avoid using global variables. Druid may share the global scope between multiple threads, which can lead to unpredictable results if global variables are used. Performance Simple JavaScript functions typically have a slight performance penalty to native speed. More complex JavaScript functions can have steeper performance penalties. Druid compiles JavaScript functions once on each data process per query. You may need to pay special attention to garbage collection when making heavy use of JavaScript functions, especially garbage collection of the compiled classes themselves. Be sure to use a garbage collector configuration that supports timely collection of unused classes (this is generally easier on JDK8 with the Metaspace than it is on JDK7). JavaScript vs. Native Extensions Generally we recommend using JavaScript when security is not an issue, and when speed of development is more important than performance or memory use. If security is an issue, or if performance and memory use are of the utmost importance, we recommend developing a native Druid extension. In addition, native Druid extensions are more flexible than JavaScript functions. There are some kinds of extensions (like sketches) that must be written as native Druid extensions due to their need for custom data formats. 本文源自Apache Druid，阅读原文 更新于： 2021-03-09 21:27:36 "},"development/build.html":{"url":"development/build.html","title":"构建","keywords":"","body":" You can build Apache Druid directly from source. Please note that these instructions are for building the latest stable version of Druid. For building the latest code in master, follow the instructions here. Prerequisites Installing Java and Maven: JDK 8, 8u92+. We recommend using an OpenJDK distribution that provides long-term support and open-source licensing, like Amazon Corretto or Azul Zulu. Maven version 3.x Other Dependencies for distribution build, Python and yaml module are required Downloading the source: git clone git@github.com:apache/druid.git cd druid Building the source The basic command to build Druid from source is: mvn clean install This will run static analysis, unit tests, compile classes, and package the projects into JARs. It will not generate the source or binary distribution tarball. In addition to the basic stages, you may also want to add the following profiles and properties: -Pdist - Distribution profile: Generates the binary distribution tarball by pulling in core extensions and dependencies and packaging the files as distribution/target/apache-druid-x.x.x-bin.tar.gz -Papache-release - Apache release profile: Generates GPG signature and checksums, and builds the source distribution tarball as distribution/target/apache-druid-x.x.x-src.tar.gz -Prat - Apache Rat profile: Runs the Apache Rat license audit tool -DskipTests - Skips unit tests (which reduces build time) -Ddruid.console.skip=true - Skip front end project Putting these together, if you wish to build the source and binary distributions with signatures and checksums, audit licenses, and skip the unit tests, you would run: mvn clean install -Papache-release,dist,rat -DskipTests 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"development/versioning.html":{"url":"development/versioning.html","title":"版本","keywords":"","body":" This page discusses how we do versioning and provides information on our stable releases. Versioning Strategy We generally follow semantic versioning. The general idea is \"Major\" version (leftmost): backwards incompatible, no guarantees exist about APIs between the versions \"Minor\" version (middle number): you can move forward from a smaller number to a larger number, but moving backwards might be incompatible. \"bug-fix\" version (\"patch\" or the rightmost): Interchangeable. The higher the number, the more things are fixed (hopefully), but the programming interfaces are completely compatible and you should be able to just drop in a new jar and have it work. Note that this is defined in terms of programming API, not in terms of functionality. It is possible that a brand new awesome way of doing something is introduced in a \"bug-fix\" release version if it doesn’t add to the public API or change it. One exception for right now, while we are still in major version 0, we are considering the APIs to be in beta and are conflating \"major\" and \"minor\" so a minor version increase could be backwards incompatible for as long as we are at major version 0. These will be communicated via email on the group. For external deployments, we recommend running the stable release tag. Releases are considered stable after we have deployed them into our production environment and they have operated bug-free for some time. Tagging strategy Tags of the codebase are equivalent to release candidates. We tag the code every time we want to take it through our release process, which includes some QA cycles and deployments. So, it is not safe to assume that a tag is a stable release, it is a solidification of the code as it goes through our production QA cycle and deployment. Tags will never change, but we often go through a number of iterations of tags before actually getting a stable release onto production. So, it is recommended that if you are not aware of what is on a tag, to stick to the stable releases listed on the Release page. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"development/experimental.html":{"url":"development/experimental.html","title":"实验功能","keywords":"","body":" Features often start out in \"experimental\" status that indicates they are still evolving. This can mean any of the following things: The feature's API may change even in minor releases or patch releases. The feature may have known \"missing\" pieces that will be added later. The feature may or may not have received full battle-testing in production environments. All experimental features are optional. Note that not all of these points apply to every experimental feature. Some have been battle-tested in terms of implementation, but are still marked experimental due to an evolving API. Please check the documentation for each feature for full details. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "},"misc/papers-and-talks.html":{"url":"misc/papers-and-talks.html","title":"论文与演讲","keywords":"","body":" Papers Druid: A Real-time Analytical Data Store - Discusses the Druid architecture in detail. The RADStack: Open Source Lambda Architecture for Interactive Analytics - Discusses how Druid supports real-time and batch workflows. Presentations Introduction to Druid - Discusses the motivations behind Druid and the architecture of the system. Druid: Interactive Queries Meet Real-Time Data - Discusses how real-time ingestion in Druid works and use cases at Netflix. Not Exactly! Fast Queries via Approximation Algorithms - Discusses how approximate algorithms work in Druid. Real-time Analytics with Open Source Technologies - Discusses Lambda architectures with Druid. Stories from the Trenches - The Challenges of Building an Analytics Stack - Discusses features that were added to scale Druid. Building Interactive Applications at Scale - Discusses building applications on top of Druid. 本文源自Apache Druid，阅读原文 更新于： 2020-10-12 09:28:46 "}}